{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCCOAJUGRQaX"
   },
   "source": [
    "# Language Model Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open in Google's Colab using : [Google Colab Link](http://colab.research.google.com/github/mdda/deep-learning-workshop/blob/master/notebooks/work-in-progress/2018-06_NLP-Trends/LM-ZeroShot.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCCOAJUGRQaX"
   },
   "source": [
    "This notebook explores the zero shot capabilities of the OpenAI multi-transformer language model in \"Improving Language Understanding by Generative Pre-Training\" adapted from their original code. In other words, the model is performing predictions with absolutely no training on these datasets.  On sentiment analysis, it achieves 73% just by appending the word \"very\" to reviews and seeing if the language model predicts \"positive\" or \"negative\". On commonsense reasoning and pronoun disambiguation, it can actually achieve results (57%, 67%) competitive with the previous state of the art (52.8%, 66.7%) by predicting the sentence with the lowest average token loss. \n",
    "\n",
    "Even though Google Brain recently surpassed these results with another language model (63.7%, 70%) in \"A Simple Method for Commonsense Reasoning\", their model is considerably massive. It is a 200GB ensemble of 14 models trained on a much larger dataset (Billion words, Gutenberg, CommonCrawl, SQuAD etc). In comparison, OpenAI's model which is <0.5GB is size is trained on 7000 books from which one could estimate a total of just 700 million words (100000 words per book). In addition, the Google Brain model trains on a additional custom dataset. Hence, it would be exciting to see what the OpenAI model is capable of, if given the same advantages.\n",
    "\n",
    "On another note, these language models are still struggling compared to humans on relatively simple text tasks like sentiment analysis or commonsense reasoning, which leads one to speculate if they are just 'brute-forcing' it, especially upon closer inspection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCCOAJUGRQaX"
   },
   "source": [
    "Datasets Tested:\n",
    "\n",
    "1.   SST2 - Sentiment Analysis\n",
    "2.   Winograd Schemas - Commonsense Reasoning\n",
    "3.   PDP - Pronoun Disambiguation\n",
    "4.   CoLA - Linguistic Acceptability\n",
    "\n",
    "OpenAI Paper: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "\n",
    "Google Brain Paper: https://arxiv.org/pdf/1806.02847.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCCOAJUGRQaX"
   },
   "source": [
    "Credit for the code in notebook goes to the OpenAI team for releasing their code, and Ken Chia (https://github.com/chiayewken) who wrangled the various datasets, testing and demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IrUTR726j5Zz"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re, json, unicodedata\n",
    "import math, time, random \n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "djaAM8vOjwik"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('finetune-transformer-lm'):\n",
    "    !git clone -q https://github.com/openai/finetune-transformer-lm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QpaSV8EjRQaY"
   },
   "outputs": [],
   "source": [
    "# !pip install -q joblib tqdm ftfy spacy\n",
    "!pip install -q joblib ftfy spacy\n",
    "if not os.path.exists('/usr/local/lib/python3.6/dist-packages/spacy/data/en'):\n",
    "    !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGiIld0hRQaX"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VhXO11thRQab"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "import ftfy, spacy\n",
    "#from tqdm import tqdm\n",
    "#tqdm = lambda x: x\n",
    "\n",
    "def parse_xml(file):\n",
    "    tree = etree.parse(file)\n",
    "    root = tree.getroot()\n",
    "    problems = list()\n",
    "    for original_problem in root.getchildren():\n",
    "        problem = dict()\n",
    "        for info in original_problem.getchildren():\n",
    "            if info.tag == 'answers':\n",
    "                answers = info.getchildren()\n",
    "                answer_list = [answer.text.strip() for answer in answers]\n",
    "                problem['answers'] = answer_list\n",
    "            elif info.tag == 'text':\n",
    "                texts = info.getchildren()\n",
    "                text_dict = dict()\n",
    "                for text1 in texts:\n",
    "                    text_dict[text1.tag] = text1.text.replace('\\n', ' ').strip()\n",
    "                problem['text'] = text_dict\n",
    "            elif info.tag == 'correctAnswer':\n",
    "                problem[info.tag] = info.text.strip().replace('.', '')\n",
    "        problems.append(problem)\n",
    "    return problems\n",
    "\n",
    "def get_pairs(word): # character bigrams as tuples\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word.\n",
    "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def text_standardize(text):\n",
    "    \"\"\"\n",
    "    fixes some issues the spacy tokenizer had on books corpus\n",
    "    also does some whitespace standardization\n",
    "    \"\"\"\n",
    "    text = text.replace('\u2014', '-')\n",
    "    text = text.replace('\u2013', '-')\n",
    "    text = text.replace('\u2015', '-')\n",
    "    text = text.replace('\u2026', '...')\n",
    "    text = text.replace('\u00b4', \"'\")\n",
    "    text = re.sub('''(-+|~+|!+|\"+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)''', r' \\1 ', text)\n",
    "    text = re.sub('\\s*\\n\\s*', ' \\n ', text)\n",
    "    text = re.sub('[^\\S\\n]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "class TextEncoder(object):\n",
    "    \"\"\"\n",
    "    mostly a wrapper for a public python bpe tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_path, bpe_path):\n",
    "        self.nlp = spacy.load('en', disable=['parser', 'tagger', 'ner', 'textcat'])\n",
    "        self.encoder = json.load(open(encoder_path))\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        merges = open(bpe_path).read().split('\\n')[1:-1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        if word == '\\n  </w>':\n",
    "            word = '\\n</w>'\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, texts, verbose=True):\n",
    "        texts_tokens = []\n",
    "        if verbose:\n",
    "            #for text in tqdm(texts, ncols=80, leave=False):\n",
    "            for text in texts:\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        else:\n",
    "            for text in texts:\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        return texts_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JZ6EmWeDRQac"
   },
   "outputs": [],
   "source": [
    "def encode_dataset(*splits, encoder):  # convert strings to token indices\n",
    "    encoded_splits = []\n",
    "    for split in splits[0]:\n",
    "        fields = []\n",
    "        for field in split:\n",
    "            if isinstance(field[0], str):\n",
    "                field = encoder.encode(field)\n",
    "            fields.append(field)\n",
    "        encoded_splits.append(fields)\n",
    "    return encoded_splits\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"\n",
    "    deal with dynamic shape in tensorflow cleanly\n",
    "    \"\"\"\n",
    "    ps = x.get_shape().as_list()\n",
    "    ts = tf.shape(x)\n",
    "    return [ts[i] if ps[i] is None else ps[i] for i in range(len(ps))]\n",
    "\n",
    "def np_softmax(x, t=1):\n",
    "    x = x/t\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    ex = np.exp(x)\n",
    "    return ex/np.sum(ex, axis=-1, keepdims=True)\n",
    "    return f\n",
    "\n",
    "def flatten(outer):\n",
    "    return [el for inner in outer for el in inner]\n",
    "\n",
    "def get_ema_if_exists(v, gvs):\n",
    "    name = v.name.split(':')[0]\n",
    "    ema_name = name+'/ExponentialMovingAverage:0'\n",
    "    ema_v = [v for v in gvs if v.name == ema_name]\n",
    "    if len(ema_v) == 0:\n",
    "        ema_v = [v]\n",
    "    return ema_v[0]\n",
    "\n",
    "def get_ema_vars(*vs):\n",
    "    if tf.get_variable_scope().reuse:\n",
    "        gvs = tf.global_variables()\n",
    "        vs = [get_ema_if_exists(v, gvs) for v in vs]\n",
    "    if len(vs) == 1:\n",
    "        return vs[0]\n",
    "    else:\n",
    "        return vs\n",
    "\n",
    "def assign_to_gpu(gpu=0, ps_dev=\"/device:CPU:0\"):\n",
    "    def _assign(op):\n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        if node_def.op == \"Variable\":\n",
    "            return ps_dev\n",
    "        else:\n",
    "            return \"/gpu:%d\" % gpu\n",
    "    return _assign\n",
    "\n",
    "def average_grads(tower_grads):\n",
    "    def average_dense(grad_and_vars):\n",
    "        if len(grad_and_vars) == 1:\n",
    "            return grad_and_vars[0][0]\n",
    "\n",
    "        grad = grad_and_vars[0][0]\n",
    "        for g, _ in grad_and_vars[1:]:\n",
    "            grad += g\n",
    "        return grad / len(grad_and_vars)\n",
    "\n",
    "    def average_sparse(grad_and_vars):\n",
    "        if len(grad_and_vars) == 1:\n",
    "            return grad_and_vars[0][0]\n",
    "\n",
    "        indices = []\n",
    "        values = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            indices += [g.indices]\n",
    "            values += [g.values]\n",
    "        indices = tf.concat(indices, 0)\n",
    "        values = tf.concat(values, 0)\n",
    "        return tf.IndexedSlices(values, indices, grad_and_vars[0][0].dense_shape)\n",
    "\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        if grad_and_vars[0][0] is None:\n",
    "            grad = None\n",
    "        elif isinstance(grad_and_vars[0][0], tf.IndexedSlices):\n",
    "            grad = average_sparse(grad_and_vars)\n",
    "        else:\n",
    "            grad = average_dense(grad_and_vars)\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgtMrB32RQah"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "K5V2NVHxRQai"
   },
   "outputs": [],
   "source": [
    "# Sentiment Dataset\n",
    "if not os.path.exists('sentiment_dataset'):\n",
    "    !git clone -q https://github.com/AcademiaSinicaNLPLab/sentiment_dataset.git\n",
    "# Winograd Schema Dataset (285 questions in XML)\n",
    "if not os.path.isfile('WSCollection.xml'):\n",
    "    !wget -q cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml\n",
    "# Pronoun Disambiguation (PDP) Dataset (60 questions in XML)\n",
    "if not os.path.isfile('PDPChallenge.xml'):\n",
    "    !wget -q cs.nyu.edu/faculty/davise/papers/PDPChallenge.xml\n",
    "# Corpus of Language Acceptability (CoLA)\n",
    "if not os.path.isfile('cola_public.zip'):\n",
    "    !wget -q https://nyu-mll.github.io/CoLA/cola_public.zip\n",
    "    !unzip -n cola_public.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nblHO_VfRQao"
   },
   "outputs": [],
   "source": [
    "# Pretrained Language Model data\n",
    "encoder_path = 'finetune-transformer-lm/model/encoder_bpe_40000.json'\n",
    "bpe_path = 'finetune-transformer-lm/model/vocab_40000.bpe'\n",
    "encoder = TextEncoder(encoder_path, bpe_path) # words and byte pairs\n",
    "n_vocab = len(encoder.encoder) # 40478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LlBUwexgdacs"
   },
   "outputs": [],
   "source": [
    "# Prepare SST2 Sentiment Analysis Dataset\n",
    "with open('sentiment_dataset/data/stsa.binary.test') as file:\n",
    "    sst = [(line[0], line[1:].strip() + ' very') for line in file]\n",
    "   \n",
    "Y_sst, X_sst = zip(*sst[:100])  # limit to 100 for speed of demo\n",
    "Y_sst = [int(i) for i in Y_sst]\n",
    "\n",
    "X_tok_sst = [encoder.encode([x])[0] for x in X_sst]\n",
    "\n",
    "!head sentiment_dataset/data/stsa.binary.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0Uhl2knkcSy3"
   },
   "outputs": [],
   "source": [
    "# Prepare Winograd Schema dataset\n",
    "# Restricted to binary questions only\n",
    "# Sorted by length for convenience of demo\n",
    "problems = parse_xml('WSCollection.xml')\n",
    "problems = [problem for problem in problems if len(problem['answers']) == 2]\n",
    "problems = sorted(problems, key = lambda x: len(x['text']['txt1'] + x['text']['txt2']))\n",
    "\n",
    "X_front = []; X_back = []; Y_wino = []\n",
    "ans_dict = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4}\n",
    "\n",
    "for problem in problems:\n",
    "    ans_list_front = []\n",
    "    ans_list_back = []\n",
    "    front = problem['text']['txt1']\n",
    "    back = problem['text']['txt2']\n",
    "    for ans in problem['answers']:\n",
    "        ans_list_front.append(' '.join([front, ans]))\n",
    "        ans_list_back.append(back)\n",
    "    X_front.append(ans_list_front)\n",
    "    X_back.append(ans_list_back)\n",
    "    Y_wino.append(ans_dict[problem['correctAnswer']])\n",
    "    \n",
    "X_front_tok = [[encoder.encode([ans])[0] for ans in x] for x in X_front]\n",
    "X_back_tok = [[encoder.encode([ans])[0] for ans in x] for x in X_back]\n",
    "\n",
    "front_lengths_wino = [[len(ans) for ans in x] for x in X_front_tok]\n",
    "\n",
    "X_tok_wino = [[X_front_tok[i][j] + X_back_tok[i][j] \n",
    "          for j in range(len(X_front_tok[i]))] \n",
    "          for i in range(len(X_front_tok))]\n",
    "\n",
    "problems[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iOrSDeHcSHi7"
   },
   "outputs": [],
   "source": [
    "# Prepare PDP dataset\n",
    "# Restricted to binary questions only\n",
    "# Sorted by length for convenience of demo\n",
    "problems = parse_xml('PDPChallenge.xml')\n",
    "problems = [problem for problem in problems if len(problem['answers']) == 2]\n",
    "problems = sorted(problems, key = lambda x: len(x['text']['txt1'] + x['text']['txt2']))\n",
    "\n",
    "X_front = []; X_back = []; Y_pdp = []\n",
    "ans_dict = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4}\n",
    "\n",
    "for problem in problems:\n",
    "    ans_list_front = []\n",
    "    ans_list_back = []\n",
    "    front = problem['text']['txt1']\n",
    "    back = problem['text']['txt2']\n",
    "    for ans in problem['answers']:\n",
    "        ans_list_front.append(' '.join([front, ans]))\n",
    "        ans_list_back.append(back)\n",
    "    X_front.append(ans_list_front)\n",
    "    X_back.append(ans_list_back)\n",
    "    Y_pdp.append(ans_dict[problem['correctAnswer']])\n",
    "    \n",
    "X_front_tok = [[encoder.encode([ans])[0] for ans in x] for x in X_front]\n",
    "X_back_tok = [[encoder.encode([ans])[0] for ans in x] for x in X_back]\n",
    "\n",
    "front_lengths_pdp = [[len(ans) for ans in x] for x in X_front_tok]\n",
    "\n",
    "X_tok_pdp = [[X_front_tok[i][j] + X_back_tok[i][j] \n",
    "          for j in range(len(X_front_tok[i]))] \n",
    "          for i in range(len(X_front_tok))]\n",
    "\n",
    "problems[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hDIMiSwUjwQa"
   },
   "outputs": [],
   "source": [
    "with open('public/raw/in_domain_dev.tsv') as file:\n",
    "    cola = [[line.split('\\t')[i].strip() for i in [1, 3]] for line in file]\n",
    "    \n",
    "Y_cola, X_cola = zip(*cola)\n",
    "Y_cola = [int(y) for y in Y_cola]\n",
    "\n",
    "X_tok_cola = [encoder.encode([x])[0] for x in X_cola]\n",
    "\n",
    "cola[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jCNBVVEARQax"
   },
   "outputs": [],
   "source": [
    "def preprocess(tokens, n_ctx, front_length=0, mode='all'):\n",
    "    # X is numpy array of tokens with positioning tokens\n",
    "    # M is masking array; the loss is only calculated from the positions with ones\n",
    "    X = np.zeros((1, 1, n_ctx, 2), dtype=np.int32)\n",
    "    M = np.zeros((1, 1, n_ctx), dtype=np.float32)\n",
    "    X[:, :, :len(tokens), 0] = tokens\n",
    "    # We manipulate the masking in order to restrict the scoring of losses\n",
    "    # Inspired by \"A Simple Method for Commonsense Reasoning\"\n",
    "    # Calculating losses for only the text after the insertion scores better\n",
    "    if mode == 'front':\n",
    "        M[:, :, :front_length] = 1\n",
    "    elif mode == 'back' or mode == 'all':\n",
    "        M[:, :, front_length:len(tokens)] = 1, \n",
    "    X[:, :, :, 1] = np.arange(n_vocab, n_vocab + n_ctx)\n",
    "    return X, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "srC9DtsaE4lD"
   },
   "outputs": [],
   "source": [
    "# Get various contexts (for masking, etc)\n",
    "n_ctx_sst = max(len(x) for x in X_tok_sst)  # 55\n",
    "n_ctx_wino = max([max([len(ans) for ans in x]) for x in X_tok_wino])  # 46\n",
    "n_ctx_pdp = max([max([len(ans) for ans in x]) for x in X_tok_pdp])  # 64\n",
    "n_ctx_cola = max(len(x) for x in X_tok_cola)  # 29\n",
    "n_ctx = max(n_ctx_sst, n_ctx_wino, n_ctx_pdp, n_ctx_cola)\n",
    "\n",
    "X_pre_sst = [preprocess(x, n_ctx, mode='all') for x in X_tok_sst]\n",
    "\n",
    "\n",
    "X_pre_all_wino = [[preprocess(X_tok_wino[i][j], n_ctx, mode='all') \n",
    "          for j in range(len(X_tok_wino[i]))] \n",
    "          for i in range(len(X_tok_wino))]\n",
    "X_pre_back_wino = [[preprocess(X_tok_wino[i][j], n_ctx, front_lengths_wino[i][j], mode='back') \n",
    "                    for j in range(len(X_tok_wino[i]))] \n",
    "                    for i in range(len(X_tok_wino))]\n",
    "X_pre_front_wino = [[preprocess(X_tok_wino[i][j], n_ctx, front_lengths_wino[i][j], mode='front') \n",
    "                     for j in range(len(X_tok_wino[i]))] \n",
    "                     for i in range(len(X_tok_wino))]\n",
    "\n",
    "\n",
    "X_pre_all_pdp = [[preprocess(X_tok_pdp[i][j], n_ctx, mode='all') \n",
    "          for j in range(len(X_tok_pdp[i]))] \n",
    "          for i in range(len(X_tok_pdp))]\n",
    "X_pre_back_pdp = [[preprocess(X_tok_pdp[i][j], n_ctx, front_lengths_pdp[i][j], mode='back') \n",
    "                    for j in range(len(X_tok_pdp[i]))] \n",
    "                    for i in range(len(X_tok_pdp))]\n",
    "X_pre_front_pdp = [[preprocess(X_tok_pdp[i][j], n_ctx, front_lengths_pdp[i][j], mode='front') \n",
    "                     for j in range(len(X_tok_pdp[i]))] \n",
    "                     for i in range(len(X_tok_pdp))]\n",
    "\n",
    "\n",
    "X_pre_cola = [preprocess(x, n_ctx, mode='all') for x in X_tok_cola]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzFq2pObRQad"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yZxN22AaRQae"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import argparse\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"\n",
    "    deal with dynamic shape in tensorflow cleanly\n",
    "    \"\"\"\n",
    "    ps = x.get_shape().as_list()\n",
    "    ts = tf.shape(x)\n",
    "    return [ts[i] if ps[i] is None else ps[i] for i in range(len(ps))]\n",
    "\n",
    "def get_ema_vars(*vs):\n",
    "    if tf.get_variable_scope().reuse:\n",
    "        gvs = tf.global_variables()\n",
    "        vs = [get_ema_if_exists(v, gvs) for v in vs]\n",
    "    if len(vs) == 1:\n",
    "        return vs[0]\n",
    "    else:\n",
    "        return vs\n",
    "\n",
    "def warmup_cosine(x, warmup=0.002):\n",
    "    s = tf.cast(x <= warmup, tf.float32)\n",
    "    return s*(x/warmup) + (1-s)*(0.5 * (1 + tf.cos(math.pi * x)))\n",
    "\n",
    "def warmup_constant(x, warmup=0.002):\n",
    "    s = tf.cast(x <= warmup, tf.float32)\n",
    "    return s*(x/warmup) + (1-s)*1\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    s = tf.cast(x <= warmup, tf.float32)\n",
    "    return (s*(x/warmup) + (1-s))*(1-x)\n",
    "\n",
    "schedules = {\n",
    "    'warmup_cosine':warmup_cosine,\n",
    "    'warmup_constant':warmup_constant,\n",
    "    'warmup_linear':warmup_linear,\n",
    "}\n",
    "\n",
    "def adam(params, grads, lr, schedule, t_total, \n",
    "         b1=0.9, b2=0.999, e=1e-8, l2=0, vector_l2=False, max_grad_norm=-1, \n",
    "         **kwargs):\n",
    "    \"\"\"\n",
    "    adam with weight decay fix\n",
    "    \"\"\"\n",
    "    t = tf.Variable(0, dtype=tf.float32, trainable=False)\n",
    "    tt = t+1\n",
    "    updates = [t.assign(tt)]\n",
    "    if max_grad_norm > 0:\n",
    "        grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "    for p, g in zip(params, grads):\n",
    "        if p is None or g is None:\n",
    "            print(\"can't train\", p.name, g)\n",
    "        else:\n",
    "            if isinstance(g, tf.IndexedSlices):\n",
    "                g = tf.convert_to_tensor(g)\n",
    "            m = tf.Variable(p*0, dtype=tf.float32, trainable=False)\n",
    "            v = tf.Variable(p*0, dtype=tf.float32, trainable=False)\n",
    "            lrt = lr*tf.sqrt(1-b2**tt)/(1-b1**tt)\n",
    "            lrt *= schedule(t/t_total)\n",
    "            mt = b1*m + (1-b1)*g\n",
    "            vt = b2*v + (1-b2)*g*g\n",
    "            if (len(p.get_shape()) > 1 or vector_l2) and l2 > 0:\n",
    "                pt = p - lrt * (mt / (tf.sqrt(vt) + e) + l2*p)\n",
    "            else:\n",
    "                pt = p - lrt * (mt / (tf.sqrt(vt) + e))\n",
    "            updates.extend([m.assign(mt), v.assign(vt), p.assign(pt)])\n",
    "    return tf.group(*updates)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(math.sqrt(2/math.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def swish(x):\n",
    "    return x*tf.nn.sigmoid(x)\n",
    "\n",
    "opt_fns = {\n",
    "    'adam':adam,\n",
    "}\n",
    "\n",
    "act_fns = {\n",
    "    'relu':tf.nn.relu,\n",
    "    'swish':swish,\n",
    "    'gelu':gelu\n",
    "}\n",
    "\n",
    "lr_schedules = {\n",
    "    'warmup_cosine':warmup_cosine,\n",
    "    'warmup_linear':warmup_linear,\n",
    "    'warmup_constant':warmup_constant,\n",
    "}\n",
    "\n",
    "def _norm(x, g=None, b=None, e=1e-5, axis=[1]):\n",
    "    u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "    s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "    x = (x - u) * tf.rsqrt(s + e)\n",
    "    if g is not None and b is not None:\n",
    "        x = x*g + b\n",
    "    return x\n",
    "\n",
    "def norm(x, scope, axis=[-1]):\n",
    "    with tf.variable_scope(scope):\n",
    "        n_state = shape_list(x)[-1]\n",
    "        g = tf.get_variable(\"g\", [n_state], initializer=tf.constant_initializer(1))\n",
    "        b = tf.get_variable(\"b\", [n_state], initializer=tf.constant_initializer(0))\n",
    "        g, b = get_ema_vars(g, b)\n",
    "        return _norm(x, g, b, axis=axis)\n",
    "\n",
    "def dropout(x, pdrop, train):\n",
    "    if train and pdrop > 0:\n",
    "        x = tf.nn.dropout(x, 1-pdrop)\n",
    "    return x\n",
    "\n",
    "def mask_attn_weights(w):\n",
    "    n = shape_list(w)[-1]\n",
    "    b = tf.matrix_band_part(tf.ones([n, n]), -1, 0)\n",
    "    b = tf.reshape(b, [1, 1, n, n])\n",
    "    w = w*b + -1e9*(1-b)\n",
    "    return w\n",
    "\n",
    "def _attn(q, k, v, train=False, scale=False):\n",
    "    w = tf.matmul(q, k)\n",
    "\n",
    "    if scale:\n",
    "        n_state = shape_list(v)[-1]\n",
    "        w = w*tf.rsqrt(tf.cast(n_state, tf.float32))\n",
    "\n",
    "    w = mask_attn_weights(w)\n",
    "    w = tf.nn.softmax(w)\n",
    "\n",
    "    w = dropout(w, attn_pdrop, train)\n",
    "\n",
    "    a = tf.matmul(w, v)\n",
    "    return a\n",
    "\n",
    "def split_states(x, n):\n",
    "    x_shape = shape_list(x)\n",
    "    m = x_shape[-1]\n",
    "    new_x_shape = x_shape[:-1]+[n, m//n]\n",
    "    return tf.reshape(x, new_x_shape)\n",
    "\n",
    "def merge_states(x):\n",
    "    x_shape = shape_list(x)\n",
    "    new_x_shape = x_shape[:-2]+[np.prod(x_shape[-2:])]\n",
    "    return tf.reshape(x, new_x_shape)\n",
    "\n",
    "def split_heads(x, n, k=False):\n",
    "    if k:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 3, 1])\n",
    "    else:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 1, 3])\n",
    "\n",
    "def merge_heads(x):\n",
    "    return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "def conv1d(x, scope, nf, rf, \n",
    "           w_init=tf.random_normal_initializer(stddev=0.02), \n",
    "           b_init=tf.constant_initializer(0), pad='VALID', train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        w = tf.get_variable(\"w\", [rf, nx, nf], initializer=w_init)\n",
    "        b = tf.get_variable(\"b\", [nf], initializer=b_init)\n",
    "        if rf == 1: #faster 1x1 conv\n",
    "            c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), \n",
    "                                     tf.reshape(w, [-1, nf]))+b, \n",
    "                           shape_list(x)[:-1]+[nf])\n",
    "        else: #was used to train LM\n",
    "            c = tf.nn.conv1d(x, w, stride=1, padding=pad)+b\n",
    "        return c\n",
    "\n",
    "def attn(x, scope, n_state, n_head, train=False, scale=False):\n",
    "    assert n_state%n_head==0\n",
    "    with tf.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3, 1, train=train)\n",
    "        q, k, v = tf.split(c, 3, 2)\n",
    "        q = split_heads(q, n_head)\n",
    "        k = split_heads(k, n_head, k=True)\n",
    "        v = split_heads(v, n_head)\n",
    "        a = _attn(q, k, v, train=train, scale=scale)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state, 1, train=train)\n",
    "        a = dropout(a, resid_pdrop, train)\n",
    "        return a\n",
    "\n",
    "def mlp(x, scope, n_state, train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        act = act_fns[afn]\n",
    "        h = act(conv1d(x, 'c_fc', n_state, 1, train=train))\n",
    "        h2 = conv1d(h, 'c_proj', nx, 1, train=train)\n",
    "        h2 = dropout(h2, resid_pdrop, train)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, train=False, scale=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        a = attn(x, 'attn', nx, n_head, train=train, scale=scale)\n",
    "        n = norm(x+a, 'ln_1')\n",
    "        m = mlp(n, 'mlp', nx*4, train=train)\n",
    "        h = norm(n+m, 'ln_2')\n",
    "        return h\n",
    "\n",
    "def embed(X, we):\n",
    "    e = tf.gather(we, X)\n",
    "    h = tf.reduce_sum(e, 2)\n",
    "    return h\n",
    "\n",
    "    \n",
    "def model(X, M, train=False, reuse=False):\n",
    "    with tf.variable_scope('model', reuse=reuse):\n",
    "        we = tf.get_variable(\"we\", [n_vocab + n_ctx, n_embd], \n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        we = dropout(we, embd_pdrop, train)\n",
    "\n",
    "        X = tf.reshape(X, [-1, n_ctx, 2])  # flatten to 1D\n",
    "        M = tf.reshape(M, [-1, n_ctx])\n",
    "\n",
    "        h = embed(X, we)\n",
    "        for layer in range(n_layer): # 12 transformer block architecture\n",
    "            h = block(h, 'h%d'%layer, train=train, scale=True)\n",
    "\n",
    "        lm_h = tf.reshape(h[:, :-1], [-1, n_embd])\n",
    "        lm_logits = tf.matmul(lm_h, we, transpose_b=True)\n",
    "        lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, labels=tf.reshape(X[:, 1:, 0], [-1]))\n",
    "        lm_losses = tf.reshape(lm_losses, [shape_list(X)[0], shape_list(X)[1]-1])\n",
    "        lm_losses = tf.reduce_sum(lm_losses*M[:, 1:], 1)/tf.reduce_sum(M[:, 1:], 1)\n",
    "\n",
    "        return lm_h, lm_logits, lm_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7VJMlDU_9M8x"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "lr = 6.25e-6\n",
    "lr_warmup = 0.002\n",
    "lr_schedule = 'warmup_linear'\n",
    "n_iter = 3\n",
    "n_batch = 4\n",
    "max_grad_norm = 1\n",
    "n_embd = 768\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "embd_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "l2 = 0.01\n",
    "vector_l2 = True\n",
    "opt = 'adam'\n",
    "afn = 'gelu'\n",
    "n_transfer = 12\n",
    "lm_coef = 0.5\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "e = 1e-8\n",
    "n_special = 0  # no special symbols like START/END needed\n",
    "n_gpu = 1\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, 1, n_ctx, 2])\n",
    "M = tf.placeholder(tf.float32, [None, 1, n_ctx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3S5UC0ur9M89"
   },
   "outputs": [],
   "source": [
    "# Build Language Model itself\n",
    "h, logits, losses = model(X, M, train=False, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PAJJJ1re9M83"
   },
   "outputs": [],
   "source": [
    "# Initialise Language Model\n",
    "params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \".*model.*\")\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MNgmEIKU9M84"
   },
   "outputs": [],
   "source": [
    "# Put in the pretrained saved weights\n",
    "shapes = json.load(open('finetune-transformer-lm/model/params_shapes.json'))\n",
    "offsets = np.cumsum([np.prod(shape) for shape in shapes])\n",
    "init_params = [np.load('finetune-transformer-lm/model/params_{}.npy'.format(n)) for n in range(10)]\n",
    "init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n",
    "init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n",
    "init_params[0] = init_params[0][:n_ctx]\n",
    "init_params[0] = np.concatenate([init_params[1], (np.random.randn(0, n_embd)*0.02).astype(np.float32), init_params[0]], 0)\n",
    "del init_params[1]\n",
    "\n",
    "n_transfer = 1 + n_transfer * 12\n",
    "_ = sess.run([p.assign(ip) for p, ip in zip(params[:n_transfer], init_params[:n_transfer])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qv5BfrxZh_io"
   },
   "source": [
    "## Test SST2 - Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6TyGwQAWmoTV"
   },
   "outputs": [],
   "source": [
    "pos = encoder.encode(['positive'])[0][0]\n",
    "neg = encoder.encode(['negative'])[0][0]\n",
    "pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GP-DSfcxigeo"
   },
   "outputs": [],
   "source": [
    "X_logits = [[sess.run(logits, {X:xx, M:m})][0] for xx,m in X_pre_sst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VpSNia_AnDsQ"
   },
   "outputs": [],
   "source": [
    "lengths = [len(x) for x in X_tok_sst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Y9us8zjhm2qy"
   },
   "outputs": [],
   "source": [
    "X_probs_pos = [X_logits[i][lengths[i]-2, pos] for i in range(len(X_logits))]\n",
    "X_probs_neg = [X_logits[i][lengths[i]-2, neg] for i in range(len(X_logits))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vWajVq09oe6u"
   },
   "outputs": [],
   "source": [
    "X_preds = [int(prob_pos > prob_neg) for prob_pos, prob_neg in zip(X_probs_pos, X_probs_neg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "prkSq__5h_ip"
   },
   "outputs": [],
   "source": [
    "accuracy_score(Y_sst, X_preds)  # 73% if full dataset is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Iu1wb6jdjt0j"
   },
   "outputs": [],
   "source": [
    "def sentiment_test(review):\n",
    "    toks = encoder.encode([review + ' very'])[0]\n",
    "    length = len(toks)\n",
    "    x, m = preprocess(toks, n_ctx, mode='all')\n",
    "    test_logits = sess.run(logits, {X:x, M:m})\n",
    "    # Test position following input for Positive vs Negative as a word\n",
    "    prob_pos = test_logits[length - 2, pos]\n",
    "    prob_neg = test_logits[length - 2, neg]\n",
    "    return prob_pos > prob_neg\n",
    "  \n",
    "def print_sentiment(review):\n",
    "    is_positive = sentiment_test(review)\n",
    "    print(\"%s = %s\" % (review, (\"Positive\" if is_positive else \"Negative\") ))\n",
    " \n",
    "print_sentiment(\"I loved this movie.\")\n",
    "print_sentiment(\"I hated this movie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uc9zW_k3SZKy"
   },
   "source": [
    "## Test Winograd Schema - Commonsense Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "U0e5Q4exRQa0"
   },
   "outputs": [],
   "source": [
    "def score_using_lm(txt):\n",
    "    tokens = encoder.encode([txt])[0]\n",
    "    x, m = preprocess(tokens, n_ctx, mode='all')\n",
    "    return \"%s - %.4f\" % (txt, sess.run(losses, {X:x, M:m})[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OBo0P4-2RQa0"
   },
   "outputs": [],
   "source": [
    "print(score_using_lm('the cat sat on the mat'))\n",
    "print(score_using_lm('the dog sat on the mat'))  # Best = Dogs!?!?\n",
    "print(score_using_lm('the mat sat on the dog'))\n",
    "print(score_using_lm('mat the on sat dog the'))  # Should be worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QrtkHBvdRQa2"
   },
   "outputs": [],
   "source": [
    "def stats_for_test_window(arr, Y_target):\n",
    "    X_losses = [ [sess.run(losses, {X:xx, M:m})[0] for xx,m in x] \n",
    "                 for x in arr]\n",
    "    X_preds = [np.argmin(x) for x in X_losses]\n",
    "\n",
    "    print(\"Accuracy:%.2f%%, F1:%.2f%%, corrcoef:%.2f\" % (\n",
    "                    accuracy_score(Y_target, X_preds)*100., \n",
    "                    f1_score(Y_target, X_preds, average='weighted')*100.,\n",
    "                    matthews_corrcoef(Y_target, X_preds),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the scores over whole of winograd sentence\n",
    "stats_for_test_window(X_pre_all_wino, Y_wino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AjpB2dByRQa5"
   },
   "outputs": [],
   "source": [
    "# This computes the scores over winograd sentence *after* replaced pronoun\n",
    "stats_for_test_window(X_pre_back_wino, Y_wino)  # Probably better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cXHpPaNiRQa7"
   },
   "outputs": [],
   "source": [
    "# This computes the scores over winograd sentence *before* replaced pronoun\n",
    "stats_for_test_window(X_pre_front_wino, Y_wino)  # Probably worse..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nw-y988YRQa_"
   },
   "outputs": [],
   "source": [
    "# Let's capture the logits for the 'whole sentence' version\n",
    "X_logits = [[sess.run(logits, {X:xx, M:m}) for xx,m in x] for x in X_pre_all_wino]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SPDHgNxFRQbB"
   },
   "outputs": [],
   "source": [
    "def get_token_probs(logits, tokens):\n",
    "    tokens = tokens[1:]\n",
    "    return [logits[i, tokens[i]] for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "X37pDsb0RQbC"
   },
   "outputs": [],
   "source": [
    "X_token_probs = [[get_token_probs(X_logits[i][j], X_tok_wino[i][j]) \n",
    "                  for j in range(len(X_logits[i]))] \n",
    "                  for i in range(len(X_logits))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g8CZBBZdRQbF"
   },
   "outputs": [],
   "source": [
    "# Make some nice charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rev = {k:v for v,k in encoder.encoder.items()}\n",
    "\n",
    "for i in range(len(X_token_probs[:10])):\n",
    "    plt.figure(figsize=(12,2))\n",
    "    for j in range(len(X_token_probs[i])):\n",
    "        plt.plot(X_token_probs[i][j])\n",
    "    plt.annotate(rev[X_tok_wino[i][0][0]].replace('</w>', ''), xy=(0,4), rotation=45, size=20)\n",
    "    for idx, tok in enumerate(X_tok_wino[i][0][1:]):\n",
    "        plt.annotate(rev[tok].replace('</w>', ''), xy=(idx, 0), rotation=45, size=20)\n",
    "    plt.show()\n",
    "    options = problems[i]['answers']\n",
    "    pred = options[X_preds[i]]\n",
    "    answer = options[0 if problems[i]['correctAnswer'] == 'A' else 1]\n",
    "    print('Options: {}\\nPredicted: {}\\nAnswer: {}'.format(options, pred, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OBrk2fqUyIA"
   },
   "source": [
    "## Test PDP - Pronoun Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mLbc-6u9UyIF"
   },
   "outputs": [],
   "source": [
    "# This computes the scores over whole of disambiguation sentence\n",
    "stats_for_test_window(X_pre_all_pdp, Y_pdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Eaeb1_kRUyIJ"
   },
   "outputs": [],
   "source": [
    "stats_for_test_window(X_pre_back_pdp, Y_pdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5c1KZtVDUyIL"
   },
   "outputs": [],
   "source": [
    "stats_for_test_window(X_pre_front_pdp, Y_pdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5-HBqVB3UyIN"
   },
   "outputs": [],
   "source": [
    "X_logits = [[sess.run(logits, {X:xx, M:m}) for xx,m in x] for x in X_pre_all_pdp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zP4Mmv6LUyIO"
   },
   "outputs": [],
   "source": [
    "def get_token_probs(logits, tokens):\n",
    "    tokens = tokens[1:]\n",
    "    return [logits[i, tokens[i]] for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rX4I5--_UyIP"
   },
   "outputs": [],
   "source": [
    "X_token_probs = [[get_token_probs(X_logits[i][j], X_tok_pdp[i][j]) \n",
    "                  for j in range(len(X_logits[i]))] \n",
    "                  for i in range(len(X_logits))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8-dRV4RWUyIT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rev = {k:v for v,k in encoder.encoder.items()}\n",
    "\n",
    "for i in range(len(X_token_probs[:10])):\n",
    "    plt.figure(figsize=(12,2))\n",
    "    for j in range(len(X_token_probs[i])):\n",
    "        plt.plot(X_token_probs[i][j])\n",
    "    plt.annotate(rev[X_tok_pdp[i][0][0]].replace('</w>', ''), xy=(0,4), rotation=45, size=20)\n",
    "    for idx, tok in enumerate(X_tok_pdp[i][0][1:]):\n",
    "        plt.annotate(rev[tok].replace('</w>', ''), xy=(idx, 0), rotation=45, size=20)\n",
    "    plt.show()\n",
    "    options = problems[i]['answers']\n",
    "    pred = options[X_preds[i]]\n",
    "    answer = options[0 if problems[i]['correctAnswer'] == 'A' else 1]\n",
    "    print('Options: {}\\nPredicted: {}\\nAnswer: {}'.format(options, pred, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CN0mpCCby7GZ"
   },
   "source": [
    "## Test CoLA - Linguistic Acceptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0GTgYZe4y7Ge"
   },
   "outputs": [],
   "source": [
    "X_losses = [sess.run(losses, {X:xx, M:m})[0] for xx,m in X_pre_cola]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9N6cKSjq0oEE"
   },
   "outputs": [],
   "source": [
    "# Let's find a threshold that works well\n",
    "thresholds_scores = []\n",
    "thresholds = []\n",
    "for threshold in np.linspace(min(X_losses)+0.1, max(X_losses), num=1000):\n",
    "    preds = [int(x < threshold) for x in X_losses]\n",
    "    thresholds_scores.append((threshold, matthews_corrcoef(Y_cola, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zQm-a6Ek23uo"
   },
   "outputs": [],
   "source": [
    "thresholds, scores = zip(*thresholds_scores)\n",
    "plt.plot(thresholds, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "A1PSujuG5rjC"
   },
   "outputs": [],
   "source": [
    "best_threshold_score = sorted(thresholds_scores, key=lambda x: x[1])[-1]\n",
    "best_threshold_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wmtlQZhp9vRR"
   },
   "outputs": [],
   "source": [
    "def acceptability_test(sentence, threshold):\n",
    "    toks = encoder.encode([sentence])[0]\n",
    "    x, m = preprocess(toks, n_ctx, mode='all')\n",
    "    loss = sess.run(losses, {X:x, M:m})\n",
    "    return loss < threshold  # True for Acceptable\n",
    "  \n",
    "def print_acceptability(sentence, threshold):\n",
    "    is_acceptable = acceptability_test(sentence, threshold)\n",
    "    print(\"%s = %s\" % (sentence, (\"Acceptable\" if is_acceptable else \"Unacceptable\") ))\n",
    " \n",
    "print_acceptability(\"This is a good sentence.\", best_threshold_score[0])\n",
    "print_acceptability(\"Sentence good a is this.\", best_threshold_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "m1IfzhjyB_rD"
   },
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "preds = [int(x < best_threshold_score[0]) for x in X_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "m1IfzhjyB_rD"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_cola, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cUQJlqzEA2l1"
   },
   "outputs": [],
   "source": [
    "# Show the sentences that the LM thinks are BAD\n",
    "for i in range(len(cola)):\n",
    "    if preds[i] == 0:\n",
    "        print(cola[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cGiIld0hRQaX",
    "SgtMrB32RQah",
    "jzFq2pObRQad",
    "Qv5BfrxZh_io",
    "uc9zW_k3SZKy",
    "9OBrk2fqUyIA",
    "CN0mpCCby7GZ"
   ],
   "default_view": {},
   "name": "OpenAI LM Zero Shot.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
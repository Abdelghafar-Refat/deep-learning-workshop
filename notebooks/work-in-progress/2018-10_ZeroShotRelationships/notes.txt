
### http://metalearning.ml/2018/#submission-instructions

Some of the fundamental questions that this workshop aims to address are:

*  What are the fundamental differences in the learning “task” compared to traditional “non-meta” learners?
*  Is there a practical limit to the number of meta-learning layers (e.g., would a meta-meta-meta-learning algorithm be of practical use)?
*  How can we design more sample-efficient meta-learning methods?
*  How can we exploit our domain knowledge to effectively guide the meta-learning process?
*  What are the meta-learning processes in nature (e.g, in humans), and how can we take inspiration from them?
*  Which ML approaches are best suited for meta-learning, in which circumstances, and why?
*  What principles can we learn from meta-learning to help us design the next generation of learning systems?


### https://r2learning.github.io/

Topics include, but are not limited to:

*  Algorithmic approaches: E.g., probabilistic generative models, message-passing neural networks, embedding methods, dimensionality reduction techniques, group-invariant architectures etc. for relational data
*  Theoretical aspects: E.g., when and why do learned representations aid relational reasoning? How does the non-i.i.d. nature of relational data conflict with our current understanding of representation learning?
*  Optimization and scalability challenges due to the inherent discreteness and curse of dimensionality of relational datasets
*  Evaluation of learned relational representations
*  Security and privacy challenges
*  Domain-specific applications
*  Any other topic of interest


Zero-Shot Relation Extraction via Reading Comprehension
*  https://arxiv.org/pdf/1706.04115.pdf
*  http://nlp.cs.washington.edu/zeroshot/  # Data
*  https://bitbucket.org/omerlevy/bidaf_no_answer  # Code

Graph Convolution over Pruned Dependency Trees Improves Relation Extraction
*  https://arxiv.org/abs/1809.10185

Scene Graph Parsing as Dependency Parsing  
*  https://arxiv.org/pdf/1803.09189.pdf
*  https://github.com/Yusics/bist-parser/tree/sgparser


BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
*  https://arxiv.org/pdf/1810.04805.pdf
  
## Other datasets

SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals
*  http://aclweb.org/anthology/S/S10/S10-1006.pdf
*  http://www.kozareva.com/downloads.html  # SemEval-2010 Task 8 Dataset

Teaching Machines to Read and Comprehend
*  https://arxiv.org/abs/1506.03340
*  NER-anonamised Daily Mail stories with questions
*  https://github.com/deepmind/rc-data # Data creator
*  https://cs.nyu.edu/~kcho/DMQA/  # Data raw download


## Pull in data

cd REPO/notebooks/work-in-progress/2018-10_ZeroShotRelationships

wget http://nlp.cs.washington.edu/zeroshot/evaluate.py

ln -s /mnt/rdai/reddragon/2018-10_ZeroShotRelationships_orig orig

cd orig
#git clone https://bitbucket.org/omerlevy/bidaf_no_answer
#hg pull && hg update default
wget https://bitbucket.org/omerlevy/bidaf_no_answer/get/2e9868b224e4.zip  # 96Kb
unzip 2e9868b224e4.zip
cd omerlevy-bidaf_no_answer-2e9868b224e4/


wget http://nlp.cs.washington.edu/zeroshot/relation_splits.tar.bz2
bunzip2 relation_splits.tar.bz2  # Output .tar : 1,936,209,920
tar -xf relation_splits.tar 
## rm relation_splits.tar  # should be fine

#wget http://nlp.cs.washington.edu/zeroshot/raw_data.tar.bz2    # 1.3Gb
#bunzip2 relation_splits.tar.bz2  # Output .tar : 6,909,859,840
#tar -xf relation_splits.tar 





# OK, so ignore the previous paper's model : It wants TF 0.11 ...
#### IGNORE START  #### 

# Inaccurate (negative_example): 
# IUCN conservation status	What is the endangered status of XXX?	Bawean deer	The Bawean deer (Hyelaphus kuhlii), also known as Kuhl's hog deer or Bawean hog deer, is a highly threatened species of deer found only in the island 
of Bawean (Gresik Regency) in Indonesia.

# Need to get 'glove.6B.100d.txt' into current directory...


. ~/env3/bin/activate

# ./run_prep.sh relation_splits  # Bad paths... so do the following : 
run_name=relation_splits
python -m zeroshot.zeroshot2squad ${run_name}/train ${run_name}/train-v1.1.json
python -m zeroshot.zeroshot2squad ${run_name}/test ${run_name}/dev-v1.1.json
python -m squad.prepro -s ${run_name} -t ${run_name}

num=1
run_squad=${run_name}-squad/${num}
mkdir -p ${run_squad}
python -m zeroshot.zeroshot2squad ${run_name}/train.${num} ${run_squad}/train-v1.1.json
python -m zeroshot.zeroshot2squad ${run_name}/test.${num} ${run_squad}/dev-v1.1.json
python -m squad.prepro -s ${run_squad} --glove_dir . -t ${run_squad}  # This will take a while...

# my/tensorflow/nn.py : First lines should be :
"""
#from tensorflow.python.ops.rnn_cell import _linear
from tensorflow.contrib.rnn.python.ops import core_rnn_cell
linear = core_rnn_cell._linear
"""

# my/tensorflow/rnn.py : Near top should be :
"""
# from tensorflow.python.ops.rnn import bidirectional_rnn as _bidirectional_rnn
from tensorflow.python.ops.rnn import static_bidirectional_rnn as _bidirectional_rnn
"""

python -m basic.cli --mode train --noload --debug

# The model was trained with NVidia Titan X (Pascal Architecture, 2016). 
# The model requires at least 12GB of GPU RAM. 
# If your GPU RAM is smaller than 12GB, you can either decrease batch size (performance might degrade), 
# or you can use multi GPU (see below). 
# The training converges at ~18k steps, and it took ~4s per step (i.e. ~20 hours).

python -m basic.cli --mode train --noload --batch_size 400 --sent_size_th 60 --num_steps 0 --num_epochs 3 \
                    --len_opt --cluster --num_gpus 1 --run_id 1 --data_dir ${run_squad} --eval_period 500
# When something starts to work ...
#Loaded 835951/840000 examples from train
#Loaded 11870/12000 examples from dev

#### IGNORE END #### 


cd  ../../orig
git clone https://github.com/openai/finetune-transformer-lm.git
rm -rf  finetune-transformer-lm/.git
git clone https://github.com/huggingface/pytorch-openai-transformer-lm.git

# cp --no-clobber orig/pytorch-openai-transformer-lm/text_utils.py ..  # Their text_utils needs updating : Bring it into my repo

# Need to go to the Rocstories website to get download links to files - put them in 'rocstories_dataset'
ls -l rocstories_dataset/
-rw-rw-r--. 1 andrewsm andrewsm   574480 Oct 14 18:58 'cloze_test_test__spring2016 - cloze_test_ALL_test.csv'
-rw-rw-r--. 1 andrewsm andrewsm   575510 Oct 14 18:56 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'
-rw-rw-r--. 1 andrewsm andrewsm 14537764 Oct 14 18:52 'ROCStories_winter2017 - ROCStories_winter2017.csv'

python train.py --dataset rocstories --desc rocstories --submit --analysis \
                --data_dir rocstories_dataset \
                --bpe_path=../finetune-transformer-lm/model/vocab_40000.bpe \
                --encoder_path=../finetune-transformer-lm/model/encoder_bpe_40000.json 
# Works




cd REPO/notebooks/work-in-progress/2018-10_ZeroShotRelationships

. ~/env3/bin/activate

python relation_split_to_hdf5.py --phase=train,dev,test


# TODO : 
#   Create HDF5 dataset reader per blog post.  Assume that ids=ALL if not given
#   Create additional 'head' with attention-direction loss fn
#   Make sure that something happens when training on the dev set (model loading, training steps, model saving, etc)
#   Kick off training with attn_loss=0 (leave running?)
#   Think about GCP P100 for additional training...  Maybe multiple P100s?  or Multiple instances?
#   Create a test script that can evaluate the results on a saved model
#   Write paper...


# Try out code with ...
python train_on_onestep.py --relation_hdf5=dev.1_all.hdf5 --stub=base --dep_fac=0.0

# Real deal :
python train_on_onestep.py --relation_hdf5=train.1_all.hdf5 --stub=base --dep_fac=0.0

Saving Checkpoint : './checkpoints/model-stepwise_base_0000-150048.pth', loss_recent=1.0170
...
Saving Checkpoint : './checkpoints/model-stepwise_base_0000-650208.pth', loss_recent=0.5590
Saving Checkpoint : './checkpoints/model-stepwise_base_0000-700224.pth', loss_recent=0.5383
Saving Checkpoint : './checkpoints/model-stepwise_base_0000-750240.pth', loss_recent=0.5181
Saving Checkpoint : './checkpoints/model-stepwise_base_0000-800256.pth', loss_recent=0.5061
Saving Checkpoint : './checkpoints/model-stepwise_base_0001-011136.pth', loss_recent=0.5059
Saving Checkpoint : './checkpoints/model-stepwise_base_0001-061152.pth', loss_recent=0.4592
Saving Checkpoint : './checkpoints/model-stepwise_base_0001-211200.pth', loss_recent=0.4179
...
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-077984.pth', loss_recent=0.2855
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-128000.pth', loss_recent=0.2753
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-178016.pth', loss_recent=0.2605
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-236992.pth', loss_recent=0.2605
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-337024.pth', loss_recent=0.2382
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-393664.pth', loss_recent=0.2382
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-494624.pth', loss_recent=0.2294
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-594656.pth', loss_recent=0.2141
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-703968.pth', loss_recent=0.2090
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-753984.pth', loss_recent=0.2079
Saving Checkpoint : './checkpoints/model-stepwise_base_0002-804000.pth', loss_recent=0.2041


-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 04:20 model-stepwise_base_0000-050016.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 04:41 model-stepwise_base_0000-100032.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 05:02 model-stepwise_base_0000-150048.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 05:23 model-stepwise_base_0000-200064.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 05:44 model-stepwise_base_0000-250080.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 06:05 model-stepwise_base_0000-300096.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 06:27 model-stepwise_base_0000-350112.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 06:48 model-stepwise_base_0000-400128.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 07:09 model-stepwise_base_0000-450144.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 07:30 model-stepwise_base_0000-500160.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 07:51 model-stepwise_base_0000-550176.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 08:13 model-stepwise_base_0000-600192.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 08:34 model-stepwise_base_0000-650208.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 08:55 model-stepwise_base_0000-700224.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 09:16 model-stepwise_base_0000-750240.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 09:37 model-stepwise_base_0000-800256.pth
-rw-rw-r--. 1 andrewsm andrewsm 1409989614 Oct 17 09:59 model-stepwise_base_0001-011136.pth


#Time per 1000.  ~ >>> (3*60+10)*60./(800256-350112)*1000.
# 25.325229259970143 secs on TitanX

morning
Factor hints (class_loss=  4.5217, deps_loss= 5344.4214, fac=0.00084607)
Factor hints (class_loss=  8.4208, deps_loss= 6178.9131, fac=0.00136283)
Factor hints (class_loss= 34.4343, deps_loss= 6007.8652, fac=0.00573154)
Factor hints (class_loss=  6.2002, deps_loss= 5839.6787, fac=0.00106173)
Factor hints (class_loss= 14.6076, deps_loss= 6349.5327, fac=0.00230058)
Factor hints (class_loss= 13.9885, deps_loss= 6815.1782, fac=0.00205256)
Factor hints (class_loss=  6.0767, deps_loss= 5829.8833, fac=0.00104233)
Factor hints (class_loss=  9.1029, deps_loss= 5803.7627, fac=0.00156844)
Factor hints (class_loss=  1.2308, deps_loss= 5650.0684, fac=0.00021784)
Factor hints (class_loss=  9.0883, deps_loss= 5750.9033, fac=0.00158032)

evening
Factor hints (class_loss=  1.3447, deps_loss= 5558.3813, fac=0.00024193)
Factor hints (class_loss= 18.4624, deps_loss= 6056.8135, fac=0.00304820)
Factor hints (class_loss=  0.1400, deps_loss= 6122.2803, fac=0.00002287)
Factor hints (class_loss=  0.0526, deps_loss= 5790.4863, fac=0.00000908)
Factor hints (class_loss= 21.1384, deps_loss= 5803.6270, fac=0.00364228)
Factor hints (class_loss=  3.1123, deps_loss= 6002.5112, fac=0.00051850)
Factor hints (class_loss= 60.7331, deps_loss= 5879.1123, fac=0.01033032)
Factor hints (class_loss=  1.3860, deps_loss= 5668.1182, fac=0.00024452)
Factor hints (class_loss=  3.6776, deps_loss= 6045.1016, fac=0.00060836)
Factor hints (class_loss= 15.7819, deps_loss= 6411.4414, fac=0.00246152)
Factor hints (class_loss= 21.9232, deps_loss= 5676.2529, fac=0.00386227)
Factor hints (class_loss=  0.3627, deps_loss= 6797.3110, fac=0.00005336)
Factor hints (class_loss= 16.1702, deps_loss= 5596.6489, fac=0.00288926)
Factor hints (class_loss=  0.0677, deps_loss= 5657.2944, fac=0.00001196)
Factor hints (class_loss=  0.7334, deps_loss= 5572.8364, fac=0.00013160)
Factor hints (class_loss=  0.7809, deps_loss= 6325.0166, fac=0.00012346)
Factor hints (class_loss=  9.2523, deps_loss= 5544.1602, fac=0.00166883)

# Locally test the test script
python train_on_onestep.py --relation_hdf5=dev.1_all.hdf5 --stub=base --predict --checkpoint=./checkpoints/model-stepwise_base_0002-804000.pth

# Locally run the test script
python train_on_onestep.py --relation_hdf5=test.1_all.hdf5 --stub=base --predict --checkpoint=./checkpoints/model-stepwise_base_0002-804000.pth

#Assessment via test_on_onestep : 
##  #precision=37.56% recall=40.10% F1=38.79%  # intersection/ans
##  precision=30.97% recall=33.06% F1=31.98%  # intersection/union of a_start_best, a_end_best

#Paper result for 'Multiple Templates' : (1_Allen_ZeroShotRelationExtraction-ReadingComprehension_1706.04115.pdf - Table 3)
##  precision=43.61% recall=36.45% F1=39.61%


# https://cloud.google.com/compute/docs/gpus/add-gpus
# Modify 'rdai-tts-p100-vm' 
#  to have 4*P100, and 
#    $0.43 USD per GPU per hour  (confirmed to have 16Gb of memory each)
#  attach a 200Gb persistent disk : 'zero-shot-relationships'
#    https://cloud.google.com/persistent-disk/

# https://cloud.google.com/tpu/docs/pricing
#   Potential : $1.35 USD per TPU per hour (preemptible)


export INSTANCE_NAME="rdai-tts-p100-vm"
gcloud compute instances start $INSTANCE_NAME

gcloud compute ssh $INSTANCE_NAME

cd ~/deep-learning-workshop/notebooks/work-in-progress/2018-10_ZeroShotRelationships/
mkdir orig # ... Follow instructions above ...


gcloud compute scp *.hdf5 rdai-tts-p100-vm:~/deep-learning-workshop/notebooks/work-in-progress/2018-10_ZeroShotRelationships/orig/omerlevy-bidaf_no_answer-2e9868b224e4/relation_splits/


cd ~/deep-learning-workshop/notebooks/work-in-progress/2018-10_ZeroShotRelationships/orig/omerlevy-bidaf_no_answer-2e9868b224e4/relation_splits/
md5sum *
6e8fe51b869fecb6acd241df9fab07d8  dev.1_all.hdf5
de44a1feb2392ce91b03f3d7b5feba35  test.1_all.hdf5
e735c9a89f98613e2e64af0ac794bf5e  train.1_all.hdf5

# locally :
md5sum *.hdf5
6e8fe51b869fecb6acd241df9fab07d8  dev.1_all.hdf5
de44a1feb2392ce91b03f3d7b5feba35  test.1_all.hdf5
e735c9a89f98613e2e64af0ac794bf5e  train.1_all.hdf5


# Mount new persistent disk : 
#   https://devopscube.com/mount-extra-disks-on-google-cloud/
sudo lsblk
#NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
#sda      8:0    0   30G  0 disk 
#└─sda1   8:1    0   30G  0 part /
#sdb      8:16   0  200G  0 disk 


cd ~/deep-learning-workshop/notebooks/work-in-progress/2018-10_ZeroShotRelationships/
mkdir checkpoints

## sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb

cd ~/deep-learning-workshop/notebooks/work-in-progress/2018-10_ZeroShotRelationships/
sudo mount -o discard,defaults /dev/sdb checkpoints
sudo chmod a+x checkpoints



. ~/env3/bin/activate

python train_on_onestep.py --relation_hdf5=dev.1_all.hdf5 --stub=base --dep_fac=0.0
# Functions (after fixing :: grep 'f"' orig/pytorch-openai-transformer-lm/*.py) 3.6 string formatting

python train_on_onestep.py --relation_hdf5=train.1_all.hdf5 --stub=base --dep_fac=0.0

Factor hints (class_loss=29012.3672, deps_loss=23580.5684, fac=1.23035063)
Factor hints (class_loss=455.4864, deps_loss=23485.1406, fac=0.01939466)
Factor hints (class_loss=271.1743, deps_loss=23552.4355, fac=0.01151364)
Factor hints (class_loss=217.7633, deps_loss=25605.2051, fac=0.00850465)
Time used for 0.06 of epoch 0: 300.5 seconds
  Time per 1000 lines : 6.005 seconds
  Expected finish time : Wednesday, October 17, 2018 12:21:37  (server)
  Expected finish time : Wednesday, October 17, 2018 12:21:37 +08+0800 (local)
Factor hints (class_loss=176.8094, deps_loss=23848.9160, fac=0.00741373)
Factor hints (class_loss=173.7655, deps_loss=22794.8516, fac=0.00762301)
Factor hints (class_loss=119.6408, deps_loss=25449.4863, fac=0.00470111)
Factor hints (class_loss=162.4608, deps_loss=24370.5605, fac=0.00666627)


screen -S QAZRE
python train_on_onestep.py --relation_hdf5=train.1_all.hdf5 --stub=fac1.0 --dep_fac=1.0


# Remotely test the test script
python train_on_onestep.py --relation_hdf5=dev.1_all.hdf5 --stub=fac1.0 --predict --checkpoint=./checkpoints/model-stepwise_fac1.0_0001-760576.pth

# Remotely run the test script
python train_on_onestep.py --relation_hdf5=test.1_all.hdf5 --stub=fac1.0 --predict --checkpoint=./checkpoints/model-stepwise_fac1.0_0001-760576.pth

export REMOTE_BASE="rdai-tts-p100-vm:~/deep-learning-workshop/notebooks/work-in-progress/2018-10_ZeroShotRelationships"
export REL_PATH="orig/omerlevy-bidaf_no_answer-2e9868b224e4/relation_splits"
## mv ${REL_PATH}/test.1_all.hdf5_base.npz ${REL_PATH}/test.1_all.hdf5_fac1.0.npz
gcloud compute scp ${REMOTE_BASE}/${REL_PATH}/test.1_all.hdf5_fac1.0.npz  ${REL_PATH}/

# Assessment via test_on_onestep : 
## precision=24.67% recall=43.13% F1=31.39%




# Next run : 
python train_on_onestep.py --relation_hdf5=train.1_all.hdf5 --stub=fac0.1 --dep_fac=0.1
python train_on_onestep.py --relation_hdf5=train.1_all.hdf5 --stub=fac0.1 --dep_fac=0.1 --checkpoint=./checkpoints/model-stepwise_fac0.1_0000-600192.pth

# Update with 'no answer' forced labels
python train_on_onestep.py --relation_hdf5=train.1_all.hdf5 --stub=fac0.1 --dep_fac=0.1 --checkpoint=./checkpoints/model-stepwise_fac0.1_0001-400128.pth

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   76C    P0   193W / 250W |  10601MiB / 16280MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-PCIE...  Off  | 00000000:00:05.0 Off |                    0 |
| N/A   71C    P0   187W / 250W |   8699MiB / 16280MiB |     97%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P100-PCIE...  Off  | 00000000:00:06.0 Off |                    0 |
| N/A   70C    P0   189W / 250W |   8699MiB / 16280MiB |     98%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P100-PCIE...  Off  | 00000000:00:07.0 Off |                    0 |
| N/A   71C    P0   203W / 250W |   8699MiB / 16280MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1374      C   python                                     10591MiB |
|    1      1374      C   python                                      8689MiB |
|    2      1374      C   python                                      8689MiB |
|    3      1374      C   python                                      8689MiB |
+-----------------------------------------------------------------------------+

# Start again with 'no answer' forced labels
python train_on_onestep.py --relation_hdf5=train.1_all.hdf5 --stub=fac0.1 --dep_fac=0.1

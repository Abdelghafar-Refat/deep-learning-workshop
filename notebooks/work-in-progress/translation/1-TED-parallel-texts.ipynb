{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Parallel Texts from TED talks\n",
    "\n",
    "Derived / inspired by : [Ajinkya Kulkarni's GitHub](https://github.com/ajinkyakulkarni14/How-I-Extracted-TED-talks-for-parallel-Corpus-/blob/master/Ipython_notebook.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "#import urllib\n",
    "import requests\n",
    "#import shutil\n",
    "#import codecs\n",
    "import os, glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enlist_talk_names(url, dict_):\n",
    "    #r = urllib.urlopen(url).read()\n",
    "    #soup = BeautifulSoup(r)\n",
    "    r = requests.get(url)\n",
    "    print(\"  Got %d bytes from %s\" % (len(r.text), url))\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    talks= soup.find_all(\"a\",class_='')\n",
    "    for i in talks:\n",
    "        if i.attrs['href'].find('/talks/')==0 and dict_.get(i.attrs['href'])!=1:\n",
    "            dict_[i.attrs['href']]=1\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_talk_names={}\n",
    "\n",
    "# Get all pages of talks (seems a bit abusive)\n",
    "#for i in xrange(1,61):\n",
    "#    url='https://www.ted.com/talks?page=%d'%(i)\n",
    "#    all_talk_names=enlist_talk_names(url, all_talk_names)\n",
    "\n",
    "# Just the AI topic talks\n",
    "url='https://www.ted.com/talks?sort=newest&q=ai'\n",
    "all_talk_names=enlist_talk_names(url,all_talk_names)\n",
    "len(all_talk_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(all_talk_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "def extract_talk(url, talk_name, language_list=['en', 'ko', 'ja']):\n",
    "    r = requests.get(url)\n",
    "    print(\"  Got %d bytes from %s\" % (len(r.text), url))\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    #df=pd.DataFrame()\n",
    "    talk_data = dict() # This is indexed on [timestep][language]\n",
    "    for i in soup.findAll('link'):\n",
    "        if i.get('href')!=None and i.attrs['href'].find('?language=')!=-1:\n",
    "            #print i.attrs['href']\n",
    "            lang=i.attrs['hreflang']\n",
    "            url_lang=i.attrs['href']\n",
    "            if not lang in language_list:\n",
    "                continue\n",
    "            r_lang = requests.get(url_lang)\n",
    "            print(\"    Lang[%s] : Got %d bytes\" % (lang, len(r_lang.text), ))\n",
    "            lang_soup = BeautifulSoup(r_lang.text, 'html.parser')\n",
    "\n",
    "            for i in lang_soup.findAll('span',class_='talk-transcript__fragment'):\n",
    "                ts = i.attrs['data-time']\n",
    "                if ts not in talk_data: \n",
    "                    talk_data[ts]=dict()\n",
    "                talk_data[ts][lang] = i.text.replace('\\n',' ')\n",
    "\n",
    "    # Now flatten out the talk_data into time_step order\n",
    "    talk_data_csv = [ ['ts']+language_list, ]\n",
    "    for ts in sorted(talk_data.keys(), key=int):\n",
    "        row = [ts] + [ talk_data[ts].get(lang, '') for lang in language_list]\n",
    "        talk_data_csv.append(row)\n",
    "        \n",
    "    with open(os.path.join(data_path, talk_name+'.csv'), 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(talk_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in all_talk_names:\n",
    "    extract_talk('https://www.ted.com'+name+'/transcript', name[7:])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
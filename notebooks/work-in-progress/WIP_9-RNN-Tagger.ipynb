{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagger\n",
    "\n",
    "This example trains a RNN to tag words from a corpus - \n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n",
    "\n",
    "\n",
    "Adversarial networks : http://carpedm20.github.io/faces/\n",
    "\n",
    "Doing this with RNNs may be pretty novel : https://www.quora.com/Can-generative-adversarial-networks-be-used-in-sequential-data-in-recurrent-neural-networks-How-effective-would-they-be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "import pickle\n",
    "#import gzip\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32\n",
    "EMBEDDING_DIM=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentence_splitter.tokenize(\"This is Mr. Smith's tokenize test. The U.S.A gives us sent two. Is this sent three?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"This is Mr. Smith's tokenize test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An interesting corpus :\n",
    "corpus_text_file = '../data/RNN/en.wikipedia.2010.100K.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, 'rt') as f:\n",
    "            for line in f.readlines():\n",
    "                #print(line)\n",
    "                n,l = line.decode(\"utf-8\").split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    tree_banked = tokenizer.tokenize(s)\n",
    "                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n",
    "                        yield tree_banked\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pos_tagger = PerceptronTagger(load=True)\n",
    "' | '.join(list(pos_tagger.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Let 's see what part of speech analysis on this sample text looks like .\".split(' ')\n",
    "#s = next(corpus_sentence_tokens_gen)\n",
    "pos_tagger.tag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twist : Not interested in all classes...\n",
    "\n",
    "To simplify (dramatically), our RNN will be trained to just tell the difference between 'is ordinary word' and 'is entity name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list = 'O E'.split(' ')\n",
    "pos_tagger_entity_tags = set('NNP'.split(' '))\n",
    "pos_tagger_to_idx   = dict([ (t,(1 if t in pos_tagger_entity_tags else 0)) for i,t in enumerate(pos_tagger.classes)])\n",
    "TAG_SET_SIZE= len(tag_list)\n",
    "\n",
    "pos_tagger_to_idx['NNP'], pos_tagger_to_idx['VBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings\n",
    "Using the python package :  https://github.com/maciejkula/glove-python , and code samples from : http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
    "\n",
    "### Create the Co-occurrence Matrix\n",
    "This should take 30 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glove\n",
    "corpus_sentences = [ \n",
    "        [ w.lower() for w in next(corpus_sentence_tokens_gen)] # All lower-case\n",
    "        for _ in range(0,100*1000) \n",
    "    ]\n",
    "# Fit the co-occurrence matrix using a sliding window of 10 words.\n",
    "glove_corpus = glove.Corpus()\n",
    "\n",
    "t0 = time.time()\n",
    "glove_corpus.fit(corpus_sentences, window=10)\n",
    "print(\"Dictionary length=%d\" % (len(glove_corpus.dictionary),))\n",
    "print(\"Co-occurrence calculated in %5.1fsec\" % (time.time()-t0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return the index of the word in the dictionary\n",
    "glove_corpus.dictionary['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the Word Embedding\n",
    "\n",
    "This will make use of up to 4 threads - and each epoch takes 20-30 seconds on a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding = glove.Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)\n",
    "t0 = time.time()\n",
    "glove_epochs, glove_threads = 20, 4 \n",
    "word_embedding.fit(glove_corpus.matrix, epochs=glove_epochs, no_threads=glove_threads, verbose=True)\n",
    "print(\"%d-d word-embedding created in %5.1fsec = %5.1fsec per epoch\" % (\n",
    "        EMBEDDING_DIM, (time.time()-t0), (time.time()-t0)/glove_epochs*glove_threads, ))\n",
    "\n",
    "# Add the word -> id dictionary to the model to allow similarity queries.\n",
    "word_embedding.add_dictionary(glove_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_embedding.save(\"../data/RNN/glove.embedding.50.pkl\")\n",
    "#word_embedding.load(\"../data/RNN/glove.embedding.50.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word-similarity test\n",
    "word_embedding.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word-analogy test\n",
    "def get_embedding_vec(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
    "    if idx<0:\n",
    "        #print(\"Missing word : '%s'\" % (word,))\n",
    "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')\n",
    "    return word_embedding.word_vectors[idx]\n",
    "\n",
    "def get_closest_word(vec, number=4):\n",
    "    dst = (np.dot(word_embedding.word_vectors, vec)\n",
    "                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n",
    "                   / np.linalg.norm(vec))\n",
    "    word_ids = np.argsort(-dst)\n",
    "    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n",
    "            if x in word_embedding.inverse_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analogy_vec = get_embedding_vec('woman') - get_embedding_vec('man') + get_embedding_vec('king')\n",
    "# analogy_vec = get_embedding_vec('paris') - get_embedding_vec('france') + get_embedding_vec('italy')\n",
    "# analogy_vec = get_embedding_vec('kitten') - get_embedding_vec('cat') + get_embedding_vec('dog')\n",
    "# analogy_vec = get_embedding_vec('laugh') - get_embedding_vec('happy') + get_embedding_vec('sad')\n",
    "get_closest_word(analogy_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem : Embedding is *Poor*\n",
    "\n",
    "Solution : Load a pre-trained word embedding, from a much larger corpus.  Source of this word embedding (created from a 6 billion tokens corpus, with results as 50d vectors): http://nlp.stanford.edu/projects/glove/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Due to size constraints, only have the first 100k vectors (i.e. 100k most frequently used words)\n",
    "word_embedding = glove.Glove.load_stanford(\"../data/RNN/glove.first-100k.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = EMBEDDING_DIM # ?+1 for capitalisation flag\n",
    "GRAD_CLIP_BOUND = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN Part-of-Speech Tagger\n",
    "\n",
    "\n",
    "### Create Training / Testing dataset\n",
    "And a 'batch generator' function that delivers data in the right format for RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_sentences(size=BATCH_SIZE):\n",
    "    return [ next(corpus_sentence_tokens_gen) for i in range(size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test it out\n",
    "batch_test = lambda : batch_sentences(size=4)\n",
    "print([ ' '.join(s) for s in batch_test()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesising a 'correct answer' for the Tagger\n",
    "\n",
    "Normally, this would be the (manual) annotations from the corpus itself.  However, we don't have an annotated corpus.  Instead, we're going to use the annotations produced by the NTLK tagger - simplified to only identify 'NNP = entities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After sampling a data batch, we transform it into a one hot feature representation with a mask\n",
    "def prep_batch_for_network(batch_of_sentences, include_targets=False):\n",
    "    sentence_max_length = np.array( [ len(w) for w in batch_of_sentences ]).max()\n",
    "    \n",
    "    # translate into one-hot matrix, mask values and targets\n",
    "    input_values = np.zeros((len(batch_of_sentences), sentence_max_length, EMBEDDING_DIM), dtype='float32')\n",
    "    mask_values  = np.zeros((len(batch_of_sentences), sentence_max_length), dtype='float32')\n",
    "    \n",
    "    for i, sent in enumerate(batch_of_sentences):\n",
    "      for j, word in enumerate(sent):\n",
    "        input_values[i,j] = get_embedding_vec(word) # this is word.lower() in dictionary\n",
    "      mask_values[i, 0:len(sent) ] = 1.\n",
    "\n",
    "    if not include_targets:\n",
    "        return input_values, mask_values        \n",
    "    \n",
    "    target_values  = np.zeros((len(batch_of_sentences), sentence_max_length), dtype='int32')\n",
    "    for i, sent in enumerate(batch_of_sentences):\n",
    "        sentence_tags = pos_tagger.tag(sent)\n",
    "        for j, word_tag in enumerate(sentence_tags):\n",
    "            target_values[i,j] = pos_tagger_to_idx[word_tag[1]]  # tags are returned as tuples (word, tag)\n",
    "    \n",
    "    return input_values, mask_values, target_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the batchifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep_batch_for_network([\"Mr. Smith works at Red Cat Labs .\".split(' ')], include_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Symbolically\n",
    "\n",
    "#### Lasagne RNN tutorial (including conventions &amp; rationale)\n",
    "\n",
    "*  http://colinraffel.com/talks/hammer2015recurrent.pdf\n",
    "\n",
    "#### Lasagne Examples\n",
    "\n",
    "*  https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/recurrent.py\n",
    "*  https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variables for input. In addition to the usual features and target, we need a mask\n",
    "rnn_input_sym = theano.tensor.tensor3()\n",
    "rnn_mask_sym  = theano.tensor.matrix()\n",
    "\n",
    "rnn_words_target_sym = theano.tensor.imatrix() # part-of-speech generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_input = lasagne.layers.InputLayer( (None, None, RNN_HIDDEN_SIZE) )  # batch_size, sequence_len, embedding_dim\n",
    "rnn_mask  = lasagne.layers.InputLayer( (None, None, RNN_HIDDEN_SIZE) )  # batch_size, sequence_len, embedding_dim\n",
    "\n",
    "n_batch, n_time_steps, n_features = rnn_input_sym.shape\n",
    "\n",
    "rnn_layer_f = lasagne.layers.GRULayer(rnn_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=rnn_mask,\n",
    "                only_return_final=False, # Need all of the output states\n",
    "            )\n",
    "\n",
    "rnn_layer_b = lasagne.layers.GRULayer(rnn_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=rnn_mask,\n",
    "                only_return_final=False, # Need all of the output states\n",
    "                backwards=True,\n",
    "            )\n",
    "\n",
    "# Before the decoder layer, we need to reshape the sequence into the batch dimension,\n",
    "# so that timesteps are decoded independently.\n",
    "rnn_reshape_f = lasagne.layers.ReshapeLayer(rnn_layer_f, (-1, RNN_HIDDEN_SIZE) )\n",
    "rnn_reshape_b = lasagne.layers.ReshapeLayer(rnn_layer_b, (-1, RNN_HIDDEN_SIZE) )\n",
    "\n",
    "# Now concatenate them\n",
    "rnn_concat = lasagne.layers.ConcatLayer([rnn_reshape_f, rnn_reshape_b])\n",
    "\n",
    "# Convert them into softmax outputs\n",
    "rnn_tag_val = lasagne.layers.DenseLayer( rnn_concat, num_units=TAG_SET_SIZE, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# And reshape them, so that they are in the original batches-of-sentences shape\n",
    "rnn_out = lasagne.layers.ReshapeLayer(rnn_tag_val, (-1, n_time_steps, TAG_SET_SIZE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, the output stage - this is for the training (over all the words in the sentences)\n",
    "rnn_output = lasagne.layers.get_output(rnn_out, \n",
    "                {\n",
    "                 rnn_input: rnn_input_sym, \n",
    "                 rnn_mask: rnn_mask_sym, \n",
    "                }\n",
    "            )\n",
    "\n",
    "# We flatten the sequence into the batch dimension before calculating the loss\n",
    "def rnn_word_cross_ent(net_output, targets):\n",
    "    preds = theano.tensor.reshape(net_output, (-1, TAG_SET_SIZE))\n",
    "    targets_flat = theano.tensor.flatten(targets)\n",
    "    cost = theano.tensor.nnet.categorical_crossentropy(preds, targets_flat)\n",
    "    return cost\n",
    "\n",
    "rnn_loss = rnn_word_cross_ent(rnn_output, rnn_words_target_sym).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and the Training and Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For stability during training, gradients are clipped and a total gradient norm constraint is also used\n",
    "#MAX_GRAD_NORM = 15\n",
    "\n",
    "rnn_params = lasagne.layers.get_all_params(rnn_out, trainable=True)\n",
    "\n",
    "rnn_grads = theano.tensor.grad(rnn_loss, rnn_params)\n",
    "#rnn_grads = [theano.tensor.clip(g, -GRAD_CLIP_BOUND, GRAD_CLIP_BOUND) for g in rnn_grads]\n",
    "#rnn_grads, rnn_norm = lasagne.updates.total_norm_constraint( rnn_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "rnn_updates = lasagne.updates.adam(rnn_grads, rnn_params)\n",
    "\n",
    "rnn_train = theano.function([rnn_input_sym, rnn_words_target_sym, rnn_mask_sym],\n",
    "                [rnn_loss],\n",
    "                updates=rnn_updates,\n",
    "            )\n",
    "\n",
    "rnn_predict = theano.function([rnn_input_sym, rnn_mask_sym], [rnn_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0, iterations_complete = time.time(), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1, iterations_recent = time.time(), iterations_complete\n",
    "epochs=1000\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    sentences = batch_sentences()\n",
    "    rnn_input_values, rnn_mask_values, rnn_target_values_int = prep_batch_for_network(sentences, include_targets=True)\n",
    "    \n",
    "    # Now train the RNN\n",
    "    rnn_loss_, = rnn_train(rnn_input_values, rnn_target_values_int, rnn_mask_values)\n",
    "\n",
    "    iterations_complete += 1\n",
    "\n",
    "    if iterations_complete % 10 == 0:\n",
    "        secs_per_batch = float(time.time() - t1)/ (iterations_complete - iterations_recent)\n",
    "        eta_in_secs = secs_per_batch*(epochs-epoch_i)\n",
    "        print(\"Iteration {:5d}, loss_train: {:.4f} ({:.1f}s per 1000 batches)  eta: {:.0f}m{:02.0f}s\".format(\n",
    "                iterations_complete, float(rnn_loss_), \n",
    "                secs_per_batch*1000., eta_in_secs/60, eta_in_secs % 60) \n",
    "             )\n",
    "        #print('Iteration {}, output: {}'.format(iteration, disc_output_, ))  # , output: {}\n",
    "        t1, iterations_recent = time.time(), iterations_complete\n",
    "\n",
    "print('Iteration {}, ran in {:.1f}sec'.format(iterations_complete, float(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Uncomment the ```pickle.dump()``` to actually save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_param_values = lasagne.layers.get_all_param_values(rnn_out)\n",
    "rnn_param_dictionary = dict(\n",
    "     params = rnn_param_values,\n",
    "     iterations_complete=iterations_complete,\n",
    "    )\n",
    "pickle.dump(rnn_param_dictionary, open('../data/RNN/tagger_rnn_trained.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_param_dictionary = pickle.load(open('../data/RNN/tagger_rnn_trained_64x310k.pkl', 'r'))\n",
    "lasagne.layers.set_all_param_values(rnn_out, rnn_param_dictionary['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tagger Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sentences = batch_sentences()\n",
    "test_sentences[0] = \"Dr. Andrews works at Red Cat Labs .\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_input_values, rnn_mask_values, rnn_target_values_int = prep_batch_for_network(test_sentences, include_targets=True)\n",
    "\n",
    "rnn_output_, = rnn_predict(rnn_input_values, rnn_mask_values)\n",
    "\n",
    "# rnn_output_ here is a softmax-vector at every word location\n",
    "for i,sent in enumerate(test_sentences[0:5]):\n",
    "    annotated = [ \n",
    "            \"%s-%d-%d\" % (word, rnn_target_values_int[i,j], np.argmax(rnn_output_[i,j]), )    \n",
    "            for j,word in enumerate(sent) \n",
    "        ]\n",
    "    print(' '.join(annotated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Make the tagger identify different PoS (say : 'verbs')\n",
    "\n",
    "2.  Make the tagger return several different tags instead\n",
    "\n",
    "3.  See whether more advanced 'LSTM' nodes would improve the scores\n",
    "\n",
    "4.  Add a special 'is_uppercase' element to the embedding vector (or, more simply, just replace one of the elements with an indicator).  Does this help the NNP accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
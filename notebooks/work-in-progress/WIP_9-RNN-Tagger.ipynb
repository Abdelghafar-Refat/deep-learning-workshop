{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagger\n",
    "\n",
    "This example trains a RNN to tag words from a corpus - \n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n",
    "\n",
    "\n",
    "Adversarial networks : http://carpedm20.github.io/faces/\n",
    "\n",
    "Doing this with RNNs may be pretty novel : https://www.quora.com/Can-generative-adversarial-networks-be-used-in-sequential-data-in-recurrent-neural-networks-How-effective-would-they-be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "#import pickle\n",
    "#import gzip\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32\n",
    "EMBEDDING_DIM=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentence_splitter.tokenize(\"This is Mr. Smith's tokenize test. The U.S.A gives us sent two. Is this sent three?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"This is Mr. Smith's tokenize test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An interesting corpus :\n",
    "corpus_text_file = '../data/RNN/en.wikipedia.2010.100K.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, 'rt') as f:\n",
    "            for line in f.readlines():\n",
    "                #print(line)\n",
    "                n,l = line.decode(\"utf-8\").split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    yield tokenizer.tokenize(s)\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pos_tagger = PerceptronTagger(load=True)\n",
    "' | '.join(list(pos_tagger.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Do part of speech on this sample text .\".split(' ')\n",
    "#s = next(corpus_sentence_tokens_gen)\n",
    "pos_tagger.tag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twist : Not interested in all classes...\n",
    "\n",
    "To simplify (dramatically), let's just look at 'is ordinary word' and 'is entity name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list = 'O E'.split(' ')\n",
    "pos_tagger_entity_tags = set('NNP'.split(' '))\n",
    "pos_tagger_to_idx   = dict([ (t,(1 if t in pos_tagger_entity_tags else 0)) for i,t in enumerate(pos_tagger.classes)])\n",
    "TAG_SET_SIZE= len(tag_list)\n",
    "\n",
    "pos_tagger_to_idx['NNP'], pos_tagger_to_idx['VBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings\n",
    "Using the python package :  https://github.com/maciejkula/glove-python , and code samples from : http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
    "\n",
    "### Create the Co-occurrence Matrix\n",
    "This should take 30 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glove\n",
    "corpus_sentences = [ \n",
    "        [ w.lower() for w in next(corpus_sentence_tokens_gen)] # All lower-case\n",
    "        for _ in range(0,100*1000) \n",
    "    ]\n",
    "\n",
    "# Fit the co-occurrence matrix using a sliding window of 10 words.\n",
    "glove_corpus = glove.Corpus()\n",
    "\n",
    "t0 = time.time()\n",
    "glove_corpus.fit(corpus_sentences, window=10)\n",
    "print(\"Co-occurrence calculated in %5.1fsec\" % (time.time()-t0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return the index of the word in the dictionary\n",
    "glove_corpus.dictionary['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the Word Embedding\n",
    "\n",
    "This will make use of up to 4 threads - and each epoch takes 20-30 seconds on a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding = glove.Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)\n",
    "t0 = time.time()\n",
    "glove_epochs, glove_threads = 20, 4 \n",
    "word_embedding.fit(glove_corpus.matrix, epochs=glove_epochs, no_threads=glove_threads, verbose=True)\n",
    "print(\"Word-embedding created in %5.1fsec = %5.1fsec per epoch\" % (\n",
    "        (time.time()-t0), (time.time()-t0)/glove_epochs*glove_threads, ))\n",
    "\n",
    "# Add the word -> id dictionary to the model to allow similarity queries.\n",
    "word_embedding.add_dictionary(glove_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding.save(\"../data/RNN/glove.embedding.64.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Alternative :: Load a pre-trained word embedding\n",
    "#word_embedding = glove.Glove.load(\"../data/RNN/glove.embedding.64.pretrained.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word-similarity test\n",
    "word_embedding.most_similar('island')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word-analogy test\n",
    "def get_embedding_vec(word):\n",
    "    idx = glove_corpus.dictionary[word.lower()]\n",
    "    return word_embedding.word_vectors[idx]\n",
    "\n",
    "def get_closest_word(vec, number=4):\n",
    "    dst = (np.dot(word_embedding.word_vectors, vec)\n",
    "                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n",
    "                   / np.linalg.norm(vec))\n",
    "    word_ids = np.argsort(-dst)\n",
    "    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n",
    "            if x in word_embedding.inverse_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analogy_vec = get_embedding_vec('man') - get_embedding_vec('woman') + get_embedding_vec('king')\n",
    "#analogy_vec = get_embedding_vec('paris') - get_embedding_vec('france') + get_embedding_vec('italy')\n",
    "#analogy_vec = get_embedding_vec('laugh') - get_embedding_vec('happy') + get_embedding_vec('sad')\n",
    "#analogy_vec = get_embedding_vec('kitten') - get_embedding_vec('cat') + get_embedding_vec('dog')\n",
    "get_closest_word(analogy_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Trigram frequency distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate base-line scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = EMBEDDING_DIM + 1  # 1 for capitalisation flag\n",
    "GRAD_CLIP_BOUND = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN Part-of-Speech Tagger\n",
    "\n",
    "\n",
    "### Create Training / Testing dataset\n",
    "And a 'batch generator' function that delivers data in the right format for RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO!\n",
    "\n",
    "def batch_dictionary(size=BATCH_SIZE/2):\n",
    "    uniform_vars = np.random.uniform( size=(size,) )\n",
    "    idx = freqs_cum.searchsorted(uniform_vars)\n",
    "    return wordsnp[ idx ].tolist()\n",
    "    \n",
    "def batch_bigram(size=BATCH_SIZE/2):\n",
    "    return [ bigram_word()[0:WORD_LENGTH_MAX] for i in range(size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Our batch generator will yield BATCH_SIZE/2 words, with their correct classification\n",
    "#def data_batch_generator(corpus, size=BATCH_SIZE):\n",
    "#    startidx = np.random.randint(0, len(corpus) - SEQUENCE_LENGTH - 1, size=size)\n",
    "#\n",
    "#    while True:\n",
    "#        items = np.array([corpus[start:start + SEQUENCE_LENGTH + 1] for start in startidx])\n",
    "#        startidx = (startidx + SEQUENCE_LENGTH) % (len(corpus) - SEQUENCE_LENGTH - 1)\n",
    "#        yield items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test it out\n",
    "#batch_test = lambda : batch_dictionary(size=4)\n",
    "batch_test = lambda : batch_bigram(size=4)\n",
    "print(batch_test())\n",
    "print(batch_test())\n",
    "print(batch_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasagne RNN tutorial (including conventions &amp; rationale)\n",
    "\n",
    "*  http://colinraffel.com/talks/hammer2015recurrent.pdf\n",
    "\n",
    "#### Lasagne Examples\n",
    "\n",
    "*  https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/recurrent.py\n",
    "*  https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After sampling a data batch, we transform it into a one hot feature representation with a mask\n",
    "def prep_batch_for_network(batch_of_words):\n",
    "    word_max_length = np.array( [ len(w) for w in batch_of_words ]).max()\n",
    "    \n",
    "    # translate into one-hot matrix, mask values and targets\n",
    "    input_values = np.zeros((len(batch_of_words), word_max_length, CHARS_SIZE), dtype='float32')\n",
    "    mask_values  = np.zeros((len(batch_of_words), word_max_length), dtype='float32')\n",
    "    \n",
    "    for i, word in enumerate(batch_of_words):\n",
    "      for j, c in enumerate(word):\n",
    "        input_values[i,j] = CHAR_TO_ONEHOT[ c ]\n",
    "      mask_values[i, 0:len(word) ] = 1.\n",
    "\n",
    "    return input_values, mask_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Symbolically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variables for input. In addition to the usual features and target,\n",
    "rnn_input_sym = theano.tensor.tensor3()\n",
    "rnn_mask_sym  = theano.tensor.matrix()\n",
    "\n",
    "rnn_words_target_sym = theano.tensor.imatrix() # part-of-speech generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_input = lasagne.layers.InputLayer( (None, None, RNN_HIDDEN_SIZE) )  # batch_size, sequence_len, embedding_dim\n",
    "rnn_mask  = lasagne.layers.InputLayer( (None, None, RNN_HIDDEN_SIZE) )  # batch_size, sequence_len, embedding_dim\n",
    "\n",
    "n_batch, n_time_steps, n_features = rnn_input_sym.shape\n",
    "\n",
    "rnn_layer_f = lasagne.layers.GRULayer(rnn_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=rnn_mask,\n",
    "                only_return_final=False, # Need all of the output states\n",
    "            )\n",
    "\n",
    "rnn_layer_b = lasagne.layers.GRULayer(rnn_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=rnn_mask,\n",
    "                only_return_final=False, # Need all of the output states\n",
    "                backwards=True,\n",
    "            )\n",
    "\n",
    "# Before the decoder layer, we need to reshape the sequence into the batch dimension,\n",
    "# so that timesteps are decoded independently.\n",
    "rnn_reshape_f = lasagne.layers.ReshapeLayer(rnn_layer_f, (-1, RNN_HIDDEN_SIZE) )\n",
    "rnn_reshape_b = lasagne.layers.ReshapeLayer(rnn_layer_b, (-1, RNN_HIDDEN_SIZE) )\n",
    "\n",
    "# Now concatenate them\n",
    "rnn_concat = lasagne.layers.ConcatLayer([rnn_reshape_f, rnn_reshape_b])\n",
    "\n",
    "# Convert them into softmax outputs\n",
    "rnn_tags = lasagne.layers.DenseLayer( rnn_concat, num_units=TAG_SET_SIZE, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# And reshape them, so that they are in the original batches-of-sentences shape\n",
    "rnn_out = lasagne.layers.ReshapeLayer(rnn_tags, (-1, n_time_steps, RNN_HIDDEN_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, the output stage - this is for the training (over all the letters in the words)\n",
    "rnn_output = lasagne.layers.get_output(rnn_out, \n",
    "                {\n",
    "                 rnn_input: rnn_input_sym, \n",
    "                 rnn_mask: rnn_mask_sym, \n",
    "                }\n",
    "            )\n",
    "\n",
    "# We flatten the sequence into the batch dimension before calculating the loss\n",
    "def rnn_word_cross_ent(net_output, targets):\n",
    "    preds = theano.tensor.reshape(net_output, (-1, TAGS_SIZE))\n",
    "    targets_flat = theano.tensor.flatten(targets)\n",
    "    cost = theano.tensor.nnet.categorical_crossentropy(preds, targets_flat)\n",
    "    return cost\n",
    "\n",
    "rnn_loss = rnn_word_cross_ent(rnn_output, rnn_words_target_sym).mean()\n",
    "\n",
    "lasagne.objectives.categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and the Training and Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For stability during training, gradients are clipped and a total gradient norm constraint is also used\n",
    "#MAX_GRAD_NORM = 15\n",
    "\n",
    "rnn_params = lasagne.layers.get_all_params(rnn_out, trainable=True)\n",
    "\n",
    "rnn_grads = theano.tensor.grad(rnn_loss, rnn_params)\n",
    "#rnn_grads = [theano.tensor.clip(g, -GRAD_CLIP_BOUND, GRAD_CLIP_BOUND) for g in rnn_grads]\n",
    "#rnn_grads, rnn_norm = lasagne.updates.total_norm_constraint( rnn_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "rnn_updates = lasagne.updates.adam(rnn_grads, rnn_params)\n",
    "\n",
    "rnn_train = theano.function([rnn_input_sym, rnn_words_target_sym, rnn_mask_sym],\n",
    "                [rnn_loss],\n",
    "                updates=rnn_updates,\n",
    "            )\n",
    "\n",
    "rnn_predict = theano.function([rnn_input_sym, rnn_mask_sym], [rnn_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO :: Add the training stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Uncomment the ```pickle.dump()``` to actually save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_param_values = lasagne.layers.get_all_param_values(disc_final)\n",
    "rnn_param_dictionary = dict(\n",
    "     params = rnn_param_values,\n",
    "     CHARS_VALID = CHARS_VALID, \n",
    "     CHAR_TO_IX = CHAR_TO_IX,\n",
    "     IX_TO_CHAR = IX_TO_CHAR,\n",
    "    )\n",
    "#pickle.dump(rnn_param_dictionary, open('../data/RNN/tagger_rnn_trained.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_param_dictionary = pickle.load(open('../data/RNN/tagger_rnn_trained_64x310k.pkl', 'r'))\n",
    "lasagne.layers.set_all_param_values(rnn_out, rnn_param_dictionary['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tagger Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text_list = [\"shape\", \"shast\", \"shaes\", \"shafg\", \"shaqw\"]\n",
    "test_text_list = [\"opposite\", \"aposite\", \"apposite\", \"xposite\", \"rrwqsite\", \"deposit\", \"idilic\", \"idyllic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_input_values, disc_mask_values = prep_batch_for_network(test_text_list)\n",
    "\n",
    "disc_output_, = disc_predict(disc_input_values, disc_mask_values)\n",
    "\n",
    "for i,v in enumerate(disc_output_.tolist()):\n",
    "    print(\"%s : %5.2f%%\" % ((test_text_list[i]+' '*20)[:20], v[0]*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NOTHING BELOW IS USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Generative Network to create sample words\n",
    "\n",
    "The network above can be used to generate text...\n",
    "\n",
    "The following set-up allows for the output of the RNN at each timestep to be mixed with the letter frequency that the bigram model would suggest - in a proportion ```bigram_overlay``` which can vary from ```0``` (being solely RNN derived) to ```1.0``` (being solely bigram frequencies, with the RNN output being disregarded). \n",
    "\n",
    "The input is a 'random field' matrix that is used to chose each letter in each slot from the generated probability distribution.\n",
    "\n",
    "Once a space is output for a specific word, then it stops being extended (equivalently, the mask is set to zero going forwards).\n",
    "\n",
    "Once spaces have been observed for all words (or the maximum length reached), the process ends, and a list of the created words is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's pre-calculate the logs of the bigram frequencies, since they may be mixed in below\n",
    "bigram_min_freq = 1e-10 # To prevent underflow in log...\n",
    "bigram_freq_log = np.log( bigram_freq + bigram_min_freq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_rnn_words(random_field, bigram_overlay=0.0):\n",
    "    batch_size, max_word_length  = random_field.shape\n",
    "    \n",
    "    idx_spc = CHAR_TO_IX[' ']\n",
    "    def append_indices_as_chars(words_current, idx_list):\n",
    "        for i, idx in enumerate(idx_list):\n",
    "            if idx == idx_spc:\n",
    "                pass # Words end at space\n",
    "                #words_current[i] += 'x'\n",
    "            else:\n",
    "                words_current[i] += IX_TO_CHAR[idx]\n",
    "        return words_current\n",
    "    \n",
    "    # Create a 'first character' by using the bigram transitions from 'space' (this is fair)\n",
    "    idx_initial = [ np.searchsorted(bigram_freq_cum[idx_spc], random_field[i, 0]) for i in range(batch_size) ]\n",
    "    bigram_freq_log_current = bigram_freq_log[ np.array(idx_initial) , :]\n",
    "    \n",
    "    words_current = [ '' for _ in range(batch_size) ]\n",
    "    words_current = append_indices_as_chars(words_current, idx_initial)\n",
    "    \n",
    "    \n",
    "    col, finished=1, False\n",
    "    while not finished:\n",
    "        gen_input_values, gen_mask_values = prep_batch_for_network(words_current)\n",
    "        #print(gen_mask_values[:,-1])\n",
    "        gen_out_, = gen_predict(gen_input_values, gen_mask_values)\n",
    "        \n",
    "        gen_freq = np.exp( # This is going to look like softmax...\n",
    "            gen_out_*(1.0-bigram_overlay) + bigram_freq_log_current*bigram_overlay \n",
    "        )  \n",
    "        \n",
    "        # This output is the final probability[CHARS_SIZE], so let's cumsum it, etc.\n",
    "        gen_prob = gen_freq / gen_freq.sum(axis=1)[:, np.newaxis] # Trick to enable unflattening of sum()\n",
    "        gen_prob_cum = gen_prob.cumsum(axis=1)\n",
    "        \n",
    "        idx_next = [ # Only add extra letters if we haven't already passed a space (i.e. mask[-1]==0)\n",
    "            idx_spc if gen_mask_values[i,-1]==0 else np.searchsorted(gen_prob_cum[i], random_field[i, col]) \n",
    "            for i in range(batch_size) \n",
    "        ]\n",
    "        \n",
    "        bigram_freq_log_current = bigram_freq_log[ np.array(idx_next) , :]\n",
    "        words_current = append_indices_as_chars(words_current, idx_next)\n",
    "        \n",
    "        words_current_max_length = np.array( [ len(w) for w in words_current ]).max()\n",
    "        \n",
    "        # If the words have reached the maximum length, or we didn't extend any of them...\n",
    "        if words_current_max_length>=max_word_length or words_current_max_length<col:\n",
    "            finished=True \n",
    "        \n",
    "        col += 1\n",
    "\n",
    "    return words_current\n",
    "        \n",
    "# Create a probability distribution across all potential positions in the output 'field'\n",
    "random_field = np.random.uniform( size=(BATCH_SIZE, WORD_LENGTH_MAX) )\n",
    "\n",
    "gen_words_output = generate_rnn_words(random_field, bigram_overlay=0.0)\n",
    "\n",
    "print( '\\n'.join(gen_words_output))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the initial (random) Network State \n",
    "\n",
    "This will come in handy when we need to reset the network back to 'untrained' later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_param_values_initial = lasagne.layers.get_all_param_values(gen_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, train the Generator RNN based on the Dictionary itself\n",
    "\n",
    "Once we have an output word, let's reward the RNN based on a specific training signal.  We'll encapsulate the training in a function that takes the input signal as a parameter, so that we can try other training schemes (later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_good_output_dictionary(output_words):\n",
    "    return np.array(\n",
    "        [ (1.0 if w in wordset else 0.0) for w in output_words ],\n",
    "        dtype='float32'\n",
    "    )\n",
    "\n",
    "t0, iterations_complete = time.time(), 0\n",
    "def reset_generative_network():\n",
    "    global t0, iterations_complete\n",
    "    t0, iterations_complete = time.time(), 0\n",
    "    lasagne.layers.set_all_param_values(gen_prob, gen_param_values_initial)\n",
    "\n",
    "def prep_batch_for_network_output(mask_values, batch_of_words):\n",
    "    output_indices = np.zeros(mask_values.shape, dtype='int32')\n",
    "\n",
    "    for i, word in enumerate(batch_of_words):\n",
    "      word_shifted = word[1:]+' '\n",
    "      for j, c in enumerate(word_shifted):\n",
    "        output_indices[i,j] = CHAR_TO_IX[ c ]\n",
    "\n",
    "    return output_indices\n",
    "    \n",
    "def train_generative_network(is_good_output_function=is_good_output_dictionary, epochs=10*1000, bigram_overlay=0.0):\n",
    "    global t0, iterations_complete\n",
    "    t1, iterations_recent = time.time(), iterations_complete\n",
    "    for epoch_i in range(epochs):\n",
    "        random_field = np.random.uniform( size=(BATCH_SIZE, WORD_LENGTH_MAX) )\n",
    "\n",
    "        gen_words_output = generate_rnn_words(random_field, bigram_overlay=bigram_overlay)\n",
    "        \n",
    "        # So, now we have a set of words, convert to 'goodness'\n",
    "        is_good_output = is_good_output_function(gen_words_output)\n",
    "        \n",
    "        # Now, create a training set of input -> output, coupled with an intensity signal\n",
    "        #   first the step-by-step network inputs\n",
    "        gen_input_values, gen_mask_values = prep_batch_for_network(gen_words_output)\n",
    "        \n",
    "        #   now create step-by-step network outputs (strip off first character, add spaces)\n",
    "        \n",
    "        #gen_output_values, _ = prep_batch_for_network([ (w[1:]+' ') for w in gen_words_output ])\n",
    "        #gen_output_values_int = np.array(gen_output_values, dtype='int32')\n",
    "        \n",
    "        gen_output_values_int = prep_batch_for_network_output(gen_mask_values, gen_words_output)\n",
    "        \n",
    "        #  now create step-by-step network outputs (strip off first character, add spaces) as *indicies*\n",
    "        \n",
    "        #   and the 'error signal' is simply (+ve for good, -ve for bad):\n",
    "        target_valid = (np.array(is_good_output) - 0.5).reshape( (len(gen_words_output),1) )\n",
    "        \n",
    "        # Now train the generator RNN\n",
    "        gen_loss_, = gen_train(gen_input_values, gen_output_values_int, target_valid, gen_mask_values)\n",
    "\n",
    "        iterations_complete += 1\n",
    "\n",
    "        if iterations_complete % 10 == 0:\n",
    "            secs_per_batch = float(time.time() - t1)/ (iterations_complete - iterations_recent)\n",
    "            eta_in_secs = secs_per_batch*(epochs-epoch_i)\n",
    "            print(\"Iteration {:5d}, loss_train: {:.4f} ({:.1f}s per 1000 batches)  eta: {:.0f}m{:02.0f}s\".format(\n",
    "                    iterations_complete, float(disc_loss_), \n",
    "                    secs_per_batch*1000., eta_in_secs/60, eta_in_secs % 60) \n",
    "                 )\n",
    "            #print('Iteration {}, output: {}'.format(iteration, disc_output_, ))  # , output: {}\n",
    "            t1, iterations_recent = time.time(), iterations_complete\n",
    "\n",
    "    print('Iteration {}, ran in {:.1f}sec'.format(iterations_complete, float(time.time() - t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_generative_network()\n",
    "train_generative_network(is_good_output_function=is_good_output_dictionary, epochs=10*1000, bigram_overlay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, Produce some text\n",
    "We will use random sentences from the validation corpus to 'prime' the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primers = val_corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed characters one at a time from the priming sequence into the network.\n",
    "\n",
    "To obtain a sample string, at each timestep we sample from the output probability distribution, and feed the chosen character back into the network. We terminate after the first linebreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = ''\n",
    "hid = np.zeros((1, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "hid2 = np.zeros((1, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "x = np.zeros((1, 1, VOCAB_SIZE), dtype='float32')\n",
    "\n",
    "primer = np.random.choice(primers) + '\\n'\n",
    "\n",
    "for c in primer:\n",
    "    p, hid, hid2 = predict_fn(x, hid, hid2)\n",
    "    x[0, 0, :] = CHAR_TO_ONEHOT[c]\n",
    "    \n",
    "for _ in range(500):\n",
    "    p, hid, hid2 = predict_fn(x, hid, hid2)\n",
    "    p = p/(1 + 1e-6)\n",
    "    s = np.random.multinomial(1, p)\n",
    "    sentence += IX_TO_CHAR[s.argmax(-1)]\n",
    "    x[0, 0, :] = s\n",
    "    if sentence[-1] == '\\n':\n",
    "        break\n",
    "        \n",
    "print('PRIMER: ' + primer)\n",
    "print('GENERATED: ' + sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=====\n",
    "\n",
    "1. Implement sampling using the \"temperature softmax\": $$p(i) = \\frac{e^{\\frac{z_i}{T}}}{\\Sigma_k e^{\\frac{z_k}{T}}}$$\n",
    "\n",
    "This generalizes the softmax with a parameter $T$ which affects the \"sharpness\" of the distribution. Lowering $T$ will make samples less error-prone but more repetitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load spoilers/tempsoftmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
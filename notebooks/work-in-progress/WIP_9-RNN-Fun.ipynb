{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Character Model + Lots More\n",
    "\n",
    "This example trains a RNN to create plausible words from a corpus.  But it includes lots of interesting \"bells and whistles\"\n",
    "\n",
    "The data used for training is one of :\n",
    "  *  a vocabulary/dictionary collected from the 1-Billion-Word Corpus\n",
    "  *  a list of Indian names (voters rolls, by year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import lasagne\n",
    "#from lasagne.utils import floatX\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load an interesting corpus :\n",
    "\n",
    "#corpus = gzip.open('./data/RNN/claims.txt.gz').read()\n",
    "#corpus = gzip.open('./data/RNN/Shakespeare.plays.txt.gz').read()\n",
    "#corpus = gzip.open('./data/RNN/Shakespeare.poetry.txt.gz').read()\n",
    "\n",
    "with open('../data/RNN/ALL_1-vocab.txt','rt') as f:\n",
    "    lines = [ l.strip().lower().split() for l in f.readlines() ]\n",
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corpus.split('\\n')[0]\n",
    "# Here are our characters : '[a-z\\- ]'\n",
    "import re\n",
    "invalid_chars = r'[^a-z\\- ]'\n",
    "lines_valid = [ l for l in lines if not re.search(invalid_chars, l[0]) ]\n",
    "#lines_valid = lines_valid[0:50000]\n",
    "lines_valid[0:10], len(lines_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /usr/share/dict/linux.words\n",
    "with open('/usr/share/dict/linux.words','rt') as f:\n",
    "    linux_words = [ l.strip() for l in f.readlines() ]\n",
    "linux_wordset = set(linux_words)\n",
    "#'united' in wordset\n",
    "lines_filtered = [l for l in lines_valid \n",
    "                     if len(l[0])>=3               # Require each word to have 3 or more characters\n",
    "                        and l[0] in linux_wordset  # Require each word to be found in regular dictionary\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split apart the words and their frequencies (Assume these are in sorted order, at least initial few)\n",
    "words = [ l[0] for l in lines_filtered ]\n",
    "wordset = set(words)\n",
    "wordsnp = np.array(words)\n",
    "freqs_raw = np.array( [ int(l[1]) for l in lines_filtered ] )\n",
    "\n",
    "freq_tot = float(freqs_raw.sum())\n",
    "\n",
    "# Frequency weighting adjustments\n",
    "freqs = freqs_raw / freq_tot\n",
    "\n",
    "cutoff_index = 30   # All words with highter frequencies will be 'limited' at this level\n",
    "freqs[0:cutoff_index] = freqs[cutoff_index]\n",
    "\n",
    "freqs = freqs / freqs.sum()\n",
    "freqs[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_cum = np.array( [.1, .5, .9, 1.0] )\n",
    "test_cum.searchsorted([ .05, 0.45, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cumulative frequency, so that we can efficiently pick weighted random words...\n",
    "#   using http://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html\n",
    "freqs_cum = freqs.cumsum()\n",
    "freqs_cum[:10], freqs_cum[-10:], "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters from Corpus\n",
    "Find the set of characters used in the corpus and construct mappings between characters, integer indices, and one hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHARS_VALID = \"abcdefghijklmnopqrstuvwxyz- \"\n",
    "CHARS_SIZE  = len(CHARS_VALID)\n",
    "\n",
    "CHAR_TO_IX = {c: i for i, c in enumerate(CHARS_VALID)}\n",
    "IX_TO_CHAR = {i: c for i, c in enumerate(CHARS_VALID)}\n",
    "CHAR_TO_ONEHOT = {c: np.eye(CHARS_SIZE)[i] for i, c in enumerate(CHARS_VALID)}\n",
    "#CHAR_TO_IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Unigram frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single letter frequencies\n",
    "unigram_freq = np.zeros( (CHARS_SIZE,))\n",
    "idx_end = CHAR_TO_IX[' ']\n",
    "for i,w in enumerate(words):\n",
    "    word_freq = freqs[i]\n",
    "    for c in w:\n",
    "        unigram_freq[ CHAR_TO_IX[c] ] += word_freq\n",
    "    unigram_freq[ idx_end ] += word_freq\n",
    "unigram_freq /= unigram_freq.sum()\n",
    "unigram_freq_cum = unigram_freq.cumsum()\n",
    "[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(unigram_freq.tolist()) ]\n",
    "#CHARS_VALID[ unigram_freq_cum.searchsorted(0.20) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unigram_word():\n",
    "    s=[]\n",
    "    while True:\n",
    "        idx = np.searchsorted(unigram_freq_cum, np.random.uniform())\n",
    "        c = IX_TO_CHAR[idx]\n",
    "        if c==' ':\n",
    "            if len(s)>0:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        s.append(c)\n",
    "    return ''.join(s)\n",
    "' '.join([ unigram_word() for i in range(0,20) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bigram frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# two-letter frequencies\n",
    "bigram_freq = np.zeros( (CHARS_SIZE,CHARS_SIZE) )\n",
    "for i,w in enumerate(words):\n",
    "    w2 = ' '+w+' '\n",
    "    word_freq = freqs[i]\n",
    "    for j in range(0, len(w2)-1):\n",
    "        bigram_freq[ CHAR_TO_IX[ w2[j] ], CHAR_TO_IX[ w2[j+1] ] ] += word_freq\n",
    "#[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(bigram_freq[ CHAR_TO_IX['q'] ].tolist()) ]\n",
    "#bigram_freq.sum(axis=1)[CHAR_TO_IX['q']]\n",
    "bigram_freq /= bigram_freq.sum(axis=1)[:, np.newaxis] # Trick to enable unflattening of sum()\n",
    "bigram_freq_cum = bigram_freq.cumsum(axis=1)\n",
    "#[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(bigram_freq_cum[ CHAR_TO_IX['q'] ].tolist()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bigram_freq.sum(axis=1)[CHAR_TO_IX['q']]\n",
    "#(bigram_freq/ bigram_freq.sum(axis=1)).sum(axis=0)\n",
    "#bigram_freq.sum(axis=1)[CHAR_TO_IX['q']]\n",
    "#bigram_freq[CHAR_TO_IX['q'], :].sum()\n",
    "#(bigram_freq / bigram_freq.sum(axis=1)[:, np.newaxis]).cumsum(axis=1)\n",
    "#Letter relative frequency for letters following 'q'\n",
    "[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(bigram_freq[ CHAR_TO_IX['q'] ].tolist()) if f>0.001]\n",
    "#bigram_freq_cum[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigram_word():\n",
    "    s=[]\n",
    "    idx_last = CHAR_TO_IX[' ']\n",
    "    while True:\n",
    "        idx = np.searchsorted(bigram_freq_cum[idx_last], np.random.uniform())\n",
    "        c = IX_TO_CHAR[idx]\n",
    "        if c==' ':\n",
    "            if len(s)>0:\n",
    "                #if len(s)<50: continue\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        s.append(c)\n",
    "        idx_last=idx\n",
    "    return ''.join(s)\n",
    "' '.join([ bigram_word() for i in range(0,20) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Trigram frequency distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Three-letter frequencies\n",
    "trigram_freq = np.zeros( (CHARS_SIZE,CHARS_SIZE,CHARS_SIZE) )\n",
    "for i,w in enumerate(words):\n",
    "    w3 = '  '+w+'  '\n",
    "    word_freq = freqs[i]\n",
    "    for j in range(0, len(w3)-2):\n",
    "        trigram_freq[ CHAR_TO_IX[ w3[j] ], CHAR_TO_IX[ w3[j+1] ], CHAR_TO_IX[ w3[j+2] ] ] += word_freq\n",
    "trigram_freq /= trigram_freq.sum(axis=2)[:, :, np.newaxis] # Trick to enable unflattening of sum()\n",
    "trigram_freq_cum = trigram_freq.cumsum(axis=2)\n",
    "[ \"ex-%s %6.3f\" % (CHARS_VALID[i], f) \n",
    "    for i,f in enumerate(trigram_freq[ CHAR_TO_IX['e'],  CHAR_TO_IX['x'] ].tolist()) if f>0.001 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trigram_word():\n",
    "    s=[]\n",
    "    idx_1 = idx_2 = CHAR_TO_IX[' ']\n",
    "    while True:\n",
    "        idx = np.searchsorted(trigram_freq_cum[idx_1, idx_2], np.random.uniform())\n",
    "        c = IX_TO_CHAR[idx]\n",
    "        if c==' ':\n",
    "            if len(s)>0:\n",
    "                #if len(s)<50: continue\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        s.append(c)\n",
    "        idx_1, idx_2 = idx_2, idx\n",
    "    return ''.join(s) \n",
    "' '.join([ trigram_word() for i in range(0,20) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate base-line scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_size=10000\n",
    "ngram_hits = [0,0,0]\n",
    "for w in [ unigram_word() for i in range(0, sample_size) ]:\n",
    "    if w in wordset: ngram_hits[0] += 1\n",
    "    #print(\"%s %s\" % ((\"YES\" if w in wordset else \" - \"), w, ))\n",
    "for w in [ bigram_word() for i in range(0, sample_size) ]:\n",
    "    if w in wordset: ngram_hits[1] += 1\n",
    "    #print(\"%s %s\" % ((\"YES\" if w in wordset else \" - \"), w, ))\n",
    "for w in [ trigram_word() for i in range(0, sample_size) ]:\n",
    "    if w in wordset: ngram_hits[2] += 1\n",
    "    #print(\"%s %s\" % ((\"YES\" if w in wordset else \" - \"), w, ))\n",
    "for i,hits in enumerate(ngram_hits):\n",
    "    print(\"%d-gram : %4.2f%%\"  % (i+1, hits*100./sample_size ))\n",
    "#[ (i,w) for i,w in enumerate(words) if 'mq' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This just checks the distribution of unigrams\n",
    "if False:\n",
    "    sample_size=1000\n",
    "    arr=[]\n",
    "    for w in [ unigram_word() for i in range(0, sample_size) ]:\n",
    "        arr.append(w)\n",
    "    s = ' '.join(arr)\n",
    "    s_len = len(s)\n",
    "    for c in CHARS_VALID:\n",
    "        f = len(s.split(c))-1\n",
    "        print(\"%s -> %6.3f%%\" % (c, f*100./s_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH_MAX = 16\n",
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = CHARS_SIZE\n",
    "\n",
    "GRAD_CLIP_BOUND = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN 'discriminator'\n",
    "\n",
    "Instead of having a binary 'YES/NO' decision about whether a word is valid (via a lookup in the vocabulary), it may make it simpler to train a word-generator if we can assign a probability that a given word is valid.  \n",
    "\n",
    "To do this, let's create a recurrent neural network (RNN) that accepts a (one-hot-encoded) word as input, and (at the end of the sequence) gives us an estimate of the probability that the word is valid.  \n",
    "\n",
    "Actually, rather than descriminate according to whether the word is *actually* valid, let's 'just' try to decide whether it was produced directly from the dictionary or from the ```generate_bigram_word()``` source.\n",
    "\n",
    "This can be tested by giving it lists of actual words, and lists of words generated by ```generate_bigram_word()``` and seeing whether they can be correctly classified.  \n",
    "\n",
    "The decision about what to do in the 12% of cases when the bigram function results in a valid word can be left until later...  (since the distribution is so heavily skewed towards producing non-words).\n",
    "\n",
    "### Create Training / Testing dataset\n",
    "And a 'batch generator' function that delivers data in the right format for RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator_dictionary(size=BATCH_SIZE/2):\n",
    "    while True:\n",
    "        uniform_vars = np.random.uniform( size=(size,) )\n",
    "        idx = freqs_cum.searchsorted(uniform_vars)\n",
    "        yield wordsnp[ idx ]\n",
    "    \n",
    "def batch_generator_bigram(size=BATCH_SIZE/2):\n",
    "    while True:\n",
    "        yield np.array([ bigram_word() for i in range(size) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Our batch generator will yield BATCH_SIZE/2 words, with their correct classification\n",
    "#def data_batch_generator(corpus, size=BATCH_SIZE):\n",
    "#    startidx = np.random.randint(0, len(corpus) - SEQUENCE_LENGTH - 1, size=size)\n",
    "#\n",
    "#    while True:\n",
    "#        items = np.array([corpus[start:start + SEQUENCE_LENGTH + 1] for start in startidx])\n",
    "#        startidx = (startidx + SEQUENCE_LENGTH) % (len(corpus) - SEQUENCE_LENGTH - 1)\n",
    "#        yield items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test it out\n",
    "#gen = batch_generator_dictionary(size=4)\n",
    "gen = batch_generator_bigram(size=4)\n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasagne RNN tutorial (including conventions &amp; rationale)\n",
    "\n",
    "*  http://colinraffel.com/talks/hammer2015recurrent.pdf\n",
    "\n",
    "#### Lasagne Examples\n",
    "\n",
    "*  https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/recurrent.py\n",
    "*  https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After sampling a data batch, we transform it into a one hot feature representation\n",
    "# and create a target sequence by shifting by one character\n",
    "#def prep_batch_for_network(batch):\n",
    "#    x_seq = np.zeros((len(batch), SEQUENCE_LENGTH, VOCAB_SIZE), dtype='float32')\n",
    "#    y_seq = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='int32')\n",
    "#\n",
    "#    for i, item in enumerate(batch):\n",
    "#        for j in range(SEQUENCE_LENGTH):\n",
    "#            x_seq[i, j] = CHAR_TO_ONEHOT[ item[j] ]\n",
    "#            #x_seq[i, j, :] = CHAR_TO_ONEHOT[ item[j] ]\n",
    "#            y_seq[i, j] = CHAR_TO_IX[ item[j+1] ]\n",
    "#\n",
    "#    return x_seq, y_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Descriminating Network Symbolically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variables for input. In addition to the usual features and target,\n",
    "# we need initial values for the RNN layer's hidden states\n",
    "disc_input_sym = theano.tensor.tensor3()\n",
    "disc_mask_sym  = theano.tensor.matrix()\n",
    "\n",
    "#disc_rnn1_t0_sym = theano.tensor.matrix()\n",
    "\n",
    "disc_target_sym = theano.tensor.vector()  # a list of probabilities of being from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our network has two stacked GRU layers processing the input sequence.\n",
    "disc_input = lasagne.layers.InputLayer( (None, None, CHARS_SIZE) )  # batch_size, sequence_len, chars_size\n",
    "disc_mask  = lasagne.layers.InputLayer( (None, None, CHARS_SIZE) )  # batch_size, sequence_len, chars_size\n",
    "\n",
    "#disc_rnn1_t0 = lasagne.layers.InputLayer( (None, RNN_HIDDEN_SIZE) )  # batch_size, RNN_hidden_size=chars_size\n",
    "#l_input_hid2 = lasagne.layers.InputLayer((None, RNN_HIDDEN_SIZE))\n",
    "\n",
    "disc_rnn1 = lasagne.layers.GRULayer(disc_input,\n",
    "                                      num_units=RNN_HIDDEN_SIZE,\n",
    "                                      gradient_steps=-1,\n",
    "                                      grad_clipping=GRAD_CLIP_BOUND,\n",
    "                                      #hid_init=disc_rnn1_t0,\n",
    "                                      learn_init=True,\n",
    "                                      mask_input=disc_mask,\n",
    "                                      only_return_final=True, # Only the state at the last timestep is needed\n",
    "                                    )\n",
    "\n",
    "#l_rnn2 = lasagne.layers.GRULayer(l_rnn,\n",
    "#                                  num_units=RNN_HIDDEN_SIZE,\n",
    "#                                  grad_clipping=5.,\n",
    "#                                  hid_init=l_input_hid2,\n",
    "#                                  #learn_init=True,\n",
    "#                                  )\n",
    "\n",
    "# Before the decoder layer, we need to reshape the sequence into the batch dimension,\n",
    "# so that timesteps are decoded independently.\n",
    "#l_shp = lasagne.layers.ReshapeLayer(l_rnn2, (-1, RNN_HIDDEN_SIZE))\n",
    "\n",
    "disc_decoder = lasagne.layers.DenseLayer(disc_rnn1,\n",
    "                   num_units=1,\n",
    "                   nonlinearity=lasagne.nonlinearities.sigmoid\n",
    "               )\n",
    "\n",
    "#disc_out = lasagne.layers.ReshapeLayer(disc_decoder, (-1, SEQUENCE_LENGTH, VOCAB_SIZE))\n",
    "#disc_final_out = disc_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, the output stage\n",
    "disc_out = lasagne.layers.get_output(disc_decoder, {\n",
    "                             disc_input: disc_input_sym, \n",
    "                             disc_mask: disc_mask_sym, \n",
    "                             #disc_rnn1_t0: disc_rnn1_t0_sym,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "#disc_rnn1_out_last  = disc_rnn1_out[:, -1]\n",
    "#hid2_out_last = hid2_out[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We flatten the sequence into the batch dimension before calculating the loss\n",
    "#def calc_cross_ent(net_output, targets):\n",
    "#    preds = T.reshape(net_output, (-1, VOCAB_SIZE))\n",
    "#    targets = T.flatten(targets)\n",
    "#    cost = T.nnet.categorical_crossentropy(preds, targets)\n",
    "#    return cost\n",
    "#\n",
    "#loss = T.mean(calc_cross_ent(prob_out, y_sym))\n",
    "disc_loss = theano.tensor.nnet.binary_crossentropy(disc_out, disc_target_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For stability during training, gradients are clipped and a total gradient norm constraint is also used\n",
    "#MAX_GRAD_NORM = 15\n",
    "\n",
    "disc_params = lasagne.layers.get_all_params(disc_out, trainable=True)\n",
    "\n",
    "disc_param_values = lasagne.layers.get_all_param_values(disc_out)\n",
    "param_dictionary = dict(\n",
    "     params = disc_param_values,\n",
    "     CHARS_VALID = CHARS_VALID, \n",
    "     CHAR_TO_IX = CHAR_TO_IX,\n",
    "     IX_TO_CHAR = IX_TO_CHAR,\n",
    "    )\n",
    "\n",
    "disc_grads = theano.tensor.grad(disc_loss, disc_params)\n",
    "#disc_grads = [theano.tensor.clip(g, -GRAD_CLIP_BOUND, GRAD_CLIP_BOUND) for g in disc_grads]\n",
    "#disc_grads, disc_norm = lasagne.updates.total_norm_constraint( disc_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "disc_updates = lasagne.updates.adam(disc_grads, disc_params) # , learning_rate=0.002)\n",
    "\n",
    "disc_train = theano.function([disc_input_sym, disc_mask_sym],  # , disc_rnn1_t0_sym\n",
    "                          [disc_loss, disc_out],  #  norm, hid_out_last, hid2_out_last\n",
    "                          updates=disc_updates,\n",
    "                         )\n",
    "\n",
    "disc_val = theano.function([disc_input_sym, disc_mask_sym], [disc_loss, disc_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the Training Loop\n",
    "\n",
    "Training takes a while :: 100 iteration takes about 2 minutes on a CPU (so, overall, it could be HOURS without a GPU)\n",
    "\n",
    "... you may want to skip this and the next cell, and load the pretrained weights instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hid = np.zeros((BATCH_SIZE, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "hid2 = np.zeros((BATCH_SIZE, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "\n",
    "train_batch_gen = data_batch_generator(train_corpus)\n",
    "\n",
    "for iteration in range(2*100*100):\n",
    "    x, y = prep_batch_for_network(next(train_batch_gen))\n",
    "    #print(iteration, np.shape(x), np.shape(y), np.shape(hid), np.shape(hid2))\n",
    "    loss_train, norm, hid, hid2 = f_train(x, y, hid, hid2)\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        print('Iteration {}, loss_train: {}, norm: {}'.format(iteration, loss_train, norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Uncomment the ```pickle.dump()``` to actually save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pickle.dump(param_dictionary, open('./data/RNN/gru_2layer_trained.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_dictionary = pickle.load(open('./data/RNN/gru_2layer_trained_claims.pkl', 'r'))\n",
    "lasagne.layers.set_all_param_values(l_out, param_dictionary['param values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce a Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_fn = theano.function([x_sym, hid_init_sym, hid2_init_sym], [prob_out, hid_out_last, hid2_out_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate validation loss (this takes a minute or so on a CPU)\n",
    "hid = np.zeros((BATCH_SIZE, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "hid2 = np.zeros((BATCH_SIZE, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "\n",
    "val_batch_gen = data_batch_generator(val_corpus)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iteration in range(50):\n",
    "    x, y = prep_batch_for_network(next(val_batch_gen))\n",
    "    #print(iteration, np.shape(x), np.shape(y), np.shape(hid), np.shape(hid2))\n",
    "    loss_val, hid, hid2 = f_val(x, y, hid, hid2)\n",
    "    losses.append(loss_val)\n",
    "    \n",
    "print(np.mean(losses))  # Preloaded data gives a result of 0.89385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a network that's optimised to *produce* text\n",
    "\n",
    "For faster sampling, we rebuild the network with a sequence length of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_input = lasagne.layers.InputLayer((None, 1, VOCAB_SIZE))\n",
    "l_input_hid = lasagne.layers.InputLayer((None, RNN_HIDDEN_SIZE))\n",
    "l_input_hid2 = lasagne.layers.InputLayer((None, RNN_HIDDEN_SIZE))\n",
    "\n",
    "l_rnn = lasagne.layers.GRULayer(l_input,\n",
    "                                  num_units=RNN_HIDDEN_SIZE,\n",
    "                                  grad_clipping=5.,\n",
    "                                  hid_init=l_input_hid,\n",
    "                                  )\n",
    "\n",
    "l_rnn2 = lasagne.layers.GRULayer(l_rnn,\n",
    "                                  num_units=RNN_HIDDEN_SIZE,\n",
    "                                  grad_clipping=5.,\n",
    "                                  hid_init=l_input_hid2,\n",
    "                                  )\n",
    "\n",
    "\n",
    "l_shp = lasagne.layers.ReshapeLayer(l_rnn2, (-1, RNN_HIDDEN_SIZE))\n",
    "\n",
    "l_decoder = lasagne.layers.DenseLayer(l_shp,\n",
    "                                      num_units=VOCAB_SIZE,\n",
    "                                      nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "l_out = lasagne.layers.ReshapeLayer(l_decoder, (-1, 1, VOCAB_SIZE))\n",
    "\n",
    "hid_out, hid2_out, prob_out = lasagne.layers.get_output([l_rnn, l_rnn2, l_out], {\n",
    "                                                         l_input: x_sym,\n",
    "                                                         l_input_hid: hid_init_sym,\n",
    "                                                         l_input_hid2: hid2_init_sym,\n",
    "                                                        })\n",
    "hid_out_last  = hid_out[:, -1]\n",
    "hid2_out_last = hid2_out[:, -1]\n",
    "prob_out_last = prob_out[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasagne.layers.set_all_param_values(l_out, param_dictionary['param values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_fn = theano.function([x_sym, hid_init_sym, hid2_init_sym], [prob_out_last, hid_out_last, hid2_out_last])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, Produce some text\n",
    "We will use random sentences from the validation corpus to 'prime' the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primers = val_corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed characters one at a time from the priming sequence into the network.\n",
    "\n",
    "To obtain a sample string, at each timestep we sample from the output probability distribution, and feed the chosen character back into the network. We terminate after the first linebreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = ''\n",
    "hid = np.zeros((1, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "hid2 = np.zeros((1, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "x = np.zeros((1, 1, VOCAB_SIZE), dtype='float32')\n",
    "\n",
    "primer = np.random.choice(primers) + '\\n'\n",
    "\n",
    "for c in primer:\n",
    "    p, hid, hid2 = predict_fn(x, hid, hid2)\n",
    "    x[0, 0, :] = CHAR_TO_ONEHOT[c]\n",
    "    \n",
    "for _ in range(500):\n",
    "    p, hid, hid2 = predict_fn(x, hid, hid2)\n",
    "    p = p/(1 + 1e-6)\n",
    "    s = np.random.multinomial(1, p)\n",
    "    sentence += IX_TO_CHAR[s.argmax(-1)]\n",
    "    x[0, 0, :] = s\n",
    "    if sentence[-1] == '\\n':\n",
    "        break\n",
    "        \n",
    "print('PRIMER: ' + primer)\n",
    "print('GENERATED: ' + sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=====\n",
    "\n",
    "1. Implement sampling using the \"temperature softmax\": $$p(i) = \\frac{e^{\\frac{z_i}{T}}}{\\Sigma_k e^{\\frac{z_k}{T}}}$$\n",
    "\n",
    "This generalizes the softmax with a parameter $T$ which affects the \"sharpness\" of the distribution. Lowering $T$ will make samples less error-prone but more repetitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment and run this cell for a solution\n",
    "#%load spoilers/tempsoftmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
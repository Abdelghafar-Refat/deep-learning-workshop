{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Character Model + Lots More\n",
    "\n",
    "This example trains a RNN to create plausible words from a corpus.  But it includes lots of interesting \"bells and whistles\"\n",
    "\n",
    "The data used for training is one of :\n",
    "  *  a vocabulary/dictionary collected from the 1-Billion-Word Corpus\n",
    "  *  a list of Indian names (voters rolls, by year)\n",
    "\n",
    "\n",
    "Adversarial networks : http://carpedm20.github.io/faces/\n",
    "\n",
    "Doing this with RNNs may be pretty novel : https://www.quora.com/Can-generative-adversarial-networks-be-used-in-sequential-data-in-recurrent-neural-networks-How-effective-would-they-be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "#theano.config.optimizer='None'\n",
    "\n",
    "import lasagne\n",
    "#from lasagne.utils import floatX\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "WORD_LENGTH_MAX = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load an interesting corpus :\n",
    "\n",
    "#corpus = gzip.open('./data/RNN/claims.txt.gz').read()\n",
    "#corpus = gzip.open('./data/RNN/Shakespeare.plays.txt.gz').read()\n",
    "#corpus = gzip.open('./data/RNN/Shakespeare.poetry.txt.gz').read()\n",
    "\n",
    "#with open('../data/RNN/ALL_1-vocab.txt','rt') as f:\n",
    "with gzip.open('../data/RNN/ALL_1-vocab.txt.gz') as f:\n",
    "    lines = [ l.strip().lower().split() for l in f.readlines() ]\n",
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corpus.split('\\n')[0]\n",
    "# Here are our characters : '[a-z\\- ]'\n",
    "import re\n",
    "invalid_chars = r'[^a-z\\- ]'\n",
    "lines_valid = [ l for l in lines if not re.search(invalid_chars, l[0]) ]\n",
    "#lines_valid = lines_valid[0:50000]\n",
    "lines_valid[0:10], len(lines_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /usr/share/dict/linux.words\n",
    "with open('/usr/share/dict/linux.words','rt') as f:\n",
    "    linux_words = [ l.strip() for l in f.readlines() ]\n",
    "linux_wordset = set(linux_words)\n",
    "#'united' in wordset\n",
    "lines_filtered = [l for l in lines_valid \n",
    "                     if len(l[0])>=3               # Require each word to have 3 or more characters\n",
    "                        and l[0] in linux_wordset  # Require each word to be found in regular dictionary\n",
    "                        and len(l[0])<WORD_LENGTH_MAX  # And limit length (to avoid crazy roll-out of RNN)\n",
    "                  ]\n",
    "lines_filtered[0:10], len(lines_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split apart the words and their frequencies (Assume these are in sorted order, at least initial few)\n",
    "words = [ l[0] for l in lines_filtered ]\n",
    "wordset = set(words)\n",
    "wordsnp = np.array(words)\n",
    "freqs_raw = np.array( [ int(l[1]) for l in lines_filtered ] )\n",
    "\n",
    "freq_tot = float(freqs_raw.sum())\n",
    "\n",
    "# Frequency weighting adjustments\n",
    "freqs = freqs_raw / freq_tot\n",
    "\n",
    "cutoff_index = 30   # All words with highter frequencies will be 'limited' at this level\n",
    "freqs[0:cutoff_index] = freqs[cutoff_index]\n",
    "\n",
    "freqs = freqs / freqs.sum()\n",
    "freqs[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_cum = np.array( [.1, .5, .9, 1.0] )\n",
    "test_cum.searchsorted([ .05, 0.45, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cumulative frequency, so that we can efficiently pick weighted random words...\n",
    "#   using http://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html\n",
    "freqs_cum = freqs.cumsum()\n",
    "freqs_cum[:10], freqs_cum[-10:], "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters from Corpus\n",
    "Find the set of characters used in the corpus and construct mappings between characters, integer indices, and one hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHARS_VALID = \"abcdefghijklmnopqrstuvwxyz- \"\n",
    "CHARS_SIZE  = len(CHARS_VALID)\n",
    "\n",
    "CHAR_TO_IX = {c: i for i, c in enumerate(CHARS_VALID)}\n",
    "IX_TO_CHAR = {i: c for i, c in enumerate(CHARS_VALID)}\n",
    "CHAR_TO_ONEHOT = {c: np.eye(CHARS_SIZE)[i] for i, c in enumerate(CHARS_VALID)}\n",
    "#CHAR_TO_IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Unigram frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single letter frequencies\n",
    "unigram_freq = np.zeros( (CHARS_SIZE,))\n",
    "idx_end = CHAR_TO_IX[' ']\n",
    "for i,w in enumerate(words):\n",
    "    word_freq = freqs[i]\n",
    "    for c in w:\n",
    "        unigram_freq[ CHAR_TO_IX[c] ] += word_freq\n",
    "    unigram_freq[ idx_end ] += word_freq\n",
    "unigram_freq /= unigram_freq.sum()\n",
    "unigram_freq_cum = unigram_freq.cumsum()\n",
    "[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(unigram_freq.tolist()) ]\n",
    "#CHARS_VALID[ unigram_freq_cum.searchsorted(0.20) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unigram_word():\n",
    "    s=[]\n",
    "    while True:\n",
    "        idx = np.searchsorted(unigram_freq_cum, np.random.uniform())\n",
    "        c = IX_TO_CHAR[idx]\n",
    "        if c==' ':\n",
    "            if len(s)>0:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        s.append(c)\n",
    "    return ''.join(s)\n",
    "' '.join([ unigram_word() for i in range(0,20) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bigram frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# two-letter frequencies\n",
    "bigram_freq = np.zeros( (CHARS_SIZE,CHARS_SIZE) )\n",
    "for i,w in enumerate(words):\n",
    "    w2 = ' '+w+' '\n",
    "    word_freq = freqs[i]\n",
    "    for j in range(0, len(w2)-1):\n",
    "        bigram_freq[ CHAR_TO_IX[ w2[j] ], CHAR_TO_IX[ w2[j+1] ] ] += word_freq\n",
    "#[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(bigram_freq[ CHAR_TO_IX['q'] ].tolist()) ]\n",
    "#bigram_freq.sum(axis=1)[CHAR_TO_IX['q']]\n",
    "bigram_freq /= bigram_freq.sum(axis=1)[:, np.newaxis] # Trick to enable unflattening of sum()\n",
    "bigram_freq_cum = bigram_freq.cumsum(axis=1)\n",
    "#[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(bigram_freq_cum[ CHAR_TO_IX['q'] ].tolist()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bigram_freq.sum(axis=1)[CHAR_TO_IX['q']]\n",
    "#(bigram_freq/ bigram_freq.sum(axis=1)).sum(axis=0)\n",
    "#bigram_freq.sum(axis=1)[CHAR_TO_IX['q']]\n",
    "#bigram_freq[CHAR_TO_IX['q'], :].sum()\n",
    "#(bigram_freq / bigram_freq.sum(axis=1)[:, np.newaxis]).cumsum(axis=1)\n",
    "#Letter relative frequency for letters following 'q'\n",
    "[ (CHARS_VALID[i], \"%6.3f\" % f) for i,f in enumerate(bigram_freq[ CHAR_TO_IX['q'] ].tolist()) if f>0.001]\n",
    "#bigram_freq_cum[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigram_word():\n",
    "    s=[]\n",
    "    idx_last = CHAR_TO_IX[' ']\n",
    "    while True:\n",
    "        idx = np.searchsorted(bigram_freq_cum[idx_last], np.random.uniform())\n",
    "        c = IX_TO_CHAR[idx]\n",
    "        if c==' ':\n",
    "            if len(s)>0:\n",
    "                #if len(s)<50: continue\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        s.append(c)\n",
    "        idx_last=idx\n",
    "    return ''.join(s)\n",
    "' '.join([ bigram_word() for i in range(0,20) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Trigram frequency distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Three-letter frequencies\n",
    "trigram_freq = np.zeros( (CHARS_SIZE,CHARS_SIZE,CHARS_SIZE) )\n",
    "for i,w in enumerate(words):\n",
    "    w3 = '  '+w+'  '\n",
    "    word_freq = freqs[i]\n",
    "    for j in range(0, len(w3)-2):\n",
    "        trigram_freq[ CHAR_TO_IX[ w3[j] ], CHAR_TO_IX[ w3[j+1] ], CHAR_TO_IX[ w3[j+2] ] ] += word_freq\n",
    "trigram_freq /= trigram_freq.sum(axis=2)[:, :, np.newaxis] # Trick to enable unflattening of sum()\n",
    "trigram_freq_cum = trigram_freq.cumsum(axis=2)\n",
    "[ \"ex-%s %6.3f\" % (CHARS_VALID[i], f) \n",
    "    for i,f in enumerate(trigram_freq[ CHAR_TO_IX['e'],  CHAR_TO_IX['x'] ].tolist()) if f>0.001 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trigram_word():\n",
    "    s=[]\n",
    "    idx_1 = idx_2 = CHAR_TO_IX[' ']\n",
    "    while True:\n",
    "        idx = np.searchsorted(trigram_freq_cum[idx_1, idx_2], np.random.uniform())\n",
    "        c = IX_TO_CHAR[idx]\n",
    "        if c==' ':\n",
    "            if len(s)>0:\n",
    "                #if len(s)<50: continue\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        s.append(c)\n",
    "        idx_1, idx_2 = idx_2, idx\n",
    "    return ''.join(s) \n",
    "' '.join([ trigram_word() for i in range(0,20) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate base-line scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_size=10000\n",
    "ngram_hits = [0,0,0]\n",
    "for w in [ unigram_word() for i in range(0, sample_size) ]:\n",
    "    if w in wordset: ngram_hits[0] += 1\n",
    "    #print(\"%s %s\" % ((\"YES\" if w in wordset else \" - \"), w, ))\n",
    "for w in [ bigram_word() for i in range(0, sample_size) ]:\n",
    "    if w in wordset: ngram_hits[1] += 1\n",
    "    #print(\"%s %s\" % ((\"YES\" if w in wordset else \" - \"), w, ))\n",
    "for w in [ trigram_word() for i in range(0, sample_size) ]:\n",
    "    if w in wordset: ngram_hits[2] += 1\n",
    "    #print(\"%s %s\" % ((\"YES\" if w in wordset else \" - \"), w, ))\n",
    "for i,hits in enumerate(ngram_hits):\n",
    "    print(\"%d-gram : %4.2f%%\"  % (i+1, hits*100./sample_size ))\n",
    "#[ (i,w) for i,w in enumerate(words) if 'mq' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the distribution of unigrams by sampling (sanity check)\n",
    "if False:\n",
    "    sample_size=1000\n",
    "    arr=[]\n",
    "    for w in [ unigram_word() for i in range(0, sample_size) ]:\n",
    "        arr.append(w)\n",
    "    s = ' '.join(arr)\n",
    "    s_len = len(s)\n",
    "    for c in CHARS_VALID:\n",
    "        f = len(s.split(c))-1\n",
    "        print(\"%s -> %6.3f%%\" % (c, f*100./s_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = CHARS_SIZE\n",
    "GRAD_CLIP_BOUND = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN 'discriminator'\n",
    "\n",
    "Instead of having a binary 'YES/NO' decision about whether a word is valid (via a lookup in the vocabulary), it may make it simpler to train a word-generator if we can assign a probability that a given word is valid.  \n",
    "\n",
    "To do this, let's create a recurrent neural network (RNN) that accepts a (one-hot-encoded) word as input, and (at the end of the sequence) gives us an estimate of the probability that the word is valid.  \n",
    "\n",
    "Actually, rather than descriminate according to whether the word is *actually* valid, let's 'just' try to decide whether it was produced directly from the dictionary or from the ```generate_bigram_word()``` source.\n",
    "\n",
    "This can be tested by giving it lists of actual words, and lists of words generated by ```generate_bigram_word()``` and seeing whether they can be correctly classified.  \n",
    "\n",
    "The decision about what to do in the 12% of cases when the bigram function results in a valid word can be left until later...  (since the distribution is so heavily skewed towards producing non-words).\n",
    "\n",
    "### Create Training / Testing dataset\n",
    "And a 'batch generator' function that delivers data in the right format for RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_dictionary(size=BATCH_SIZE/2):\n",
    "    uniform_vars = np.random.uniform( size=(size,) )\n",
    "    idx = freqs_cum.searchsorted(uniform_vars)\n",
    "    return wordsnp[ idx ].tolist()\n",
    "    \n",
    "def batch_bigram(size=BATCH_SIZE/2):\n",
    "    return [ bigram_word()[0:WORD_LENGTH_MAX] for i in range(size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Our batch generator will yield BATCH_SIZE/2 words, with their correct classification\n",
    "#def data_batch_generator(corpus, size=BATCH_SIZE):\n",
    "#    startidx = np.random.randint(0, len(corpus) - SEQUENCE_LENGTH - 1, size=size)\n",
    "#\n",
    "#    while True:\n",
    "#        items = np.array([corpus[start:start + SEQUENCE_LENGTH + 1] for start in startidx])\n",
    "#        startidx = (startidx + SEQUENCE_LENGTH) % (len(corpus) - SEQUENCE_LENGTH - 1)\n",
    "#        yield items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test it out\n",
    "#batch_test = lambda : batch_dictionary(size=4)\n",
    "batch_test = lambda : batch_bigram(size=4)\n",
    "print(batch_test())\n",
    "print(batch_test())\n",
    "print(batch_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasagne RNN tutorial (including conventions &amp; rationale)\n",
    "\n",
    "*  http://colinraffel.com/talks/hammer2015recurrent.pdf\n",
    "\n",
    "#### Lasagne Examples\n",
    "\n",
    "*  https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/recurrent.py\n",
    "*  https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After sampling a data batch, we transform it into a one hot feature representation with a mask\n",
    "def prep_batch_for_network(batch_of_words):\n",
    "    word_max_length = np.array( [ len(w) for w in batch_of_words ]).max()\n",
    "    \n",
    "    # translate into one-hot matrix, mask values and targets\n",
    "    input_values = np.zeros((len(batch_of_words), word_max_length, CHARS_SIZE), dtype='float32')\n",
    "    mask_values  = np.zeros((len(batch_of_words), word_max_length), dtype='float32')\n",
    "    \n",
    "    for i, word in enumerate(batch_of_words):\n",
    "      for j, c in enumerate(word):\n",
    "        input_values[i,j] = CHAR_TO_ONEHOT[ c ]\n",
    "      mask_values[i, 0:len(word) ] = 1.\n",
    "\n",
    "    return input_values, mask_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Descriminating Network Symbolically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variables for input. In addition to the usual features and target,\n",
    "# we need initial values for the RNN layer's hidden states\n",
    "disc_input_sym = theano.tensor.tensor3()\n",
    "disc_mask_sym  = theano.tensor.matrix()\n",
    "\n",
    "#disc_rnn1_t0_sym = theano.tensor.matrix()\n",
    "\n",
    "disc_target_sym = theano.tensor.matrix()  # probabilities of being from the dictionary (i.e. a single column matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our network has two stacked GRU layers processing the input sequence.\n",
    "disc_input = lasagne.layers.InputLayer( (None, None, CHARS_SIZE) )  # batch_size, sequence_len, chars_size\n",
    "disc_mask  = lasagne.layers.InputLayer( (None, None, CHARS_SIZE) )  # batch_size, sequence_len, chars_size\n",
    "\n",
    "#disc_rnn1_t0 = lasagne.layers.InputLayer( (None, RNN_HIDDEN_SIZE) )  # batch_size, RNN_hidden_size=chars_size\n",
    "#l_input_hid2 = lasagne.layers.InputLayer((None, RNN_HIDDEN_SIZE))\n",
    "\n",
    "disc_rnn1 = lasagne.layers.GRULayer(disc_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                #hid_init=disc_rnn1_t0,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=disc_mask,\n",
    "                only_return_final=True, # Only the state at the last timestep is needed\n",
    "            )\n",
    "\n",
    "disc_decoder = lasagne.layers.DenseLayer(disc_rnn1,\n",
    "                num_units=1,\n",
    "                nonlinearity=lasagne.nonlinearities.sigmoid\n",
    "            )\n",
    "\n",
    "disc_final = disc_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, the output stage\n",
    "disc_output = lasagne.layers.get_output(disc_final, {\n",
    "                             disc_input: disc_input_sym, \n",
    "                             disc_mask: disc_mask_sym, \n",
    "                             #disc_rnn1_t0: disc_rnn1_t0_sym,\n",
    "                            }\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_loss = theano.tensor.nnet.binary_crossentropy(disc_output, disc_target_sym).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and the Training and Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For stability during training, gradients are clipped and a total gradient norm constraint is also used\n",
    "#MAX_GRAD_NORM = 15\n",
    "\n",
    "disc_params = lasagne.layers.get_all_params(disc_final, trainable=True)\n",
    "\n",
    "disc_grads = theano.tensor.grad(disc_loss, disc_params)\n",
    "#disc_grads = [theano.tensor.clip(g, -GRAD_CLIP_BOUND, GRAD_CLIP_BOUND) for g in disc_grads]\n",
    "#disc_grads, disc_norm = lasagne.updates.total_norm_constraint( disc_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "disc_updates = lasagne.updates.adam(disc_grads, disc_params)\n",
    "\n",
    "disc_train = theano.function([disc_input_sym, disc_target_sym, disc_mask_sym],  # , disc_rnn1_t0_sym\n",
    "                          [disc_loss],  #  , disc_output, norm, hid_out_last, hid2_out_last\n",
    "                          updates=disc_updates,\n",
    "                         )\n",
    "\n",
    "disc_predict = theano.function([disc_input_sym, disc_mask_sym], [disc_output])\n",
    "print(\"Discriminator network functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the Discriminator Training Loop\n",
    "\n",
    "*  Training takes a while :: 1000 iteration takes about 20 seconds on a CPU\n",
    "*  ... you may want to skip this and the next cell, and load the pretrained weights instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0, iterations_complete = time.time(), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 10*1000\n",
    "t1, iterations_recent = time.time(), iterations_complete\n",
    "for epoch_i in range(epochs):\n",
    "    # create a batch of words : half are dictionary, half are from bigram\n",
    "    batch_of_words = batch_dictionary() + batch_bigram()\n",
    "    \n",
    "    # get the one-hot input values and corresponding mask matrix\n",
    "    disc_input_values, disc_mask_values = prep_batch_for_network(batch_of_words)\n",
    "\n",
    "    # and here are the assocated target values \n",
    "    disc_target_values= np.zeros((len(batch_of_words),1), dtype='float32')\n",
    "    \n",
    "    disc_target_values[ 0:(BATCH_SIZE/2), 0 ] = 1.0 # First half are dictionary values\n",
    "    for i, word in enumerate(batch_of_words):\n",
    "      if True and i>BATCH_SIZE/2 and word in wordset:\n",
    "        disc_target_values[ i , 0 ] = 1.0 # bigram has hit a dictionary word by luck...\n",
    "\n",
    "        \n",
    "    # Now train the discriminator RNN\n",
    "    disc_loss_, = disc_train(disc_input_values, disc_target_values, disc_mask_values)\n",
    "    \n",
    "    #disc_output_, = disc_predict(disc_input_values, disc_mask_values)\n",
    "    iterations_complete += 1\n",
    "    \n",
    "    if iterations_complete % 250 == 0:\n",
    "        secs_per_batch = float(time.time() - t1)/ (iterations_complete - iterations_recent)\n",
    "        eta_in_secs = secs_per_batch*(epochs-epoch_i)\n",
    "        print(\"Iteration {:5d}, loss_train: {:.4f} ({:.1f}s per 1000 batches)  eta: {:.0f}m{:02.0f}s\".format(\n",
    "                iterations_complete, float(disc_loss_), \n",
    "                secs_per_batch*1000., np.floor(eta_in_secs/60), np.floor(eta_in_secs % 60) \n",
    "             ))\n",
    "        #print('Iteration {}, output: {}'.format(iteration, disc_output_, ))  # , output: {}\n",
    "        t1, iterations_recent = time.time(), iterations_complete\n",
    "        \n",
    "print('Iteration {}, ran in {:.1f}sec'.format(iterations_complete, float(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Uncomment the ```pickle.dump()``` to actually save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_param_values = lasagne.layers.get_all_param_values(disc_final)\n",
    "disc_param_dictionary = dict(\n",
    "     params = disc_param_values,\n",
    "     CHARS_VALID = CHARS_VALID, \n",
    "     CHAR_TO_IX = CHAR_TO_IX,\n",
    "     IX_TO_CHAR = IX_TO_CHAR,\n",
    "    )\n",
    "#pickle.dump(disc_param_dictionary, open('../data/RNN/disc_trained.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_param_dictionary = pickle.load(open('../data/RNN/disc_trained_64x310k.pkl', 'r'))\n",
    "lasagne.layers.set_all_param_values(disc_final, disc_param_dictionary['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Discriminator Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text_list = [\"shape\", \"shast\", \"shaes\", \"shafg\", \"shaqw\"]\n",
    "test_text_list = [\"opposite\", \"aposite\", \"apposite\", \"xposite\", \"rrwqsite\", \"deposit\", \"idilic\", \"idyllic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_input_values, disc_mask_values = prep_batch_for_network(test_text_list)\n",
    "\n",
    "disc_output_, = disc_predict(disc_input_values, disc_mask_values)\n",
    "\n",
    "for i,v in enumerate(disc_output_.tolist()):\n",
    "    print(\"%s : %5.2f%%\" % ((test_text_list[i]+' '*20)[:20], v[0]*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Generative network\n",
    "\n",
    "Next, let's build an RNN that produces text, and train it using (a) a pure dictionary look-up, and (b) the correctness signal from the Discriminator above.\n",
    "\n",
    "Plan of attack : \n",
    "\n",
    "*  Create a GRU that outputs a character probability distribution for every time step\n",
    "*  Run the RNN several times :\n",
    "   *   each time is an additional character input longer \n",
    "   *   with the next character chosen according to the probability distribution given\n",
    "   *   and then re-run with the current input words (up to that point)\n",
    "*  Stop adding characters when they've all reached 'space'\n",
    "\n",
    "This seems very inefficient (since the first RNN steps are being run multiple times on the same starting letters), but is the same as in https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Symbolic variables for input. In addition to the usual features and target,\n",
    "gen_input_sym = theano.tensor.tensor3()\n",
    "gen_mask_sym  = theano.tensor.matrix()\n",
    "\n",
    "gen_valid_target_sym = theano.tensor.matrix()  # probabilities of being from the dictionary (i.e. a single column matrix)\n",
    "gen_words_target_sym = theano.tensor.imatrix() # characters generated (as character indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_input = lasagne.layers.InputLayer( (None, None, CHARS_SIZE) )  # batch_size, sequence_len, chars_size\n",
    "gen_mask  = lasagne.layers.InputLayer( (None, None, CHARS_SIZE) )  # batch_size, sequence_len, chars_size\n",
    "\n",
    "#gen_rnn1_t0 = lasagne.layers.InputLayer( (None, RNN_HIDDEN_SIZE) )  # batch_size, RNN_hidden_size=chars_size\n",
    "\n",
    "#n_batch, n_time_steps, n_features = gen_input.input_var.shape\n",
    "n_batch, n_time_steps, n_features = gen_input_sym.shape\n",
    "\n",
    "gen_rnn1 = lasagne.layers.GRULayer(gen_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                #hid_init=disc_rnn1_t0,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=gen_mask,\n",
    "                only_return_final=False, # Need all of the output states\n",
    "            )\n",
    "\n",
    "# Before the decoder layer, we need to reshape the sequence into the batch dimension,\n",
    "# so that timesteps are decoded independently.\n",
    "gen_reshape = lasagne.layers.ReshapeLayer(gen_rnn1, (-1, RNN_HIDDEN_SIZE) )\n",
    "\n",
    "gen_prob_raw = lasagne.layers.DenseLayer(gen_reshape, \n",
    "                num_units=CHARS_SIZE, \n",
    "                nonlinearity=lasagne.nonlinearities.linear # No squashing (yet)\n",
    "            )\n",
    "\n",
    "gen_prob = lasagne.layers.ReshapeLayer(gen_prob_raw, (-1, n_time_steps, CHARS_SIZE))\n",
    "\n",
    "#gen_prob_final = lasagne.layers.SliceLayer(gen_prob_raw, indices=(-1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, the output stage - this is for the training (over all the letters in the words)\n",
    "gen_output = lasagne.layers.get_output(gen_prob, \n",
    "                {\n",
    "                 gen_input: gen_input_sym, \n",
    "                 gen_mask: gen_mask_sym, \n",
    "                }\n",
    "            )\n",
    "\n",
    "# And for prediction (which is done incrementally, adding one letter at a time)\n",
    "gen_output_last = gen_output[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The generative network is trained by encouraging the outputs across time to match the given sequence of letters\n",
    "\n",
    "# We flatten the sequence into the batch dimension before calculating the loss\n",
    "def gen_word_cross_ent(net_output, targets):\n",
    "    preds = theano.tensor.reshape(net_output, (-1, CHARS_SIZE))\n",
    "    targets_flat = theano.tensor.flatten(targets)\n",
    "    cost = theano.tensor.nnet.categorical_crossentropy(preds, targets_flat)\n",
    "    return cost\n",
    "\n",
    "gen_loss = theano.tensor.dot( gen_word_cross_ent(gen_output, gen_words_target_sym), gen_valid_target_sym ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For stability during training, gradients are clipped and a total gradient norm constraint is also used\n",
    "#MAX_GRAD_NORM = 15\n",
    "\n",
    "gen_params = lasagne.layers.get_all_params(gen_prob, trainable=True)\n",
    "\n",
    "gen_grads = theano.tensor.grad(gen_loss, gen_params)\n",
    "#disc_grads = [theano.tensor.clip(g, -GRAD_CLIP_BOUND, GRAD_CLIP_BOUND) for g in disc_grads]\n",
    "#disc_grads, disc_norm = lasagne.updates.total_norm_constraint( disc_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "gen_updates = lasagne.updates.adam(gen_grads, gen_params)\n",
    "\n",
    "gen_train = theano.function([gen_input_sym, gen_words_target_sym, gen_valid_target_sym, gen_mask_sym],\n",
    "                [gen_loss],\n",
    "                updates=gen_updates,\n",
    "            )\n",
    "\n",
    "gen_predict = theano.function([gen_input_sym, gen_mask_sym], [gen_output_last])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Generative Network to create sample words\n",
    "\n",
    "The network above can be used to generate text...\n",
    "\n",
    "The following set-up allows for the output of the RNN at each timestep to be mixed with the letter frequency that the bigram model would suggest - in a proportion ```bigram_overlay``` which can vary from ```0``` (being solely RNN derived) to ```1.0``` (being solely bigram frequencies, with the RNN output being disregarded). \n",
    "\n",
    "The input is a 'random field' matrix that is used to chose each letter in each slot from the generated probability distribution.\n",
    "\n",
    "Once a space is output for a specific word, then it stops being extended (equivalently, the mask is set to zero going forwards).\n",
    "\n",
    "Once spaces have been observed for all words (or the maximum length reached), the process ends, and a list of the created words is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's pre-calculate the logs of the bigram frequencies, since they may be mixed in below\n",
    "bigram_min_freq = 1e-10 # To prevent underflow in log...\n",
    "bigram_freq_log = np.log( bigram_freq + bigram_min_freq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_rnn_words(random_field, bigram_overlay=0.0):\n",
    "    batch_size, max_word_length  = random_field.shape\n",
    "    \n",
    "    idx_spc = CHAR_TO_IX[' ']\n",
    "    def append_indices_as_chars(words_current, idx_list):\n",
    "        for i, idx in enumerate(idx_list):\n",
    "            if idx == idx_spc:\n",
    "                pass # Words end at space\n",
    "                #words_current[i] += 'x'\n",
    "            else:\n",
    "                words_current[i] += IX_TO_CHAR[idx]\n",
    "        return words_current\n",
    "    \n",
    "    # Create a 'first character' by using the bigram transitions from 'space' (this is fair)\n",
    "    idx_initial = [ np.searchsorted(bigram_freq_cum[idx_spc], random_field[i, 0]) for i in range(batch_size) ]\n",
    "    bigram_freq_log_current = bigram_freq_log[ np.array(idx_initial) , :]\n",
    "    \n",
    "    words_current = [ '' for _ in range(batch_size) ]\n",
    "    words_current = append_indices_as_chars(words_current, idx_initial)\n",
    "    \n",
    "    \n",
    "    col, finished=1, False\n",
    "    while not finished:\n",
    "        gen_input_values, gen_mask_values = prep_batch_for_network(words_current)\n",
    "        #print(gen_mask_values[:,-1])\n",
    "        gen_out_, = gen_predict(gen_input_values, gen_mask_values)\n",
    "        \n",
    "        gen_freq = np.exp( # This is going to look like softmax...\n",
    "            gen_out_*(1.0-bigram_overlay) + bigram_freq_log_current*bigram_overlay \n",
    "        )  \n",
    "        \n",
    "        # This output is the final probability[CHARS_SIZE], so let's cumsum it, etc.\n",
    "        gen_prob = gen_freq / gen_freq.sum(axis=1)[:, np.newaxis] # Trick to enable unflattening of sum()\n",
    "        gen_prob_cum = gen_prob.cumsum(axis=1)\n",
    "        \n",
    "        idx_next = [ # Only add extra letters if we haven't already passed a space (i.e. mask[-1]==0)\n",
    "            idx_spc if gen_mask_values[i,-1]==0 else np.searchsorted(gen_prob_cum[i], random_field[i, col]) \n",
    "            for i in range(batch_size) \n",
    "        ]\n",
    "        \n",
    "        bigram_freq_log_current = bigram_freq_log[ np.array(idx_next) , :]\n",
    "        words_current = append_indices_as_chars(words_current, idx_next)\n",
    "        \n",
    "        words_current_max_length = np.array( [ len(w) for w in words_current ]).max()\n",
    "        \n",
    "        # If the words have reached the maximum length, or we didn't extend any of them...\n",
    "        if words_current_max_length>=max_word_length or words_current_max_length<col:\n",
    "            finished=True \n",
    "        \n",
    "        col += 1\n",
    "\n",
    "    return words_current\n",
    "        \n",
    "# Create a probability distribution across all potential positions in the output 'field'\n",
    "random_field = np.random.uniform( size=(BATCH_SIZE, WORD_LENGTH_MAX) )\n",
    "\n",
    "gen_words_output = generate_rnn_words(random_field, bigram_overlay=0.0)\n",
    "\n",
    "print( '\\n'.join(gen_words_output))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remeber the initial (random) Network State \n",
    "\n",
    "This will come in handy when we need to reset the network back to 'untrained' later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_param_values_initial = lasagne.layers.get_all_param_values(gen_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, train the Generator RNN based on the Dictionary itself\n",
    "\n",
    "Once we have an output word, let's reward the RNN based on a specific training signal.  We'll encapsulate the training in a function that takes the input signal as a parameter, so that we can try other training schemes (later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_good_output_dictionary(output_words):\n",
    "    return np.array(\n",
    "        [ (1.0 if w in wordset else 0.0) for w in output_words ],\n",
    "        dtype='float32'\n",
    "    )\n",
    "\n",
    "t0, iterations_complete = time.time(), 0\n",
    "def reset_generative_network():\n",
    "    global t0, iterations_complete\n",
    "    t0, iterations_complete = time.time(), 0\n",
    "    lasagne.layers.set_all_param_values(gen_prob, gen_param_values_initial)\n",
    "\n",
    "def prep_batch_for_network_output(mask_values, batch_of_words):\n",
    "    output_indices = np.zeros(mask_values.shape, dtype='int32')\n",
    "\n",
    "    for i, word in enumerate(batch_of_words):\n",
    "      word_shifted = word[1:]+' '\n",
    "      for j, c in enumerate(word_shifted):\n",
    "        output_indices[i,j] = CHAR_TO_IX[ c ]\n",
    "\n",
    "    return output_indices\n",
    "    \n",
    "def train_generative_network(is_good_output_function=is_good_output_dictionary, epochs=10*1000, bigram_overlay=0.0):\n",
    "    global t0, iterations_complete\n",
    "    t1, iterations_recent = time.time(), iterations_complete\n",
    "    for epoch_i in range(epochs):\n",
    "        random_field = np.random.uniform( size=(BATCH_SIZE, WORD_LENGTH_MAX) )\n",
    "\n",
    "        gen_words_output = generate_rnn_words(random_field, bigram_overlay=bigram_overlay)\n",
    "        \n",
    "        # Now, create a training set of input -> output, coupled with an intensity signal\n",
    "        #   first the step-by-step network inputs\n",
    "        gen_input_values, gen_mask_values = prep_batch_for_network(gen_words_output)\n",
    "        \n",
    "        #  now create step-by-step network outputs (strip off first character, add spaces) as *indicies*\n",
    "        gen_output_values_int = prep_batch_for_network_output(gen_mask_values, gen_words_output)\n",
    "        \n",
    "        \n",
    "        # And, since we have a set of words, we can also determine their 'goodness'\n",
    "        is_good_output = is_good_output_function(gen_words_output)\n",
    "        \n",
    "        #   and the 'error signal' is simply (+ve for good, -ve for bad):\n",
    "        target_valid = (np.array(is_good_output) - 0.5).reshape( (len(gen_words_output),1) )\n",
    "        \n",
    "        print(target_valid.shape)\n",
    "        \n",
    "        # Now train the generator RNN\n",
    "        gen_loss_, = gen_train(gen_input_values, gen_output_values_int, target_valid, gen_mask_values)\n",
    "\n",
    "        iterations_complete += 1\n",
    "\n",
    "        if iterations_complete % 10 == 0:\n",
    "            secs_per_batch = float(time.time() - t1)/ (iterations_complete - iterations_recent)\n",
    "            eta_in_secs = secs_per_batch*(epochs-epoch_i)\n",
    "            print(\"Iteration {:5d}, loss_train: {:.4f} ({:.1f}s per 1000 batches)  eta: {:.0f}m{:02.0f}s\".format(\n",
    "                    iterations_complete, float(gen_loss_), \n",
    "                    secs_per_batch*1000., np.floor(eta_in_secs/60), np.floor(eta_in_secs % 60), )\n",
    "                 )\n",
    "            #print('Iteration {}, output: {}'.format(iteration, disc_output_, ))  # , output: {}\n",
    "            t1, iterations_recent = time.time(), iterations_complete\n",
    "\n",
    "    print('Iteration {}, ran in {:.1f}sec'.format(iterations_complete, float(time.time() - t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_generative_network()\n",
    "train_generative_network(is_good_output_function=is_good_output_dictionary, epochs=10*1000, bigram_overlay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, Produce some text\n",
    "We will use random sentences from the validation corpus to 'prime' the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primers = val_corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed characters one at a time from the priming sequence into the network.\n",
    "\n",
    "To obtain a sample string, at each timestep we sample from the output probability distribution, and feed the chosen character back into the network. We terminate after the first linebreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = ''\n",
    "hid = np.zeros((1, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "hid2 = np.zeros((1, RNN_HIDDEN_SIZE), dtype='float32')\n",
    "x = np.zeros((1, 1, VOCAB_SIZE), dtype='float32')\n",
    "\n",
    "primer = np.random.choice(primers) + '\\n'\n",
    "\n",
    "for c in primer:\n",
    "    p, hid, hid2 = predict_fn(x, hid, hid2)\n",
    "    x[0, 0, :] = CHAR_TO_ONEHOT[c]\n",
    "    \n",
    "for _ in range(500):\n",
    "    p, hid, hid2 = predict_fn(x, hid, hid2)\n",
    "    p = p/(1 + 1e-6)\n",
    "    s = np.random.multinomial(1, p)\n",
    "    sentence += IX_TO_CHAR[s.argmax(-1)]\n",
    "    x[0, 0, :] = s\n",
    "    if sentence[-1] == '\\n':\n",
    "        break\n",
    "        \n",
    "print('PRIMER: ' + primer)\n",
    "print('GENERATED: ' + sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=====\n",
    "\n",
    "1. Implement sampling using the \"temperature softmax\": $$p(i) = \\frac{e^{\\frac{z_i}{T}}}{\\Sigma_k e^{\\frac{z_k}{T}}}$$\n",
    "\n",
    "This generalizes the softmax with a parameter $T$ which affects the \"sharpness\" of the distribution. Lowering $T$ will make samples less error-prone but more repetitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load spoilers/tempsoftmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
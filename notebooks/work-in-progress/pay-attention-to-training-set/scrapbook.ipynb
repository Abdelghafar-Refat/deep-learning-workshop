{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many layers are frozen in :\n",
    "\n",
    "https://github.com/ShehabMMohamed/TinyImageNet-KaggleCompetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import Xception\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 299\n",
    "img_width = 299\n",
    "\n",
    "input_tensor = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "pre_trained_model = Xception(weights='imagenet', input_tensor=input_tensor, include_top=False, pooling='avg')\n",
    "# Downloads 83.7Mb of model..., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model.layers[20].name\n",
    "# 'block3_sepconv2' ( next are 'block3_sepconv2_bn' and 'conv2d_2' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model.summary()\n",
    "#Total params: 20,861,480\n",
    "#Trainable params: 20,806,952\n",
    "#Non-trainable params: 54,528\n",
    "# Input ... block3_sepconv2 ... block14_sepconv2 ... global_average_pooling2d_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### That's a lot of fine tuning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position of block3_sepconv2 in xception : \n",
    "\n",
    "https://github.com/keras-team/keras-applications/blob/master/keras_applications/xception.py#L188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should check the layout of the PyTorch model too...\n",
    "\n",
    "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/xception.py#L137\n",
    "\n",
    "Seems to be only 12 blocks, which is a little odd..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See UMAP results on basic 'digits' raw image files (per documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "#print(digits.DESCR) # Documentation\n",
    "#digits.images.shape # (1797, 8, 8)\n",
    "digits.data.shape   # (1797, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(digits.data)  # <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(digits.data)\n",
    "# Verify that the result of calling transform is\n",
    "# idenitical to accessing the embedding_ attribute\n",
    "assert(np.all(embedding == reducer.embedding_))\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See UMAP results on plain MNIST raw image files\n",
    "\n",
    "*  https://github.com/snakers4/playing_with_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install Pillow-SIMD  # Hmm : Missing jpeg library..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "mnist_data = torchvision.datasets.MNIST('./data/mnist', download=True, \n",
    "  train=True, transform=transform, )\n",
    "mnist_data_test = torchvision.datasets.MNIST('./data/mnist', download=True, \n",
    "  train=False, transform=transform, )\n",
    "\n",
    "data_set = mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "  data_set, batch_size=batch_size, #shuffle=True,\n",
    "  num_workers=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dim = 50\n",
    "\n",
    "proj = torch.nn.Linear( 28*28, proj_dim )  # Does the initialisation 'better' than torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This maps the result of the projection directly into the results numpy arrays\n",
    "xs, ys = np.zeros( (len(data_set), proj_dim) ), np.zeros( (len(data_set),) )\n",
    "for i_batch, (x_batch, y_batch) in enumerate(data_loader):\n",
    "    x_proj = proj( x_batch.view(-1, 28*28) )\n",
    "    #print(x_proj.size()); break\n",
    "    xs[i_batch*batch_size:(i_batch+1)*batch_size, :] = x_proj.detach()\n",
    "    ys[i_batch*batch_size:(i_batch+1)*batch_size] = y_batch.detach()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i_batch*batch_size\n",
    "base=59990; ys[base+0:base+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(xs) \n",
    "print(\"Fitting %d-D took %d secs\" % (proj_dim, time.time()-t0,) )  \n",
    "# 400d : 74 secs\n",
    "# 100d : 76 secs \n",
    "#  50d : 69 secs\n",
    "#  35d : 68 secs\n",
    "#  20d : 68 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(xs)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=ys, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP %d-D projection of MNIST' % (proj_dim,), fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train CNN on dataset (with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so now let's build a CNN to classify regular MNIST \n",
    "#   with last layer being 50D, and re-run the UMAP on that result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv1_mp = torch.nn.MaxPool2d(2)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_mp = torch.nn.MaxPool2d(2)\n",
    "        self.conv2_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(320, 50)\n",
    "        self.drop = torch.nn.Dropout()\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, middle_layer=False):\n",
    "        x = self.relu(self.conv1_mp(self.conv1(x)))\n",
    "        x = self.relu(self.conv2_mp(self.conv2_drop(self.conv2(x))))\n",
    "        x = x.view(-1, 320) # Flatten\n",
    "        x = self.fc1(x)\n",
    "        if middle_layer:\n",
    "            return x\n",
    "        x = self.drop(self.relu(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.logsoftmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=40\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  data_set, batch_size=batch_size, #shuffle=True,\n",
    "  num_workers=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (10000//batch_size) == 0:\n",
    "            print('Train Epoch: {} [{:5d}/{:5d} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use the test dataset to evaluate our model\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  mnist_data_test, batch_size=batch_size, #shuffle=True,\n",
    "  num_workers=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0., 0\n",
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data)\n",
    "    test_loss += loss_fn(output, target).item() # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the indices of the max log-probability\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum().numpy()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    correct * 100. / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do UMAP on the final hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This maps the result of the projection directly into the results numpy arrays\n",
    "xs_cnn, ys_cnn = np.zeros( (len(data_set), proj_dim) ), np.zeros( (len(data_set),) )\n",
    "model.eval()\n",
    "for i_batch, (x_batch, y_batch) in enumerate(data_loader):\n",
    "    data = x_batch.to(device)\n",
    "    output = model(data, middle_layer=True)\n",
    "    x_cnn = output.cpu().detach().numpy()\n",
    "    xs_cnn[i_batch*batch_size:(i_batch+1)*batch_size, :] = x_cnn\n",
    "    ys_cnn[i_batch*batch_size:(i_batch+1)*batch_size] = y_batch.detach()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "reducer_cnn = umap.UMAP(random_state=42)\n",
    "reducer_cnn.fit(xs_cnn) \n",
    "print(\"Fitting %d-D to CNN middle layer took %d secs\" % (proj_dim, time.time()-t0,) )  \n",
    "# 50d : 70 secs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cnn = reducer_cnn.transform(xs_cnn)\n",
    "embedding_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "# Other potential colormaps : Spectral, tab10, tab20, tab20b?, terrain, jet\n",
    "plt.scatter(embedding_cnn[:, 0], embedding_cnn[:, 1], c=ys_cnn, cmap='Spectral', s=5) \n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP %d-D CNN middle layer for MNIST' % (proj_dim,), fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "*  Investigate what the UMAP 'reducer' and 'embeddings' are doing\n",
    "   +   What does the embedding represent? \n",
    "   +   Is the embedding usable in its own right?\n",
    "*  Train MNIST but leave several digits out\n",
    "   +   Look at where they would lie in UMAP space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
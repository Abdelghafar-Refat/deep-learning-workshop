*  Meta-Learning workshop ?
   +  http://metalearning.ml/#schedule
      -  Submission deadline: 17 October 2018 (Anywhere on Earth)
      -  Notification: 23 November 2018
      -  Camera ready: 3 December 2018
      -  Workshop: 8 December 2018  (Saturday)
   +  Tiny ImageNet (default course project for Stanford CS231N)
      -  https://tiny-imagenet.herokuapp.com/ : also : https://www.kaggle.com/c/tiny-imagenet/data
         *  train.images.zip 194.08 MB
      -  Tiny Imagenet has 200 classes. 
      -  Each class has 500 training images, 50 validation images, and 50 test images. 
      -  Training and validation sets with labels, images and bounding boxes. 
      -  Only class label to be predicted.  Test labels not released.
   +  Pick top-k using pretrained network
   +  Then fine-train a meta-learned network to differentiate between the top-k (a mini-batch-worth?)
      -  What does this actually mean?
      -  The search for the top-k has produced a list of images with similar logits to the test image
      -  But these images probably have different classes (up to k different ones)
      -  Want to create a new model (meta-learned) that distinguishes between *classes* based on the logits
      -  Loss for the meta-learned model could be :
         *  Regular cross-entropy (between k examples and their class labels) after n-optimiser-steps
            -  to avoid renumbering the labels, use real ones.  
            -  Except it might just learn to do 'argmax'
            -  OTOH, the argmax position information is somewhat factored into the search step already
            -  So, perhaps the meta-learner could just build a refined model (like the SVM step in 'my' transfer learning)
         *  Have a pair-wise comparison model, and train it to learn the co-occurrence matrix in only n-steps
            -  Then pair-wise compare the test vector vs all the searched ones, and vote...
            -  Possibly make loss dependent on final scoring rather than exclusively co-occurrence matrix fidelity
   +  Use that to raise 70-80s top-1 to 90s top-n (?)
      -  Problem: All images are really tiny, and so many mistakes are 'understandable'

   +  Useful repos
      -  ** Tiny ImageNet evaluaton server
         *  Data download : http://cs231n.stanford.edu/tiny-imagenet-200.zip
         *  https://tiny-imagenet.herokuapp.com/
         
      -  ** Success in Kaggle Tiny ImageNet (83.3% = 2nd place) 
         *  Had to restructure the original folders to fit Keras' standard ingestion
         *  Fine-tuned pre-trained Xception network
            *  Just expands small images to regular size using load_img(target_size=())
            *  Freeze first 20 layers  :: for layer in pre_trained_model.layers[:20]:  layer.trainable = False
            *  Load model with (include_top=False, pooling='avg') + Dense(200, softmax) on top
            *  Augmentations : ... /blob/master/train_with_Xception.py#L72
         *  https://github.com/ShehabMMohamed/TinyImageNet-KaggleCompetition
         
      -  ** Handle PyTorch DataSet for original data  (MIT)
         *  https://github.com/leemengtaiwan/tiny-imagenet
            *  Augmentations : ... /blob/master/tiny-imagenet.ipynb
            
      -  ** Pretrained xception for PyTorch  (BSD3)
         *  https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/xception.py 
            
      -  Fine tuned a pre-trained net, Google's InceptionV3 on the Tiny ImageNet dataset
         *  No LICENSE file
         *  https://github.com/nexus-kgp/transfer-learning-inception-v3
      -  Misc experiments
         *  No LICENSE file
         *  https://github.com/ZoeYUU/Tiny_ImageNet_Challenge
      -  Kaggle competition page
         *  Not clear that this is identical to the real thing
         *  https://www.kaggle.com/c/tiny-imagenet/data




Set-up:
git clone https://github.com/mdda/deep-learning-workshop.git

PROJECTBASE=deep-learning-workshop/notebooks/work-in-progress/pay-attention-to-training-set
cd ${PROJECTBASE}

wget http://cs231n.stanford.edu/tiny-imagenet-200.zip  # Length: 248100043 (237M) [application/zip]
unzip tiny-imagenet-200.zip 
rm tiny-imagenet-200.zip 

# now have BASE/tiny-imagenet-200/
## drwxrwxr-x.   3 andrewsm andrewsm    4096 Dec 12  2014 test
## drwxrwxr-x. 202 andrewsm andrewsm    4096 Dec 12  2014 train
## drwxrwxr-x.   3 andrewsm andrewsm    4096 Dec 12  2014 val
## -rw-rw-r--.   1 andrewsm andrewsm    2000 Feb  9  2015 wnids.txt
## -rw-------.   1 andrewsm andrewsm 2655750 Feb  9  2015 words.txt

ls -l tiny-imagenet-200/train/ | wc
#    201    1802   12010   # 200 class directories
ls -l tiny-imagenet-200/train/n02415577/images/ | wc
#    501    4502   34401   # Each class has 500 images in it

ls -l tiny-imagenet-200/val/images/ | wc
#  10001   90002  638902   # 10000 validation images

head tiny-imagenet-200/val/val_annotations.txt 
# val_0.JPEG	n03444034	0	32	44	62
# val_1.JPEG	n04067472	52	55	57	59
# val_2.JPEG	n04070727	4	0	60	55

ls -l tiny-imagenet-200/test/images/ |wc
#  10001   90002  648902   # Lots of images


# Now fine-tune an xception model 
#   Model downloaded : 91,674,713 bytes
# Ensure you're in a virtualenv that has python3 and pytorch, torchvision installed
#  Also probably a good idea to do this within a ```screen```

python train_xception.py # Defaults already set for a complete run of 50 epochs 
# P100 1 epoch=1324sec = 22mins, so 50epochs = 18hrs

python train_xception.py --checkpoint=./checkpoints/model_xception_latest.pth --epoch=2

python train_xception.py --checkpoint=./checkpoints/model_xception_0002.pth  # Updated version contains optimizer state and epoch number

export INSTANCE_NAME="rdai-tts-p100-vm"  # As above
gcloud compute scp $INSTANCE_NAME:~/deep-learning-workshop/notebooks/work-in-progress/pay-attention-to-training-set/checkpoints/model_xception_0035-preserve.pth checkpoints/
# This achieves ~3.5Mb/s to download 158Mb
gcloud compute scp $INSTANCE_NAME:~/deep-learning-workshop/notebooks/work-in-progress/pay-attention-to-training-set/checkpoints/model_xception_0052.pth checkpoints/

# Try again, using reduceonplateau - to get ~77.55% validation set accuracy
gcloud compute scp $INSTANCE_NAME:~/deep-learning-workshop/notebooks/work-in-progress/pay-attention-to-training-set/checkpoints-04-sgd-reduceonplateau/model_xception_0021.pth ./checkpoints-04-sgd-reduceonplateau/
gcloud compute scp $INSTANCE_NAME:~/deep-learning-workshop/notebooks/work-in-progress/pay-attention-to-training-set/checkpoints-04-sgd-reduceonplateau/model_xception_0038.pth ./checkpoints-04-sgd-reduceonplateau/


python score_model.py --model=xception --checkpoint=./checkpoints-04-sgd-reduceonplateau/model_xception_0038.pth
#   Score acc: 77.58

python train_xception.py --checkpoint=./checkpoints-04-sgd-reduceonplateau/model_xception_0038.pth --save_trainvalues=./tiny-imagenet-200_trainval.pth
# Max GPU RAM : 6257Mb (95% usage for Titan X Maxwell)
# Time used to generate features 861.8secs

python train_judge.py --checkpoint=./checkpoints-04-sgd-reduceonplateau/model_xception_0038.pth --trainvalues=./tiny-imagenet-200_trainval.pth



TODO: 
-----

DONE : Try different Optimizers : (Adam=poor, compared to SGD with momentum)
DONE : Try different LR-schedulers : (LRStep difficult to judge.  ReduceOnPlateau may make most sense)

DONE : Move model-xception-fine definition code to xception.py
DONE : TZ-aware estimates of finish time

DONE : Create a model evaluation script
DONE : Test downloaded checkpoint score(s)

DONE : Convert all training images to features

DONE : Check a few validation image logits to prove that similar images are retrieved ::

Target= 28, Found :  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28, Weights: +0.55, +0.51, +0.50, +0.49, +0.49, +0.49, +0.49, +0.49, +0.49, +0.48, +0.47, +0.47, +0.47, +0.46, +0.46, +0.46
Target=161, Found : 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, Weights: +1.00, +0.62, +0.57, +0.56, +0.56, +0.56, +0.56, +0.56, +0.56, +0.55, +0.55, +0.54, +0.54, +0.54, +0.54, +0.54
Target= 79, Found :  89,  89,  89,  89,  89, 122,  89,  89,  89,  89,  89,  92,  89,  89,  89,  89, Weights: +0.57, +0.56, +0.53, +0.51, +0.51, +0.51, +0.50, +0.50, +0.50, +0.49, +0.49, +0.49, +0.48, +0.47, +0.47, +0.47
Target= 23, Found :  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14, 109, 196,  14, Weights: +0.65, +0.65, +0.62, +0.62, +0.62, +0.61, +0.61, +0.61, +0.61, +0.59, +0.59, +0.57, +0.57, +0.56, +0.56, +0.56
Target=188, Found : 188, 188, 188, 188, 188, 188, 188, 188, 188, 188, 188, 188, 188, 188, 188, 188, Weights: +0.81, +0.59, +0.57, +0.55, +0.55, +0.54, +0.53, +0.53, +0.53, +0.51, +0.51, +0.50, +0.50, +0.50, +0.50, +0.50
Target= 64, Found :  64,  64,  64,  64,  64,  64,  64, 152,  64, 152,  64,  64,  64,  64,  64,  64, Weights: +0.92, +0.52, +0.51, +0.51, +0.50, +0.50, +0.49, +0.48, +0.48, +0.48, +0.47, +0.47, +0.46, +0.46, +0.46, +0.45
Target=140, Found : 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, Weights: +0.64, +0.61, +0.60, +0.60, +0.60, +0.59, +0.59, +0.58, +0.58, +0.56, +0.56, +0.55, +0.55, +0.54, +0.52, +0.52
Target=178, Found : 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, Weights: +0.68, +0.67, +0.66, +0.66, +0.65, +0.65, +0.65, +0.64, +0.63, +0.63, +0.63, +0.63, +0.63, +0.63, +0.62, +0.62
Target= 15, Found :  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15, Weights: +0.77, +0.65, +0.62, +0.61, +0.60, +0.60, +0.60, +0.59, +0.59, +0.57, +0.56, +0.56, +0.56, +0.55, +0.55, +0.55
Target= 53, Found :  53,  53,  53,  53,  53,  53,  53,  53,  53,  53,  53,  53,  53,  53,  53,  53, Weights: +0.88, +0.66, +0.65, +0.64, +0.64, +0.62, +0.62, +0.61, +0.60, +0.60, +0.60, +0.59, +0.59, +0.59, +0.59, +0.58
Target=  0, Found : 186, 149,  92,  92,   0, 186, 186,  83, 109, 187, 186,  83, 186,   0, 186, 187, Weights: +0.52, +0.51, +0.50, +0.50, +0.49, +0.48, +0.48, +0.47, +0.47, +0.47, +0.47, +0.46, +0.46, +0.46, +0.46, +0.45
Target= 74, Found :  74,  74, 158,  82,  82,  74,  74,  82,  82,  82,  82,  82, 171,  82, 158, 158, Weights: +0.86, +0.48, +0.48, +0.45, +0.42, +0.41, +0.41, +0.40, +0.38, +0.38, +0.37, +0.37, +0.36, +0.36, +0.36, +0.35
Target= 65, Found : 149, 109, 129, 109,  14,  14, 125,  95,  14, 183,  21,  92,  92, 150,  14, 132, Weights: +0.48, +0.43, +0.42, +0.42, +0.40, +0.40, +0.40, +0.38, +0.38, +0.37, +0.37, +0.37, +0.37, +0.37, +0.37, +0.36
Target=155, Found : 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, Weights: +1.00, +0.73, +0.70, +0.70, +0.69, +0.69, +0.67, +0.66, +0.66, +0.66, +0.66, +0.66, +0.65, +0.65, +0.64, +0.64
Target= 77, Found :  77,  77,  77, 119,  77,  77,  77, 119, 119, 119,  77, 119,  77,  77,  77,  77, Weights: +1.00, +0.70, +0.60, +0.56, +0.56, +0.55, +0.54, +0.53, +0.52, +0.52, +0.52, +0.52, +0.52, +0.51, +0.51, +0.50
Target= 57, Found :  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57, Weights: +0.86, +0.77, +0.75, +0.75, +0.74, +0.74, +0.74, +0.74, +0.74, +0.73, +0.73, +0.73, +0.73, +0.72, +0.72, +0.72
Target= 80, Found :  80, 175,  98, 131,  40,  16,  19,  98,  52,  88, 142,  40,  40,  40, 122, 139, Weights: +0.45, +0.41, +0.39, +0.39, +0.38, +0.36, +0.35, +0.35, +0.35, +0.35, +0.34, +0.34, +0.34, +0.34, +0.34, +0.33
Target=162, Found : 162,   6, 190, 190, 162, 192,  85,   6, 134,   6, 190, 190,   6, 162,  85,   6, Weights: +0.87, +0.54, +0.53, +0.50, +0.49, +0.49, +0.49, +0.49, +0.49, +0.49, +0.48, +0.48, +0.48, +0.48, +0.48, +0.47
Target= 29, Found :  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29, Weights: +0.68, +0.54, +0.50, +0.47, +0.46, +0.46, +0.45, +0.45, +0.45, +0.44, +0.44, +0.44, +0.44, +0.44, +0.44, +0.43
Target=104, Found : 104, 174, 174,  72, 104, 174, 104, 174, 174, 174, 104, 174, 174, 104, 104, 104, Weights: +1.00, +0.52, +0.48, +0.46, +0.44, +0.44, +0.44, +0.42, +0.42, +0.42, +0.42, +0.42, +0.42, +0.42, +0.42, +0.41
Target=  5, Found :  46,  46,  46,  46,  46,  46,   5,  46,  46,  46,  46,  46,  46,  46,  46,  46, Weights: +0.43, +0.42, +0.42, +0.40, +0.40, +0.40, +0.40, +0.40, +0.40, +0.39, +0.39, +0.39, +0.39, +0.39, +0.39, +0.39
Target=117, Found : 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, Weights: +0.85, +0.53, +0.48, +0.45, +0.45, +0.44, +0.44, +0.43, +0.43, +0.43, +0.42, +0.41, +0.41, +0.41, +0.41, +0.41
Target=187, Found : 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, Weights: +0.82, +0.71, +0.70, +0.69, +0.68, +0.68, +0.68, +0.67, +0.66, +0.66, +0.66, +0.65, +0.65, +0.65, +0.65, +0.65
Target= 67, Found :  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67, Weights: +0.89, +0.67, +0.61, +0.58, +0.58, +0.56, +0.54, +0.53, +0.53, +0.53, +0.53, +0.52, +0.52, +0.51, +0.51, +0.51
Target= 15, Found :  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15, Weights: +1.00, +0.49, +0.46, +0.45, +0.43, +0.43, +0.42, +0.42, +0.42, +0.42, +0.41, +0.41, +0.40, +0.40, +0.40, +0.40
Target= 39, Found :  39, 184, 184, 184,  39, 184, 184,  39, 184, 184, 184, 184,  39,  39, 184, 184, Weights: +1.00, +0.66, +0.64, +0.62, +0.59, +0.59, +0.59, +0.59, +0.59, +0.57, +0.57, +0.57, +0.56, +0.56, +0.55, +0.55
Target=142, Found : 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, Weights: +0.86, +0.55, +0.54, +0.51, +0.51, +0.51, +0.51, +0.50, +0.50, +0.50, +0.49, +0.49, +0.48, +0.48, +0.48, +0.48
Target=179, Found : 179, 179, 179, 179, 178, 179, 179, 178, 179, 179, 179, 178, 179, 179, 179, 179, Weights: +1.00, +0.51, +0.50, +0.50, +0.46, +0.46, +0.45, +0.45, +0.45, +0.45, +0.44, +0.44, +0.44, +0.44, +0.43, +0.43
Target= 69, Found :  69,  66,  66,  66,  56,  52,  66,  66,  52,  66,  66,  66,  66,  52,  66,  69, Weights: +0.52, +0.41, +0.39, +0.38, +0.37, +0.37, +0.35, +0.35, +0.35, +0.34, +0.34, +0.34, +0.33, +0.33, +0.33, +0.33
Target=189, Found : 189, 189, 189, 189, 189, 189, 189, 189, 189, 189, 189, 189, 189, 189, 189, 189, Weights: +1.00, +0.62, +0.61, +0.60, +0.59, +0.59, +0.59, +0.59, +0.59, +0.58, +0.58, +0.58, +0.57, +0.57, +0.57, +0.57
Target=  6, Found :   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6, Weights: +0.91, +0.71, +0.67, +0.66, +0.66, +0.66, +0.65, +0.64, +0.64, +0.63, +0.63, +0.63, +0.63, +0.63, +0.63, +0.62
Target= 27, Found :  27,  27,  27,  27,  27,  74,  27,  74,  27,  27,  27,  27,  74,  27,  27,  27, Weights: +0.88, +0.46, +0.44, +0.43, +0.43, +0.43, +0.43, +0.42, +0.41, +0.41, +0.40, +0.40, +0.39, +0.39, +0.39, +0.39

TODO 
  Should ignore first column in each group as (likely) it contains the training datapoint itself
  Maybe pass back training example idx to caller
  Add train_judge.py to git
  Check that x_hat=(x-means)/norm means that x_hat is now mean==0 and stdev==1
  Check with validation transforms (top-1 should be 100% match)
  Check on validation set 
  See whether top-n logits correspond (at all) to top-n retrievals (requires model up to logit layer)
  Maybe need to get 10 of each training class from the list of 'close' classes
  Move pre-proc + attn into xception.py
  Score for picking (say) max-vote in top-16 (x first one)
  Think about subtracting/projecting x_features from all found examples (~PCA)


New training loop to train a 16-channel logit analysis tool to 'do better' than the regular network on 'test image' logits

New meta-training loop to train a model that learns to classify the 16 examples more firmly, and then gets applied to 'test image' logits

? Check a few validation image network random projections to prove that similar images are retrieved


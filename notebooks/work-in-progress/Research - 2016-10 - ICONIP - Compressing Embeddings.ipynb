{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Word Embeddings\n",
    "\n",
    "Downloadable version of GloVe embedding (with fallback source).\n",
    "\n",
    "Probably best to include instructions for Levy test-suite installation, so that any given embedding can be tested.\n",
    "\n",
    "Then require two main sections : \n",
    " \n",
    "*  Lloyd embedding generation\n",
    "\n",
    "*  Sparsified embedding generation\n",
    "\n",
    "Include downloadable version of sparsified GloVe embedding from own hosting.\n",
    "\n",
    "And functions/tools to play with the loaded embedding (of whatever type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Omer-Levy Test Regime\n",
    "\n",
    "https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf\n",
    "\n",
    "```\n",
    "wget https://bitbucket.org/omerlevy/hyperwords/get/688addd64ca2.zip\n",
    "unzip 688addd64ca2.zip\n",
    "rm 688addd64ca2.zip\n",
    "\n",
    "mv omerlevy-hyperwords-688addd64ca2 omerlevy\n",
    "\n",
    "chmod 755 omerlevy/*.sh omerlevy/scripts/*.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test a (text) Embedding\n",
    "\n",
    "Based on this script : \n",
    "```\n",
    "more omerlevy/test-vectors.sh \n",
    "#!/bin/sh\n",
    "\n",
    "# ./test-vectors.sh /home/andrewsm/sketchpad/redcatlabs/embeddings/data/1-glove-1-billion-and-wiki/window11-lc-36/vectors.txt \n",
    "\n",
    "# arg1 == filepath of word-vectors file\n",
    "VECTORS_FILE=$1\n",
    "  \n",
    "# Fix up the 'file header' of a 'glove' vectors file into the one expected here\n",
    "VECTORS_WORDS=${VECTORS_FILE}.words\n",
    "\n",
    "if [ ! -f ${VECTORS_WORDS} ]; then \n",
    "  echo \"Creating ${VECTORS_WORDS}\"\n",
    "  #echo \"262144 300\" > ${VECTORS_WORDS}\n",
    "  #head -262144 ${VECTORS_FILE} >> ${VECTORS_WORDS}\n",
    "\n",
    "  ## Glove min-freq : 36 -> 263633 words (just above 12^18=262144 words)\n",
    "  echo \"131072 300\" > ${VECTORS_WORDS}\n",
    "  head -131072 ${VECTORS_FILE} >> ${VECTORS_WORDS}\n",
    "fi\n",
    "\n",
    "VECTORS_NPY=${VECTORS_WORDS}.npy\n",
    "\n",
    "\n",
    "#word2vecf/word2vecf -train w2.sub/pairs -pow 0.75 -cvocab w2.sub/counts.contexts.vocab -wvocab w2.sub/counts.words.vocab -dumpcv w2.sub/sgns.contexts -output w2.sub/sgns.words -threads 10 -\n",
    "negative 15 -size 500;\n",
    "\n",
    "python hyperwords/text2numpy.py ${VECTORS_WORDS}\n",
    "\n",
    "# No need for this temporary file now\n",
    "##rm ${VECTORS_WORDS}\n",
    "\n",
    "\n",
    "#python hyperwords/text2numpy.py w2.sub/sgns.contexts\n",
    "#rm w2.sub/sgns.contexts\n",
    "\n",
    "\n",
    "echo\n",
    "echo \"Similarity\"\n",
    "echo \"----------\"\n",
    "# Evaluate on Word Similarity\n",
    "#python hyperwords/ws_eval.py --neg 5 PPMI  w2.sub/pmi testsets/ws/ws353.txt\n",
    "#python hyperwords/ws_eval.py --eig 0.5 SVD w2.sub/svd testsets/ws/ws353.txt\n",
    "#python hyperwords/ws_eval.py --w+c SGNS    w2.sub/sgns testsets/ws/ws353.txt\n",
    "\n",
    "#echo -n \"WS353 Results     \"\n",
    "#python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/ws353.txt\n",
    "\n",
    "echo -n \"WS353 Similarity  \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/ws353_similarity.txt\n",
    "\n",
    "echo -n \"WS353 Relatedness \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/ws353_relatedness.txt\n",
    "\n",
    "echo -n \"Bruni MEN         \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/bruni_men.txt\n",
    "\n",
    "echo -n \"Radinsky M.Turk   \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/radinsky_mturk.txt\n",
    "\n",
    "echo -n \"Luoung Rare Words \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/luong_rare.txt\n",
    "\n",
    "echo\n",
    "echo \"Geometry\"\n",
    "echo \"--------\"\n",
    "# Evaluate on Analogies\n",
    "#python hyperwords/analogy_eval.py PPMI        w2.sub/pmi testsets/analogy/google.txt\n",
    "#python hyperwords/analogy_eval.py --eig 0 SVD w2.sub/svd testsets/analogy/google.txt\n",
    "#python hyperwords/analogy_eval.py SGNS        w2.sub/sgns testsets/analogy/google.txt\n",
    "\n",
    "echo -n \"Google Analogy Results  \"\n",
    "python hyperwords/analogy_eval.py VECTORS ${VECTORS_FILE} testsets/analogy/google.txt\n",
    "\n",
    "echo -n \"MSR Analogy Results     \"\n",
    "python hyperwords/analogy_eval.py VECTORS ${VECTORS_FILE} testsets/analogy/msr.txt\n",
    "\n",
    "echo\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "def test_embedding_file(vectors_txt, vocab_max=131072 ):\n",
    "    # Do we need to process VECTORS_FILE->{ VECTORS_WORDS, VECTORS_NPY }?\n",
    "    # Answer = YES : the .words is required, and is used to create .npy and .vocab\n",
    "    \n",
    "    vectors_txt_words = '%s.words' % (vectors_txt,)\n",
    "    if not os.path.isfile(vectors_txt_words):\n",
    "        # This is just a copy of 'text file' with the vocab_size and embedding_size pre-pended\n",
    "        #echo \"131072 300\" > ${VECTORS_WORDS}\n",
    "        #head -131072 ${VECTORS_FILE} >> ${VECTORS_WORDS}\n",
    "        with open(vectors_txt) as fin:\n",
    "            first_line = fim.readline()\n",
    "            embedding_dim = len(first_line.strip().split()) -1 \n",
    "            vocab_size = len(fin.readlines()) +1  # Ouch! - read in whole file to find length\n",
    "\n",
    "        if vocab_size>vocab_max:\n",
    "            vocab_size=vocab_max\n",
    "            \n",
    "        with open(vectors_txt) as fin:\n",
    "            with open(vectors_txt_words, 'wt') as fout:\n",
    "                # Write the first line, which, ironically, will be discarded by the omerlevy code\n",
    "                fout.write(\"%d %d\\n\" % (vocab_size, embedding_dim))\n",
    "                \n",
    "                # And copy over at most vocab_max lines of the original file \n",
    "                for i, line in enumerate(fin.readlines()):\n",
    "                    if i>vocab_size:\n",
    "                        break\n",
    "                    fout.write(line)\n",
    "                \n",
    "    vectors_txt_npy   = '%s.npy' % (vectors_txt_words,)\n",
    "    if not os.path.isfile(vectors_txt_words):\n",
    "        # Sadly, we can't just invoke this as a python function - need to go via shell...\n",
    "        subprocess.call([ \"python\", \"hyperwords/text2numpy.py\", vectors_txt_words ])\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
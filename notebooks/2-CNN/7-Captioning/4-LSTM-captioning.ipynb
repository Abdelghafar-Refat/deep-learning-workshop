{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Captioning\n",
    "\n",
    "This is a very basic model: \n",
    "\n",
    "*  Take the featurized images (2048d), and tokenised captions\n",
    "*  Add a (trainable) features -> 50d dense layer\n",
    "*  Use a 50d GloVe embedding (for the LSTM inputs, non-trainable)\n",
    "   *  #stop-words ~ 150 (say)\n",
    "*  50d of hidden units for the LSTM\n",
    "*  But have a 'pluggable' output transform :\n",
    "   *   Concat : (256 one-hot - including '0'=mask, '1'={UNK}, '2'={START}, '3'={STOP}, '4'={UseOther})\n",
    "   *   (a) UseOther + (8192-250 of more one-hot)\n",
    "   *   (b) UseOther + (50d of same GloVe embedding, for nearest-neighbour)\n",
    "   *   (c) UseOther + (log2(8192)==13 bits + error correction of index of word)\n",
    "*  Want to monitor some kind of score over time for test cases   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "TRAIN_PCT=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the captions/corpus/embedding\n",
    "with open('./data/cache/CAPTIONS_data_Flickr30k_2017-06-07_23-15.pkl', 'rb') as f:\n",
    "    text_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "text_data ~ dict(\n",
    "    img_to_captions = img_to_valid_captions,\n",
    "    \n",
    "    action_words = action_words, \n",
    "    stop_words = stop_words_sorted,\n",
    "    \n",
    "    embedding = embedding,\n",
    "    embedding_word_arr = embedding_word_arr,\n",
    "    \n",
    "    img_arr = img_arr_save,\n",
    "    train_test = np.random.random( (len(img_arr_save),) ),\n",
    ")\"\"\"\n",
    "\n",
    "dictionary = { w:i for i,w in enumerate(text_data['embedding_word_arr']) }\n",
    "\n",
    "img_arr_train = [ img for i, img in enumerate(text_data['img_arr']) if text_data['train_test'][i]<TRAIN_PCT ]\n",
    "\n",
    "print(\"Loaded captions, corpus and embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the features\n",
    "with open('./data/cache/FEATURES_data_Flickr30k_flickr30k-images_2017-06-06_18-07.pkl', 'rb') as f:\n",
    "    image_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "image_data ~ dict(\n",
    "    features = features,\n",
    "    img_arr = img_arr,\n",
    ")\n",
    "\"\"\"\n",
    "image_feature_idx = { img:idx for idx, img in enumerate(image_data['img_arr']) }\n",
    "\n",
    "print(\"Loaded image features for all images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CAPTION_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_to_idx_arr(caption):  # This is actually 1 longer than max - need to shift about a bit later\n",
    "    ret = np.zeros( (CAPTION_LEN+1,), dtype='int32')  # {MASK}.idx===0\n",
    "    ret[0] = dictionary['{START}']\n",
    "    for i, w in enumerate( caption.lower().split() ):\n",
    "        ret[i+1] = dictionary.get(w, dictionary['{UNK}'])\n",
    "    ret[i+2] = dictionary['{STOP}']\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for j in range(0,10):\n",
    "#    print(j)\n",
    "#print(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_training_example():\n",
    "    img_arr = img_arr_train\n",
    "    while True:\n",
    "        random.shuffle( img_arr )\n",
    "        for img in img_arr:\n",
    "            captions = text_data['img_to_captions'][img]\n",
    "            caption = random.choice(captions)\n",
    "            print(caption)\n",
    "            yield image_feature_idx[ img ], caption_to_idx_arr( caption )\n",
    "        print(\"Captions : Looping\")\n",
    "caption_training_example_gen = caption_training_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next(caption_training_example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Captioning\n",
    "\n",
    "This is a very basic model: \n",
    "\n",
    "*  Take the featurized images (2048d), and tokenised captions\n",
    "*  Add a (trainable) features -> 50d dense layer\n",
    "*  Use a 50d GloVe embedding (for the LSTM inputs, non-trainable)\n",
    "   *  #stop-words ~ 150 (say)\n",
    "*  50d of hidden units for the LSTM\n",
    "*  But have a 'pluggable' output transform :\n",
    "   *   Concat : (256 one-hot - including '0'=mask, '1'={UNK}, '2'={START}, '3'={STOP}, '4'={UseOther})\n",
    "   *   (a) UseOther + (8192-250 of more one-hot)\n",
    "   *   (b) UseOther + (50d of same GloVe embedding, for nearest-neighbour)\n",
    "   *   (c) UseOther + (log2(8192)==13 bits + error correction of index of word)\n",
    "*  Want to monitor some kind of score over time for test cases   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "TRAIN_PCT=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the captions/corpus/embedding\n",
    "with open('./data/cache/CAPTIONS_data_Flickr30k_2017-06-07_23-15.pkl', 'rb') as f:\n",
    "    text_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "text_data ~ dict(\n",
    "    img_to_captions = img_to_valid_captions,\n",
    "    \n",
    "    action_words = action_words, \n",
    "    stop_words = stop_words_sorted,\n",
    "    \n",
    "    embedding = embedding,\n",
    "    embedding_word_arr = embedding_word_arr,\n",
    "    \n",
    "    img_arr = img_arr_save,\n",
    "    train_test = np.random.random( (len(img_arr_save),) ),\n",
    ")\"\"\"\n",
    "\n",
    "embedding = text_data['embedding']\n",
    "vocab_arr = text_data['embedding_word_arr']\n",
    "dictionary = { w:i for i,w in enumerate(vocab_arr) }\n",
    "\n",
    "img_arr_train = [ img for i, img in enumerate(text_data['img_arr']) if text_data['train_test'][i]<TRAIN_PCT ]\n",
    "\n",
    "print(\"Loaded captions, corpus and embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the features\n",
    "with open('./data/cache/FEATURES_data_Flickr30k_flickr30k-images_2017-06-06_18-07.pkl', 'rb') as f:\n",
    "    image_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "image_data ~ dict(\n",
    "    features = features,\n",
    "    img_arr = img_arr,\n",
    ")\n",
    "\"\"\"\n",
    "image_feature_idx = { img:idx for idx, img in enumerate(image_data['img_arr']) }\n",
    "\n",
    "print(\"Loaded image features for all images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CAPTION_LEN = 32\n",
    "EMBEDDING_DIM = embedding.shape[1]\n",
    "VOCAB_SIZE = len(vocab_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_to_idx_arr(caption):  # This is actually 1 longer than max - need to shift about a bit later\n",
    "    ret = np.zeros( (CAPTION_LEN+1,), dtype='int32')  # {MASK}.idx===0\n",
    "    ret[0] = dictionary['{START}']\n",
    "    for i, w in enumerate( caption.lower().split() ):\n",
    "        ret[i+1] = dictionary.get(w, dictionary['{UNK}'])\n",
    "    ret[i+2] = dictionary['{STOP}']\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for j in range(0,10):\n",
    "#    print(j)\n",
    "#print(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_training_example():\n",
    "    img_arr = img_arr_train\n",
    "    while True:\n",
    "        random.shuffle( img_arr )\n",
    "        for img in img_arr:\n",
    "            captions = text_data['img_to_captions'][img]\n",
    "            caption = random.choice(captions)\n",
    "            print(caption)\n",
    "            yield image_feature_idx[ img ], caption_to_idx_arr( caption )\n",
    "        print(\"Captions : Looping\")\n",
    "caption_training_example_gen = caption_training_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next(caption_training_example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow / Keras imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.keras\n",
    "from tensorflow.contrib.keras.python.keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create pluggable IO stage for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Direct_to_OneHot():\n",
    "    input_width = EMBEDDING_DIM\n",
    "    output_width = VOCAB_SIZE\n",
    "    output_cross_entropy_width = VOCAB_SIZE\n",
    "    \n",
    "    #def __init__(self):\n",
    "    #    pass\n",
    "    \n",
    "    def to_input(caption_arr):  # (ignore last entry of caption_arr)\n",
    "        # Input is plain embedding of each symbol\n",
    "        return embedding[ caption_arr[:-1], : ]\n",
    "    \n",
    "    def to_output(caption_arr): # (ignore first entry in caption_arr)\n",
    "        # Output desired is one-hot of each symbol (cross entropy match this) \n",
    "        return to_categorical(caption_arr[1:], num_classes=VOCAB_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_width = len(text_data['action_words']) + len(text_data['stop_words'])\n",
    "EXTRA_idx = dictionary['{EXTRA}']\n",
    "\n",
    "def OneHotBasePlus(arr):\n",
    "    one_hot_base_plus = np.zeros( (base_width, arr.shape[0]), dtype='float32')\n",
    "    one_hot_base_plus[ EXTRA_idx, arr>=base_width] = 1.0\n",
    "    #one_hot_base_plus[ arr, arr<base_width] = 1.0\n",
    "    return one_hot_base_plus\n",
    "    #return np.where(arr<base_width, \n",
    "    #                embedding[arr],\n",
    "    #                embedding[arr]\n",
    "    #               )\n",
    "\n",
    "class OneHotBasePlusEmbedding_to_Same():\n",
    "    \n",
    "    input_width = base_width + EMBEDDING_DIM\n",
    "    output_width = input_width\n",
    "    output_cross_entropy_width = base_width\n",
    "    \n",
    "    #def __init__(self):\n",
    "    #    pass\n",
    "    \n",
    "    def to_input(caption_arr):  # (ignore last entry of caption_arr)\n",
    "        # Input is plain embedding of each symbol\n",
    "        return embedding[ caption_arr[:-1], : ]\n",
    "    \n",
    "    def to_output(caption_arr): # (ignore first entry in caption_arr)\n",
    "        # Output desired is one-hot of each symbol (cross entropy match this) \n",
    "        return to_categorical(caption_arr[1:], num_classes=VOCAB_SIZE)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io = Direct_to_OneHot\n",
    "io.input_width\n",
    "\n",
    "caption_sample = 'The cat sat on the mat .'\n",
    "caption_sample_idx = caption_to_idx_arr(caption_sample)\n",
    "caption_sample_idx\n",
    "\n",
    "#io.to_input( caption_sample_idx )\n",
    "#io.to_output( caption_sample_idx )\n",
    "\n",
    "x=OneHotBasePlus(caption_sample_idx[0:10])\n",
    "x[EXTRA_idx]\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
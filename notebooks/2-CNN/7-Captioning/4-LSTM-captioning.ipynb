{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Captioning\n",
    "\n",
    "This is a very basic model: \n",
    "\n",
    "*  Take the featurized images (2048d), and tokenised captions\n",
    "*  Add a (trainable) features -> 50d dense layer\n",
    "*  Use a 50d GloVe embedding (for the LSTM inputs, non-trainable)\n",
    "   *  #stop-words ~ 150 (say)\n",
    "*  50d of hidden units for the LSTM\n",
    "*  But have a 'pluggable' output transform :\n",
    "   *   Concat : (256 one-hot - including '0'=mask, '1'={UNK}, '2'={START}, '3'={STOP}, '4'={UseOther})\n",
    "   *   (a) UseOther + (8192-250 of more one-hot)\n",
    "   *   (b) UseOther + (50d of same GloVe embedding, for nearest-neighbour)\n",
    "   *   (c) UseOther + (log2(8192)==13 bits + error correction of index of word)\n",
    "*  Want to monitor some kind of score over time for test cases   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "TRAIN_PCT=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the captions/corpus/embedding\n",
    "with open('./data/cache/CAPTIONS_data_Flickr30k_2017-06-07_23-15.pkl', 'rb') as f:\n",
    "    text_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "text_data ~ dict(\n",
    "    img_to_captions = img_to_valid_captions,\n",
    "    \n",
    "    action_words = action_words, \n",
    "    stop_words = stop_words_sorted,\n",
    "    \n",
    "    embedding = embedding,\n",
    "    embedding_word_arr = embedding_word_arr,\n",
    "    \n",
    "    img_arr = img_arr_save,\n",
    "    train_test = np.random.random( (len(img_arr_save),) ),\n",
    ")\"\"\"\n",
    "\n",
    "embedding = text_data['embedding']\n",
    "vocab_arr = text_data['embedding_word_arr']\n",
    "dictionary = { w:i for i,w in enumerate(vocab_arr) }\n",
    "\n",
    "img_arr_train = [ img for i, img in enumerate(text_data['img_arr']) if text_data['train_test'][i]<TRAIN_PCT ]\n",
    "\n",
    "print(\"Loaded captions, corpus and embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the features\n",
    "with open('./data/cache/FEATURES_data_Flickr30k_flickr30k-images_2017-06-06_18-07.pkl', 'rb') as f:\n",
    "    image_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "image_data ~ dict(\n",
    "    features = features,\n",
    "    img_arr = img_arr,\n",
    ")\n",
    "\"\"\"\n",
    "image_feature_idx = { img:idx for idx, img in enumerate(image_data['img_arr']) }\n",
    "\n",
    "print(\"Loaded image features for all images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CAPTION_LEN = 32\n",
    "EMBEDDING_DIM = embedding.shape[1]\n",
    "\n",
    "VOCAB_SIZE = len(vocab_arr)\n",
    "LOG2_VOCAB_SIZE = 13  # 1024->10, 8192->13\n",
    "if not (2**LOG2_VOCAB_SIZE/2) < VOCAB_SIZE < 2**LOG2_VOCAB_SIZE:\n",
    "    print(\"LOG2_VOCAB_SIZE incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_to_idx_arr(caption):  # This is actually 1 longer than CAPTION_LEN - need to shift about a bit later\n",
    "    ret = np.zeros( (CAPTION_LEN+1,), dtype='int32')  # {MASK}.idx===0\n",
    "    ret[0] = dictionary['{START}']\n",
    "    for i, w in enumerate( caption.lower().split() ):\n",
    "        ret[i+1] = dictionary.get(w, dictionary['{UNK}'])\n",
    "    ret[i+2] = dictionary['{STOP}']\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for j in range(0,10):\n",
    "#    print(j)\n",
    "#print(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_training_example():\n",
    "    img_arr = img_arr_train\n",
    "    while True:\n",
    "        random.shuffle( img_arr )\n",
    "        for img in img_arr:\n",
    "            captions = text_data['img_to_captions'][img]\n",
    "            caption = random.choice(captions)\n",
    "            print(caption)\n",
    "            yield image_feature_idx[ img ], caption_to_idx_arr( caption )\n",
    "        print(\"Captions : Looping\")\n",
    "caption_training_example_gen = caption_training_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next(caption_training_example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow / Keras imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.keras\n",
    "from tensorflow.contrib.keras.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.contrib.keras.api.keras.losses import cosine_proximity, categorical_crossentropy, mean_squared_error\n",
    "from tensorflow.contrib.keras.api.keras.activations import softmax, sigmoid\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create pluggable IO stages for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RepresentAs_FullEmbedding():\n",
    "    width = EMBEDDING_DIM\n",
    "    \n",
    "    def encode(caption_arr):\n",
    "        # plain embedding of each symbol\n",
    "        return embedding[ caption_arr, : ]\n",
    "\n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        return cosine_proximity( ideal_output, network_output )\n",
    "    \n",
    "class RepresentAs_FullOneHot():\n",
    "    width = VOCAB_SIZE\n",
    "    \n",
    "    def encode(caption_arr):\n",
    "        # Output desired is one-hot of each symbol (cross entropy match whole thing) \n",
    "        return to_categorical(caption_arr, num_classes=VOCAB_SIZE) \n",
    "    \n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        smx = softmax(network_output, axis=-1)\n",
    "        return categorical_crossentropy( ideal_output, smx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_width = len(text_data['action_words']) + len(text_data['stop_words'])\n",
    "MASK_idx = dictionary['{MASK}']   # ==0\n",
    "EXTRA_idx = dictionary['{EXTRA}']\n",
    "\n",
    "def OneHotBasePlus(arr): # Contains indexing magic\n",
    "    #  Arrangement should be :: (samples, timesteps, features),\n",
    "    one_hot_base_plus = np.zeros( (CAPTION_LEN, base_width), dtype='float32')\n",
    "    # Set the indicator for entries not in action or stop words\n",
    "    one_hot_base_plus[ arr>=base_width, EXTRA_idx ] = 1.0\n",
    "    # Set the one-hot for everthing in the one-hot-region\n",
    "    one_hot_base_plus[ arr< base_width, arr[np.where(arr<base_width)] ] = 1.0\n",
    "    # Force masked values to all-zeros\n",
    "    one_hot_base_plus[ arr==0, MASK_idx ] = 0.0\n",
    "    return one_hot_base_plus\n",
    "\n",
    "class RepresentAs_OneHotBasePlusEmbedding():\n",
    "    width = base_width + EMBEDDING_DIM\n",
    "\n",
    "    def encode(caption_arr): \n",
    "        # Input is onehot for first part, with the embedding included for all words too\n",
    "        return np.hstack( [ OneHotBasePlus(caption_arr), embedding[caption_arr] ] )\n",
    "    \n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        # One-hot of each action symbol and stop words (cross entropy match these) and \n",
    "        #   RMSE on remaining embedding (weighted according to onehot[{EXTRA}]~0...1)\n",
    "        \n",
    "        # Perhaps need this idea https://github.com/fchollet/keras/issues/890:\n",
    "        smx = softmax(network_output[:base_width], axis=-1)\n",
    "        \n",
    "        is_extra = smx[:, EXTRA_idx]\n",
    "        one_hot_loss = categorical_crossentropy( ideal_output[:base_width], smx )    \n",
    "        embedding_loss = cosine_proximity( ideal_output[base_width:], \n",
    "                                                network_output[base_width:] )\n",
    "        return (1.-is_extra)*one_hot_loss + (is_extra)*embedding_loss\n",
    "    \n",
    "class RepresentAs_OneHotBasePlusBinaryIdx():\n",
    "    width = base_width + 3*LOG2_VOCAB_SIZE\n",
    "    powers_of_two = 2**np.arange(LOG2_VOCAB_SIZE)\n",
    "\n",
    "    def encode(caption_arr):\n",
    "        # Input is onehot for first part, with 3 copies of the binary index of all words afterwards\n",
    "        #   Idea is from : https://arxiv.org/abs/1704.06918\n",
    "\n",
    "        # Thanks to : https://stackoverflow.com/questions/21918267/\n",
    "        #         convert-decimal-range-to-numpy-array-with-each-bit-being-an-array-element\n",
    "        binary = (caption_arr[:, np.newaxis] & powers_of_two) / powers_of_two\n",
    "        \n",
    "        return np.hstack( [ OneHotBasePlus(caption_arr), binary, binary, binary ] )\n",
    "  \n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        smx = softmax(network_output[:base_width], axis=-1)\n",
    "        sig = sigmoid(network_output[base_width:])\n",
    "        \n",
    "        is_extra = smx[:, EXTRA_idx]\n",
    "        one_hot_loss = categorical_crossentropy( ideal_output[:base_width], smx )\n",
    "        #binary_loss  = categorical_crossentropy( ideal_output[base_width:], sig )\n",
    "        binary_loss  = mean_squared_error( ideal_output[base_width:], sig )  # reported better in paper\n",
    "        return (1.-is_extra)*one_hot_loss + (is_extra)*binary_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#io = RepresentAs_FullEmbedding             # .encode : 32, 50\n",
    "#io = RepresentAs_FullOneHot                # .encode : 32, 6946\n",
    "io = RepresentAs_OneHotBasePlusEmbedding   # .encode : 32, 191 \n",
    "#io = RepresentAs_OneHotBasePlusBinaryIdx   # .encode : 32, 180\n",
    "io.width\n",
    "\n",
    "caption_sample = 'The cat sat on the mat .'\n",
    "caption_sample_idx = caption_to_idx_arr(caption_sample)\n",
    "caption_sample_idx  # array([   2,    8, 1461, 2496,   11,    8,  998,    5,    3,    0,    0...\n",
    "\n",
    "onehot_start=range(0,12)\n",
    "x=OneHotBasePlus(caption_sample_idx[:-1])  # Just the first 10 one-hot entries\n",
    "#x\n",
    "#x[onehot_start, dictionary['{MASK}']]\n",
    "#x[onehot_start, dictionary['{START}']]\n",
    "#x[onehot_start,  EXTRA_idx]\n",
    "#x[onehot_start, dictionary['{STOP}']]\n",
    "#x[onehot_start, dictionary['on']]\n",
    "\n",
    "#x.shape  # 32, 141 \n",
    "#embedding[caption_sample_idx[:-1]].shape  # 32, 50\n",
    "\n",
    "if False:\n",
    "    powers_of_two = 2**np.arange(LOG2_VOCAB_SIZE)\n",
    "    (caption_sample_idx[:, np.newaxis] & powers_of_two) / powers_of_two\n",
    "\n",
    "io.encode( caption_sample_idx[:-1] ).shape  # [0:6,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See : https://keras.io/layers/core/#masking\n",
    "#model = Sequential()\n",
    "#model.add(Masking(mask_value=0., input_shape=(timesteps, features)))\n",
    "#model.add(LSTM(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
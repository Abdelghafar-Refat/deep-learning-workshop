{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Art Style Transfer\n",
    "\n",
    "This notebook is a re-implementation of the algorithm described in \"A Neural Algorithm of Artistic Style\" (http://arxiv.org/abs/1508.06576) by Gatys, Ecker and Bethge. Additional details of their method are available at http://arxiv.org/abs/1505.07376 and http://bethgelab.org/deepneuralart/.\n",
    "\n",
    "An image is generated which combines the content of a photograph with the \"style\" of a painting. This is accomplished by jointly minimizing the squared difference between feature activation maps of the photo and generated image, and the squared difference of feature correlation between painting and generated image. A total variation penalty is also applied to reduce high frequency noise. \n",
    "\n",
    "This notebook was originally sourced from [Lasagne Recipes](https://github.com/Lasagne/Recipes/tree/master/examples/styletransfer), but has been modified to use a GoogLeNet network (pre-trained and pre-loaded), in TensorFlow and given some features to make it easier to experiment with.\n",
    "\n",
    "Other implementations : \n",
    "  *  https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb (with [video](https://www.youtube.com/watch?v=LoePx3QC5Js))\n",
    "  *  https://github.com/cysmith/neural-style-tf\n",
    "  *  https://github.com/anishathalye/neural-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc  # for imresize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "from urllib.request import urlopen  # Python 3+ version (instead of urllib2)\n",
    "\n",
    "import os # for directory listings\n",
    "import pickle\n",
    "\n",
    "AS_PATH='./images/art-style'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add TensorFlow Slim Model Zoo to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "tf_zoo_models_dir = './models/tensorflow_zoo'\n",
    "\n",
    "if not os.path.exists(tf_zoo_models_dir):\n",
    "    print(\"Creating %s directory\" % (tf_zoo_models_dir,))\n",
    "    os.makedirs(tf_zoo_models_dir)\n",
    "if not os.path.isfile( os.path.join(tf_zoo_models_dir, 'models', 'README.md') ):\n",
    "    print(\"Cloning tensorflow model zoo under %s\" % (tf_zoo_models_dir, ))\n",
    "    !cd {tf_zoo_models_dir}; git clone https://github.com/tensorflow/models.git\n",
    "\n",
    "sys.path.append(tf_zoo_models_dir + \"/models/slim\")\n",
    "\n",
    "print(\"Model Zoo model code installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Inception v1 (GoogLeNet) Architecture|\n",
    "\n",
    "![GoogLeNet Architecture](../../images/presentation/googlenet-arch_1228x573.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Inception V1 checkpoint\u00b6\n",
    "\n",
    "Functions for building the GoogLeNet model with TensorFlow / slim and preprocessing the images are defined in ```model.inception_v1_tf``` - which was downloaded from the TensorFlow / slim [Model Zoo](https://github.com/tensorflow/models/tree/master/slim).\n",
    "\n",
    "The actual code for the ```slim``` model will be <a href=\"model/tensorflow_zoo/models/slim/nets/inception_v1.py\" target=_blank>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import dataset_utils\n",
    "\n",
    "targz = \"inception_v1_2016_08_28.tar.gz\"\n",
    "url = \"http://download.tensorflow.org/models/\"+targz\n",
    "checkpoints_dir = './data/tensorflow_zoo/checkpoints'\n",
    "\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)\n",
    "\n",
    "if not os.path.isfile( os.path.join(checkpoints_dir, 'inception_v1.ckpt') ):\n",
    "    tarfilepath = os.path.join(checkpoints_dir, targz)\n",
    "    if os.path.isfile(tarfilepath):\n",
    "        import tarfile\n",
    "        tarfile.open(tarfilepath, 'r:gz').extractall(checkpoints_dir)\n",
    "    else:\n",
    "        dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)\n",
    "        \n",
    "    # Get rid of tarfile source (the checkpoint itself will remain)\n",
    "    os.unlink(tarfilepath)\n",
    "        \n",
    "print(\"Checkpoint available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "\n",
    "IMAGE_W=224\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from model import googlenet\n",
    "#\n",
    "#net = googlenet.build_model()\n",
    "#net_input_var = net['input'].input_var\n",
    "#net_output_layer = net['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#params = pickle.load(open('./data/googlenet/blvc_googlenet.pkl', 'rb'), encoding='iso-8859-1')\n",
    "#model_param_values = params['param values']\n",
    "##classes = params['synset words']\n",
    "#lasagne.layers.set_all_param_values(net_output_layer, model_param_values)\n",
    "#print(\"Loaded Model parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_image(im):\n",
    "    if len(im.shape) == 2:\n",
    "        im = im[:, :, np.newaxis]\n",
    "        im = np.repeat(im, 3, axis=2)\n",
    "        \n",
    "    # Resize so smallest dim = 224, preserving aspect ratio\n",
    "    h, w, _ = im.shape\n",
    "    if h < w:\n",
    "        im = scipy.misc.imresize(im, (224, int(w*224/h)))\n",
    "    else:\n",
    "        im = scipy.misc.imresize(im, (int(h*224/w), 224))\n",
    "\n",
    "    # Central crop to 224x224\n",
    "    h, w, _ = im.shape\n",
    "    im = im[h//2-112:h//2+112, w//2-112:w//2+112]\n",
    "    \n",
    "    rawim = np.copy(im).astype('uint8')\n",
    "    return rawim, im\n",
    "    \n",
    "    # Shuffle axes to c01\n",
    "    #im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "    \n",
    "    # Convert to BGR\n",
    "    #im = im[::-1, :, :]\n",
    "\n",
    "    #MEAN_VALUES = np.array([104, 117, 123]).reshape((3,1,1))\n",
    "    #im = im - MEAN_VALUES\n",
    "    #return rawim, floatX(im[np.newaxis])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choose the Photo to be *Enhanced*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "photos = [ '%s/photos/%s' % (AS_PATH, f) for f in os.listdir('%s/photos/' % AS_PATH) if not f.startswith('.')]\n",
    "photo_i=-1 # will be incremented in next cell (i.e. to start at [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the cell below will iterate through the images in the ```./images/art-style/photos``` directory, so you can choose the one you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "photo_i += 1\n",
    "photo = plt.imread(photos[photo_i % len(photos)])\n",
    "photo_rawim, photo = prep_image(photo)\n",
    "plt.imshow(photo_rawim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the photo with the required 'Style'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "styles = [ '%s/styles/%s' % (AS_PATH, f) for f in os.listdir('%s/styles/' % AS_PATH) if not f.startswith('.')]\n",
    "style_i=-1 # will be incremented in next cell (i.e. to start at [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the cell below will iterate through the images in the ```./images/art-style/styles``` directory, so you can choose the one you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_i += 1\n",
    "style = plt.imread(styles[style_i % len(styles)])\n",
    "style_rawim, style = prep_image(style)\n",
    "plt.imshow(style_rawim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines various measures of difference that we'll use to compare the current output image with the original sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_layout(combined):\n",
    "    def no_axes():\n",
    "        plt.gca().xaxis.set_visible(False)    \n",
    "        plt.gca().yaxis.set_visible(False)    \n",
    "        \n",
    "    plt.figure(figsize=(9,6))\n",
    "\n",
    "    plt.subplot2grid( (2,3), (0,0) )\n",
    "    no_axes()\n",
    "    plt.imshow(photo_rawim)\n",
    "\n",
    "    plt.subplot2grid( (2,3), (1,0) )\n",
    "    no_axes()\n",
    "    plt.imshow(style_rawim)\n",
    "\n",
    "    plt.subplot2grid( (2,3), (0,1), colspan=2, rowspan=2 )\n",
    "    no_axes()\n",
    "    plt.imshow(combined, interpolation='nearest')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def gram_matrix(x):\n",
    "#    x = x.flatten(ndim=3)\n",
    "#    g = T.tensordot(x, x, axes=([2], [2]))\n",
    "#    return g\n",
    "\n",
    "#def content_loss(P, X, layer):\n",
    "#    p = P[layer]\n",
    "#    x = X[layer]\n",
    "#    \n",
    "#    loss = 1./2 * ((x - p)**2).sum()\n",
    "#    return loss\n",
    "\n",
    "#def style_loss(A, X, layer):\n",
    "#    a = A[layer]\n",
    "#    x = X[layer]\n",
    "#    \n",
    "#    A = gram_matrix(a)\n",
    "#    G = gram_matrix(x)\n",
    "#    \n",
    "#    N = a.shape[1]\n",
    "#    M = a.shape[2] * a.shape[3]\n",
    "#    \n",
    "#    loss = 1./(4 * N**2 * M**2) * ((G - A)**2).sum()\n",
    "#    return loss\n",
    "\n",
    "#def total_variation_loss_l125(x):\n",
    "#    return (((x[:,:,:-1,:-1] - x[:,:,1:,:-1])**2 + (x[:,:,:-1,:-1] - x[:,:,:-1,1:])**2)**1.25).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute layer activations for photo and artwork \n",
    "This takes ~ 20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# This creates an image 'placeholder'\n",
    "input_image = tf.placeholder(tf.uint8, shape=[None, None, 3], name='input_image')\n",
    "#input_image_var = tf.Variable(tf.zeros([image_size,image_size,3], dtype=tf.uint8), name='input_image_var' )\n",
    "\n",
    "# This will convert uint8(0...255) to float32(0.0...1.0)\n",
    "input_image_float = tf.cast( input_image, tf.float32 ) / 255.0\n",
    "\n",
    "# Define the pre-processing chain within the graph - based on the input 'image' above\n",
    "#processed_image = inception_preprocessing.preprocess_image(input_image, image_size, image_size, is_training=False)\n",
    "processed_image = inception_preprocessing.preprocess_for_eval(input_image_float, 0, 0, central_fraction=None)\n",
    "processed_images = tf.expand_dims(processed_image, 0)\n",
    "\n",
    "# Reverse out some of the transforms, so we can see the area/scaling of the inception input\n",
    "numpyish_image = tf.multiply(processed_image, 0.5)\n",
    "numpyish_image = tf.add(numpyish_image, 0.5)\n",
    "numpyish_image = tf.multiply(numpyish_image, 255.0)\n",
    "\n",
    "# Create the model - which uses the above pre-processing on image\n",
    "#   it also uses the default arg scope to configure the batch norm parameters.\n",
    "print(\"Model builder starting\")\n",
    "\n",
    "# Here is the actual model zoo model being instantiated :\n",
    "with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "    logits, end_points = inception.inception_v1(processed_images, num_classes=1001, is_training=False)\n",
    "#probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "# Create an operation that loads the pre-trained model from the checkpoint\n",
    "init_fn = slim.assign_from_checkpoint_fn(\n",
    "    os.path.join(checkpoints_dir, 'inception_v1.ckpt'),\n",
    "    slim.get_model_variables('InceptionV1')\n",
    ")\n",
    "#init_image = input_image_var.assign( input_image )\n",
    "\n",
    "print(\"Model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dir(slim.get_model_variables('InceptionV1')[10])\n",
    "#[ v.name for v in slim.get_model_variables('InceptionV1') ]\n",
    "sorted(end_points.keys())\n",
    "#dir(end_points['Mixed_4b'])\n",
    "#end_points['Mixed_4b'].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that gives us a pallette of GoogLeNet layers from which we can choose to pay attention to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_names = [\n",
    "    # used for 'content' in photo - a mid-tier convolutional layer \n",
    "    'Mixed_4b', #Theano : 'inception_4b/output', \n",
    "    \n",
    "    # used for 'style' - conv layers throughout model (not same as content one)\n",
    "    'Conv2d_1a_7x7', #Theano : 'conv1/7x7_s2',        \n",
    "    'Conv2d_2c_3x3', #Theano : 'conv2/3x3', \n",
    "    'Mixed_3b', #Theano : 'inception_3b/output',  \n",
    "    'Mixed_4d', #Theano : 'inception_4d/output',\n",
    "]\n",
    "\n",
    "# Different set of layers, for experimentation\n",
    "#layers = [  \n",
    "#    # used for 'content' in photo - a mid-tier convolutional layer \n",
    "#    'pool4/3x3_s2', \n",
    "#    \n",
    "#    # used for 'style' - conv layers throughout model (not same as content one)\n",
    "#    'conv1/7x7_s2', 'conv2/3x3', 'pool3/3x3_s2', 'inception_5b/output',\n",
    "#]\n",
    "\n",
    "#layers = { k: end_points[k] for k in layer_names }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab (constant) values for all the ```layer_names``` for the original photo, and the style image :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's run the pre-trained model on the photo and the style\n",
    "style_features={}\n",
    "photo_features={}\n",
    "#layer_shapes={}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # This is the loader 'op' we defined above\n",
    "    init_fn(sess)  \n",
    "    #init_image(sess)\n",
    "    \n",
    "    #style_layers_np = sess.run([ end_points[k] for k in layer_names ], feed_dict={input_image: style})\n",
    "    #with tf.variable_scope('style', reuse=True):\n",
    "    #    for i,l in enumerate(style_layers_np):\n",
    "    #        style_features[ layer_names[i] ] = tf.get_variable(layer_names[i], l.shape, \n",
    "    #                                                       initializer=tf.constant_initializer(l))\n",
    "    \n",
    "    # This is two ops : one merely loads the image from numpy, \n",
    "    #   the other runs the network to get the layer outputs\n",
    "    \n",
    "    #sess.run( input_image_var.assign(style) ) \n",
    "    style_layers_np = sess.run([ end_points[k] for k in layer_names ], feed_dict={input_image: style})\n",
    "    \n",
    "    for i,l in enumerate(style_layers_np):\n",
    "        style_features[ layer_names[i] ] = l\n",
    "\n",
    "\n",
    "    #sess.run( input_image_var.assign(photo) ) \n",
    "    photo_layers_np = sess.run([ end_points[k] for k in layer_names ], feed_dict={input_image: photo})\n",
    "    \n",
    "    for i,l in enumerate(photo_layers_np):\n",
    "        photo_features[ layer_names[i] ] = l\n",
    "    \n",
    "    \n",
    "    # Print summary of the layers we're capturing\n",
    "    print(\"Number of layers to capture as constants : %d\" % (len(style_layers_np),))\n",
    "    for i,l in enumerate(style_layers_np):\n",
    "        print(\"  Layer[%d].shape=%s, .name = '%s'\" % (i, str(l.shape), layer_names[i],))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are what the layers each see (photo on the top, style on the bottom for each set) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name in layer_names:\n",
    "    print(\"Layer Name : '%s'\" % (name,))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(photo_features[ name ][0, :, :, i], interpolation='nearest') # , cmap='gray'\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 4, 4+i+1)\n",
    "        plt.imshow(style_features[ name ][0, :, :, i], interpolation='nearest') #, cmap='gray'\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the overall loss / badness function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create model losses, which involve the ```end_points``` evaluated from the generated image, coupled with the appropriate constant layer losses from above : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the inital 'art' image to being the photo (quicker convergence)\n",
    "#art_image = np.random.uniform(-128, 128, (1, 3, IMAGE_W, IMAGE_W) ) \n",
    "#art_image = photo\n",
    "\n",
    "art_features = {}\n",
    "for name in layer_names:  \n",
    "    art_features[name] = end_points[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    shape = tensor.get_shape()\n",
    "    \n",
    "    # Get the number of feature channels for the input tensor,\n",
    "    # which is assumed to be from a convolutional layer with 4-dim.\n",
    "    num_channels = int(shape[3])\n",
    "\n",
    "    # Reshape the tensor so it is a 2-dim matrix. This essentially\n",
    "    # flattens the contents of each feature-channel.\n",
    "    matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
    "    \n",
    "    # Calculate the Gram-matrix as the matrix-product of\n",
    "    # the 2-dim matrix with itself. This calculates the\n",
    "    # dot-products of all combinations of the feature-channels.\n",
    "    gram = tf.matmul(tf.transpose(matrix), matrix)\n",
    "    return gram\n",
    "\n",
    "def content_loss(P, X, layer):\n",
    "    p = tf.constant( P[layer] )\n",
    "    x = X[layer]\n",
    "    \n",
    "    loss = 1./2 * tf.reduce_mean(tf.square(x - p))\n",
    "    return loss\n",
    "\n",
    "def style_loss(S, X, layer):\n",
    "    s = tf.constant( S[layer] )\n",
    "    x = X[layer]\n",
    "    \n",
    "    S_gram = gram_matrix(s)\n",
    "    X_gram = gram_matrix(x)\n",
    "    \n",
    "    #layer_shape = layer_shapes[layer]\n",
    "    layer_shape = s.get_shape()\n",
    "    N = layer_shape[1]\n",
    "    M = layer_shape[2] * layer_shape[3]\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(X_gram - S_gram)) / (4. * tf.cast( tf.square(N) * tf.square(M), tf.float32))\n",
    "    return loss\n",
    "\n",
    "#def create_denoise_loss(model):\n",
    "def total_variation_loss_l1(x):\n",
    "    loss = tf.add( \n",
    "            tf.reduce_sum(tf.abs(x[1:,:,:] - x[:-1,:,:])), \n",
    "            tf.reduce_sum(tf.abs(x[:,1:,:] - x[:,:-1,:]))\n",
    "           )\n",
    "    return loss\n",
    "    #return tf.cast(loss, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And here are some more TF nodes, to compute the losses using the layer values 'saved off' earlier\n",
    "losses = []\n",
    "\n",
    "# content loss\n",
    "cl = 10.\n",
    "losses.append( cl *100. * content_loss(photo_features, art_features, 'Mixed_4b'))\n",
    "\n",
    "# style loss\n",
    "sl = 20.\n",
    "losses.append(sl *         style_loss(style_features, art_features, 'Conv2d_1a_7x7'))\n",
    "losses.append(sl *100.   * style_loss(style_features, art_features, 'Conv2d_2c_3x3'))\n",
    "losses.append(sl *10000. * style_loss(style_features, art_features, 'Mixed_3b'))\n",
    "losses.append(sl *10000. * style_loss(style_features, art_features, 'Mixed_4d'))\n",
    "\n",
    "# total variation penalty\n",
    "vp = 0.01 /1000.\n",
    "losses.append(vp *10.    * total_variation_loss_l1(input_image_float))\n",
    "\n",
    "#total_loss = content_loss(photo_features, art_features, 'Mixed_4b')\n",
    "#total_loss = style_loss(style_features, art_features, 'Conv2d_2c_3x3')\n",
    "#total_loss = total_variation_loss_l1(input_image_float)\n",
    "\n",
    "#['0.000372', '0.681764', '0.002374', '0.000064', '0.000004', '0.074681']\n",
    "#['2.715543', '0.701646', '0.223764', '0.492285', '0.031124', '0.749490']\n",
    "\n",
    "total_loss = tf.reduce_sum(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *Famous* Symbolic Gradient operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_grad = tf.gradients(total_loss, [input_image_float])[0] / 255.0  # Needs scaling due to initial /255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Ready for Optimisation by SciPy\n",
    "\n",
    "This uses the BFGS routine : \n",
    "  *  R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound Constrained Optimization, (1995), SIAM Journal on Scientific and Statistical Computing, 16, 5, pp. 1190-1208."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize with the original ```photo```, since going from noise (the code that's commented out) takes many more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "art_image = photo\n",
    "#art_image = np.random.uniform(0, 255, (image_size, image_size, 3))\n",
    "\n",
    "x0 = art_image.flatten().astype('float64')\n",
    "iteration=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize all those losses, and show the image\n",
    "\n",
    "To refine the result, just keep hitting 'run' on this cell (each iteration is about 60 seconds) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_fn(sess)\n",
    "    \n",
    "    # These helper functions (to interface with scipy.optimize) must close over sess\n",
    "    def eval_loss(x):  # x0 is a 3*image_size*image_size float64 vector\n",
    "        #print(\"Eval Loss @ \", x)\n",
    "        x_image = x.reshape(image_size,image_size,3).astype('uint8')\n",
    "        x_loss = sess.run( total_loss, feed_dict={input_image: x_image} )\n",
    "        #print(\"Eval Loss = \", x_loss)\n",
    "        losses_ = sess.run( losses, feed_dict={input_image: x_image} )\n",
    "        print(\"Eval loss components = \", [ \"%.6f\" % l for l in losses_])\n",
    "        return x_loss.astype('float64')\n",
    "\n",
    "    def eval_grad(x):\n",
    "        #print(\"Eval Grad @ \", x)\n",
    "        x_image = x.reshape(image_size,image_size,3).astype('uint8')\n",
    "        x_grad = sess.run( total_grad, feed_dict={input_image: x_image} )\n",
    "        #print(\"Eval Grad.shape = \", x_grad.shape)\n",
    "        #print(\"Eval Grad = \", x_grad.flatten()[100:106])\n",
    "        return x_grad.flatten().astype('float64')\n",
    "\n",
    "    x0, x0_loss, state = scipy.optimize.fmin_l_bfgs_b( eval_loss, x0, fprime=eval_grad, maxfun=40) \n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "print(\"Iteration %d, in %.1fsec, Current loss : %.4f\" % (iteration, float(time.time() - t0), x0_loss))\n",
    "plot_layout(x0.reshape(image_size,image_size,3).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    pass\n",
    "    #print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
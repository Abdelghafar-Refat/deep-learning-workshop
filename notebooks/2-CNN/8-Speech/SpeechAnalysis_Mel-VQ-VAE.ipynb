{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = [ './librivox/guidetomen_%02d_rowland_64kb.mp3' % (i,) for i in [1,2,3]]\n",
    "audio_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "librosa.__version__  # '0.5.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate= 24000 # input will be standardised to this rate\n",
    "\n",
    "fft_step   = 12.5/1000. # 12.5ms\n",
    "fft_window = 50.0/1000.  # 50ms\n",
    "\n",
    "n_fft = 512*4\n",
    "\n",
    "hop_length = int(fft_step*sample_rate)\n",
    "win_length = int(fft_window*sample_rate)\n",
    "\n",
    "n_mels = 80\n",
    "fmin = 125 # Hz\n",
    "#fmax = ~8000\n",
    "\n",
    "#np.exp(-7.0), np.log(spectra_abs_min)  # \"Audio tests\" suggest a min log of -4.605 (-6 confirmed fine)\n",
    "spectra_abs_min = 0.01 # From Google paper, seems justified\n",
    "\n",
    "win_length, hop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for the training windowing :\n",
    "mel_samples  = 1024\n",
    "batch_size   = 8\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install https://github.com/telegraphic/hickle/archive/dev.zip\n",
    "import hickle as hkl\n",
    "\n",
    "def audio_to_melspectrafile(audio_filepath, regenerate=False):\n",
    "    print(\"convert_wavs_to_spectra_learnable_records(%s)\" % (audio_filepath,))\n",
    "    melspectra_filepath = audio_filepath.replace('.mp3', '.melspectra.hkl')\n",
    "    if os.path.isfile(melspectra_filepath) and not regenerate:\n",
    "        print(\"  Already present\")\n",
    "        return melspectra_filepath\n",
    "\n",
    "    samples, _sample_rate = librosa.core.load(audio_filepath, sr=sample_rate)\n",
    "    samples = samples/np.max(samples)  # Force amplitude of waveform into range ~-1 ... +1.0\n",
    "\n",
    "    spectra_complex = librosa.stft(samples, n_fft=n_fft, \n",
    "                       hop_length=hop_length, \n",
    "                       win_length=win_length, window='hann', )\n",
    "\n",
    "    power_spectra = np.abs(spectra_complex)**2\n",
    "    melspectra = librosa.feature.melspectrogram(S=power_spectra, n_mels=n_mels, fmin=fmin)\n",
    "    \n",
    "    mel_log = np.log( np.maximum(spectra_abs_min, np.abs(melspectra) ))\n",
    "\n",
    "    # Shape of batches will be (Batch, MelsChannel, TimeStep) for PyTorch - no need for Transpose\n",
    "    data = dict( \n",
    "        mels = melspectra,\n",
    "        mel_log = mel_log,\n",
    "        spectra_complex = spectra_complex,\n",
    "        #spectra_real = spectra_complex.real, \n",
    "        #spectra_imag = spectra_complex.imag, \n",
    "    )\n",
    "    \n",
    "    hkl.dump(data, melspectra_filepath, mode='w', compression='gzip')\n",
    "    return melspectra_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_filenames = [ audio_to_melspectrafile(f) for f in audio_filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't see a clean way of shuffling without having loaded all the input first...\n",
    "\n",
    "#class DatasetFromMelspectraFile(torch.utils.data.Dataset):\n",
    "#    def __init__(self, melspectra_filepath):\n",
    "#        super(DatasetFromMelspectraFile, self).__init__()\n",
    "#        \n",
    "#        data = hkl.load(melspectra_filepath)\n",
    "#        self.mels = data['mels']\n",
    "#\n",
    "#    def __getitem__(self, index):\n",
    "#        offset = index*mel_samples \n",
    "#        a = self.mels[:, offset:offset+mel_samples]\n",
    "#        return a,a  # This is a VAE situation\n",
    "#\n",
    "#    def __len__(self):  \n",
    "#        return self.mels.shape[1]//mel_samples\n",
    "#    \n",
    "#class DatasetFromFiles(torch.utils.data.Dataset):\n",
    "#    def __init__(self, filepath_arr, length_arr):\n",
    "#        super(DatasetFromFiles, self).__init__()\n",
    "#        self.filepaths = filepath_arr\n",
    "#        self.file_index, self.item_index = -1,-1\n",
    "#        self.d = None\n",
    "#        \n",
    "#    def __getitem__(self, index):\n",
    "#        self.item_index+=1\n",
    "#        if self.d is None or self.item_index >= len(self.d):\n",
    "#            self.file_index+=1\n",
    "#            self.d = DatasetFromMelspectraFile(self.filepaths[self.file_index])\n",
    "#            self.item_index=0\n",
    "#        return d[self.item_index]\n",
    "#\n",
    "#    def __len__(self):  \n",
    "#        #return len(self.filepaths)\n",
    "#        return -1 # DUNNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach allows us to load the files into memory only as needed - \n",
    "#   But may not be necessary for our purposes, since the data is actually pretty small\n",
    "\n",
    "def yield_batches_from(melspectra_filepath, bs=batch_size, shuffle=False):\n",
    "    data = hkl.load(melspectra_filepath)\n",
    "    mels = data['mels']\n",
    "    offsets = np.arange(0, mels.shape[1]-mel_samples, mel_samples)\n",
    "    print(\"Batches from file : \", melspectra_filepath, mels.shape, offsets.shape)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(offsets)  # in-place\n",
    "    batch_x = np.zeros( shape=(bs, n_mels, mel_samples) )  # Allocate once\n",
    "    for batch_idx in range(0, offsets.shape[0], bs):\n",
    "        for i in range(0, bs):\n",
    "             batch_x[i, :, :] = mels[:, offsets[i]:offsets[i]+mel_samples]\n",
    "        yield batch_x, batch_x # input -> target\n",
    "    # Stop\n",
    "\n",
    "def yield_batches_from_files(filepaths, bs=batch_size, shuffle=False, shuffle_within=False):\n",
    "    if shuffle:\n",
    "        #random.shuffle(filepaths)  # in-place = meh\n",
    "        filepaths = random.sample( filepaths, len(filepaths) )  # original unchanged(~)\n",
    "    for filepath in filepaths:\n",
    "        file_batcher = yield_batches_from(filepath, bs=bs, shuffle=shuffle_within)\n",
    "        for batch in file_batcher:\n",
    "            yield batch\n",
    "    # Stop\n",
    "\n",
    "# This is how this code looks when used :\n",
    "#for epoch in range(epochs):\n",
    "#    t0 = datetime.datetime.now()\n",
    "#    train_batcher = yield_batches_from_files(mel_filenames, bs=batch_size, shuffle=True, shuffle_within=True)\n",
    "#    for batch_idx, batch in enumerate(train_batcher):\n",
    "#        input, target = batch\n",
    "#        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data # required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # Test ops to get correct Tensor format\n",
    "    t = torch.from_numpy(np.array([[10,11,12,13,14,15,16,17,18,19], \n",
    "                                   [20,21,22,23,24,25,26,27,28,29], \n",
    "                                   [30,31,32,33,34,35,36,37,38,39]\n",
    "                                  ]))\n",
    "    t\n",
    "    #t.view(2,3,5)\n",
    "    t.transpose(0,1).contiguous().view(2,5,3).transpose(1,2)\n",
    "\n",
    "    # Want to convert long set of mels into batches of length mel_samples: \n",
    "    # 0 :\n",
    "    #   10    11    12    13    14    \n",
    "    #   20    21    22    23    24   \n",
    "    #   30    31    32    33    34   \n",
    "    # 1 :\n",
    "    #   15    16    17    18    19\n",
    "    #   25    26    27    28    29\n",
    "    #   35    36    37    38    39\n",
    "\n",
    "def TensorFromMelspectraFile(melspectra_filepath, block_len=mel_samples):\n",
    "    data = hkl.load(melspectra_filepath)\n",
    "    mel_log = data['mel_log']\n",
    "    \n",
    "    if block_len is None: # Allow for 'whole of file' tensor(1,mels,everything)\n",
    "        block_len=mel_log.shape[1]\n",
    "    n_blocks = mel_log.shape[1]//block_len\n",
    "    print(\"Read %5d log(mel[%2d]) = %4d blocks from %s\" % \n",
    "          (mel_log.shape[1], mel_log.shape[0], n_blocks, melspectra_filepath,))\n",
    "    \n",
    "    mel_log_trunc_t = mel_log[:, :n_blocks*block_len ].T\n",
    "    #print(torch.from_numpy(mel_log_trunc_t).contiguous().size())\n",
    "    return ( torch.from_numpy(mel_log_trunc_t).contiguous()\n",
    "             .view(n_blocks, block_len, n_mels).transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_datasets = []\n",
    "for f in mel_filenames:\n",
    "    t = TensorFromMelspectraFile(f)\n",
    "    mel_datasets.append( torch.utils.data.TensorDataset(t, t) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_dataset = torch.utils.data.ConcatDataset(mel_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "ftype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "ltype = torch.cuda.LongTensor  if use_cuda else torch.LongTensor\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNettyCell(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, cond_channels=0, \n",
    "                 kernel_size=2, stride=1, dilation=1):\n",
    "        super(WaveNettyCell, self).__init__()\n",
    "        \n",
    "        self.gate   = torch.nn.Conv1d(in_channels, hidden_channels, \n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, dilation=dilation, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "        self.signal = torch.nn.Conv1d(in_channels, hidden_channels, \n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, dilation=dilation, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "        \n",
    "        self.cond = cond_channels>0\n",
    "        if self.cond:\n",
    "            self.gate_cond   = torch.nn.Conv1d(cond_channels, hidden_channels, kernel_size=1, bias=False)\n",
    "            self.signal_cond = torch.nn.Conv1d(cond_channels, hidden_channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.pad_end = (kernel_size-1)*(dilation+stride*0)\n",
    "\n",
    "        self.recombine = torch.nn.Conv1d(hidden_channels, in_channels, \n",
    "                                    kernel_size=1, stride=1, dilation=1, \n",
    "                                    padding=0, # Only accepts symmetrical values - pad separately\n",
    "                                    groups=1, bias=True)\n",
    "            \n",
    "    def forward(self, input, condition=None):\n",
    "        gate = self.gate(input)\n",
    "        signal = self.signal(input)\n",
    "        if self.cond:\n",
    "            gate   = gate   + self.gate_cond(condition)\n",
    "            signal = signal + self.signal_cond(condition)\n",
    "\n",
    "        gate = F.sigmoid(gate)\n",
    "            \n",
    "        mult = gate * F.tanh(signal)\n",
    "        \n",
    "        # Yes : There's no side/skip here : It's just a fancy feed-forward\n",
    "        #return input + F.pad( self.recombine(mult), (0, self.pad_end) )\n",
    "        return input*0.8 + F.pad( self.recombine(mult), (0, self.pad_end) )\n",
    "        #return F.pad( self.recombine(mult), (0, self.pad_end) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=128):\n",
    "        super(VQ_encoder, self).__init__()\n",
    "        \n",
    "        # See https://fomoro.com/tools/receptive-fields/\n",
    "        \n",
    "        #   #3,2,1,VALID;3,2,1,VALID;3,2,1,VALID;3,2,1,VALID;3,2,1,VALID\n",
    "        #self.conv = [ WaveNettyCell(in_channels, hidden_channels, \n",
    "        #                            stride=2) for c in range(4) ]\n",
    "            \n",
    "        #   #3,1,1,VALID;3,1,2,VALID;3,1,4,VALID;3,1,8,VALID;3,1,16,VALID\n",
    "        #   receptive field = 63 timesteps\n",
    "        self.conv = torch.nn.ModuleList([ WaveNettyCell(in_channels, hidden_channels, \n",
    "                                    dilation=d) for d in [1,2,4,8,16] ])\n",
    "        \n",
    "        self.pad_end = sum([c.pad_end for c in self.conv])\n",
    "            \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for c in self.conv:\n",
    "            x = c(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_quantiser(torch.nn.Module):\n",
    "    def __init__(self, n_symbols, latent_dim):\n",
    "        super(VQ_quantiser, self).__init__()\n",
    "        \n",
    "        # See : https://github.com/nakosung/VQ-VAE/blob/master/model.py#L16\n",
    "        self.n_symbols, self.latent_dim = n_symbols, latent_dim\n",
    "        self.embedding = torch.nn.Embedding(n_symbols, latent_dim)  # k_dim=n_symbols, z_dim=latent_dim\n",
    "        # , max_norm=1.0\n",
    "        \n",
    "        #self.init_weights_random()        \n",
    "        self.init_weights_done = False\n",
    "        self.symbol_hist_init()\n",
    "        \n",
    "    def init_weights_random(self):\n",
    "        initrange = 1. / self.n_symbols\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)             \n",
    "        \n",
    "    def init_weights_informed(self, Z):\n",
    "        #self.embedding.weight.data.uniform_(-initrange, initrange)             \n",
    "        print(\"init_weights_informed : Z.size() \", Z.size())\n",
    "        order = torch.randperm(Z.size(0)).type(ltype)\n",
    "        Z_ordered = Z[order]\n",
    "        self.embedding.weight.data = Z_ordered.data[0:self.n_symbols, :]\n",
    "        self.init_weights_done = True\n",
    "        \n",
    "    def symbol_hist_init(self):\n",
    "        self.symbol_hist = torch.zeros( (self.n_symbols,) ).type(ltype)\n",
    "        \n",
    "    def symbol_hist_update(self, nearest_idx):\n",
    "        nearest_idx_values = nearest_idx.data.cpu().type(torch.FloatTensor)\n",
    "        hist_now = torch.histc(nearest_idx_values, bins=self.n_symbols, \n",
    "                                        min=0, max=self.n_symbols-1).type(ltype)\n",
    "        self.symbol_hist += hist_now\n",
    "            \n",
    "    def forward(self, input):\n",
    "        #return input, [], 0.,  0.  # Doesn't do quantisation yet...\n",
    "        sz = input.size()\n",
    "        \n",
    "        # BCT -> BTC -> B(TC in one long strip)\n",
    "        Z = input.permute(0,2,1).contiguous().view(-1, self.latent_dim)\n",
    "        if not self.init_weights_done:\n",
    "            self.init_weights_informed(Z)\n",
    "\n",
    "        W = self.embedding.weight\n",
    "\n",
    "        def L2_dist(a,b):\n",
    "            return ((a - b) ** 2)\n",
    "        \n",
    "        # Find nearest embedding for every vector in the long strip\n",
    "        #   Form matrix of all L2 (sum) distances, finds most minimum's index\n",
    "        nearest_idx = L2_dist( Z[:,None], W[None,:] ).sum(2).min(1)[1]\n",
    "        W_nearest_latent = W[nearest_idx]  # Convert indices into latent vectors\n",
    "        \n",
    "        self.symbol_hist_update(nearest_idx)\n",
    "\n",
    "        # B(TC) -> BCT i.e. re-roll back into \n",
    "        out = W_nearest_latent.view(sz[0],sz[2],sz[1]).permute(0,2,1)\n",
    "\n",
    "        def hook(grad):\n",
    "            # This is being called for Embedding updates.  \n",
    "            # Store the grad to pass along as an input update too\n",
    "            # This isn't 'perfect' according to the paper, but should work well enough\n",
    "            self.saved_input_to_vq = input\n",
    "            self.saved_grad_for_input_to_vq = grad\n",
    "            return grad\n",
    "\n",
    "        out.register_hook(hook)\n",
    "        \n",
    "        # Stop gradients (_sg) for additional loss terms\n",
    "        Z_sg = Z.detach()\n",
    "        W_nearest_latent_sg = W_nearest_latent.detach()\n",
    "\n",
    "        return (out,\n",
    "                nearest_idx.view(sz[0],sz[2]),\n",
    "                # return additional loss values too to optimise embedding and input respectively\n",
    "                L2_dist(Z_sg, W_nearest_latent).sum(1).mean(),\n",
    "                L2_dist(Z, W_nearest_latent_sg).sum(1).mean(), \n",
    "               )\n",
    "\n",
    "    # back propagation for inputs to VQ (rather than just to the embeddings)\n",
    "    def backward_input_itself(self):\n",
    "        self.saved_input_to_vq.backward(self.saved_grad_for_input_to_vq)\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(np.array([[10,11,12,13,14,15,16,17,18,19], \n",
    "                               [20,21,22,23,24,25,26,27,28,29], \n",
    "                               [30,31,32,33,34,35,36,37,38,39]\n",
    "                              ], dtype=np.float32))\n",
    "#t.view(2,3,5)\n",
    "t_batch = t.transpose(0,1).contiguous().view(2,5,3).transpose(1,2)\n",
    "t_batch\n",
    "\n",
    "e = torch.from_numpy(np.array([\n",
    "    [10,20,30], \n",
    "    [12,22,32], \n",
    "    [15,25,35], \n",
    "    [15,23,35], \n",
    "    [17,24,32], \n",
    "    [14,27,35], \n",
    "    [27,24,12], \n",
    "  ], dtype=np.float32))\n",
    "e\n",
    "def L2_dist_local(a,b):\n",
    "    return ((a - b) ** 2)\n",
    "Z = t_batch.permute(0,2,1).contiguous().view(-1, 3) # Laid out as one big batch\n",
    "Z\n",
    "W = e\n",
    "# Sample nearest embedding \n",
    "#   Form matrix of all L2 (sum) distances, finds most minimum's index\n",
    "#nearest_idx = L2_dist_local( Z[:,None], W[None,:] )\n",
    "#nearest_idx = L2_dist_local( Z[:,None], W[None,:] ).sum(2)\n",
    "nearest_idx = L2_dist_local( Z[:,None], W[None,:] ).sum(2).min(1)\n",
    "#nearest_idx\n",
    "#nearest_idx = L2_dist_local( Z[:,None], W[None,:] ).sum(2).min(1)[1]\n",
    "W_nearest_latent = W[nearest_idx[1]]  # Convert indices into latent vectors\n",
    "W_nearest_latent\n",
    "\n",
    "#t = torch.from_numpy(np.array([[10,11,12], \n",
    "#                           [20,21,22], \n",
    "#                          ]))\n",
    "#t.size(0)\n",
    "#t[None, :] # Adds an extra column at beginning\n",
    "#t[:, None] # Inserts an extra column in middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_decoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, latent_channels=0, hidden_channels=128):\n",
    "        super(VQ_decoder, self).__init__()\n",
    "        \n",
    "        self.conv = torch.nn.ModuleList([ WaveNettyCell(in_channels, hidden_channels, \n",
    "                                    #cond_channels=latent_channels,\n",
    "                                    dilation=d) for d in [1,2,4,8,16] ])\n",
    "        \n",
    "        self.pad_end = sum([c.pad_end for c in self.conv])\n",
    "            \n",
    "    def forward(self, input, latent=None):\n",
    "        x = input\n",
    "        for c in self.conv:\n",
    "            #x = c(x, latent)\n",
    "            x = c(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_VAE_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VQ_VAE_Model, self).__init__()\n",
    "        \n",
    "        self.channels, self.n_symbols = n_mels, 16\n",
    "        #self.channels, self.n_symbols = n_mels, 64\n",
    "        \n",
    "        self.encoder = VQ_encoder(self.channels)\n",
    "        self.quant   = VQ_quantiser(self.n_symbols, self.channels)\n",
    "        self.decoder = VQ_decoder(self.channels)\n",
    "        \n",
    "        print(f\"Number of parameter variables : {len(list(self.parameters()))}\")\n",
    "        \n",
    "        self.pad_end = self.encoder.pad_end + self.decoder.pad_end \n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        #self.optimizer = torch.optim.RMSprop(self.parameters())  # Converges hardly at all\n",
    "        #self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        \n",
    "        # Hmm : Adaptive learning rate ideas:\n",
    "        #   https://github.com/fastai/fastai/blob/master/fastai/learner.py#L216\n",
    "        # And : http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "        \n",
    "    def update_lr(self, lr):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    def forward(self, input):\n",
    "        #vq_input = input\n",
    "        x = self.encoder(input)\n",
    "        x, symbols, loss_e1, loss_e2 = self.quant(x)\n",
    "        x = self.decoder(x)\n",
    "        #x = self.decoder(x)\n",
    "        return x, symbols, loss_e1, loss_e2\n",
    "\n",
    "    def train_(self, input, target, take_step=True):\n",
    "        valid_len = input.size(2) - self.pad_end\n",
    "        \n",
    "        self.train()  # Set mode\n",
    "        \n",
    "        output, symbols, loss_e1, loss_e2 = self(input)\n",
    "        \n",
    "        #if float(loss_e2)>10.:\n",
    "        #print(\"Symbols : %s\" % (' '.join([ ('%2d' % int(v)) for v in symbols[0,0:25]]),))\n",
    "        #print(\"Symbol.hist : %s\" % (' '.join([ ('%2d' % int(v)) for v in self.quant.symbol_hist[0:20]]),))\n",
    "        \n",
    "        loss_rec = F.mse_loss(output[:,:,:valid_len], target[:,:,:valid_len])\n",
    "        #loss_rec = F.smooth_l1_loss(output[:,:,:valid_len], target[:,:,:valid_len])\n",
    "        \n",
    "        loss = 10.*loss_rec + loss_e1 + 0.25*loss_e2  # MAGIC NUMBERS\n",
    "        \n",
    "        if take_step:\n",
    "            self.gradient_step(loss)\n",
    "        \n",
    "        return ( loss, loss_rec, loss_e1, loss_e2 )\n",
    "    \n",
    "    def gradient_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.quant.backward_input_itself()\n",
    "        torch.nn.utils.clip_grad_norm(self.parameters(), 0.5)  # MAGIC NUMBER\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict_(self, input):\n",
    "        self.eval()\n",
    "        output, symbols, loss_e1, loss_e2 = self(input)\n",
    "        return output, symbols\n",
    "\n",
    "    def get_state(self):          # Returns a tuple of the states\n",
    "        return self.state_dict(), self.optimizer.state_dict()\n",
    "    def set_state(self, states):  # ... resumable here...\n",
    "        self.load_state_dict(states[0])\n",
    "        self.optimizer.load_state_dict(states[1])\n",
    "    \n",
    "    def save(self, filename='model/tmp.pkl', with_optimiser=False):\n",
    "        #torch.save(self.state_dict(), 'model/epoch_{}_{:02d}.pth'.format(self.name, epoch))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        if with_optimiser:\n",
    "            torch.save(self.optimizer.state_dict(), filename.replace('.pkl', '.optim.pkl'))\n",
    "\n",
    "    def load(self, filename='model/tmp.pkl', with_optimiser=False):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.quant.init_weights_done = True\n",
    "        if with_optimiser:\n",
    "            self.optimizer.load_state_dict(torch.load(filename.replace('.pkl', '.optim.pkl')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQ_VAE_Model()\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "def train_epoch(epoch, learning_rate, take_step=True, shuffle=True):\n",
    "    t0 = datetime.datetime.now()\n",
    "    train_batches = torch.utils.data.DataLoader(mel_dataset, batch_size=batch_size, \n",
    "                                                shuffle=shuffle, num_workers=1)\n",
    "    model.quant.symbol_hist_init()\n",
    "    model.update_lr(learning_rate)\n",
    "    losses = np.zeros( shape=(4,) )\n",
    "    for batch_idx, batch in enumerate(train_batches):\n",
    "        input, target = batch\n",
    "        \n",
    "        x = Variable( input.type(ftype) )\n",
    "        y = Variable( target.type(ftype) )\n",
    "        losses_arr = model.train_(x, y, take_step=take_step)\n",
    "        losses += np.array( [float(v) for v in losses_arr ])\n",
    "        \n",
    "        #print(f\"Epoch {epoch:2}, Batch {batch_idx:2}, %.6f\" % (float(mse*1000*1000),))\n",
    "    \n",
    "    print(\"  Symbol.hist : %s\" % (' '.join([ ('%5d' % int(v)) for v in model.quant.symbol_hist[0:16]]),))\n",
    "    #print(f\"Epoch {epoch:4}, %s\" % (', '.join([ (\"%8.2f\" % v) for v in (losses/batch_idx).tolist() ]),))\n",
    "    print(f\"Epoch {epoch:4}, %2d batches, %s\" % (\n",
    "        batch_idx, ', '.join([ (\"%8.2f\" % v) for v in (losses).tolist() ]),))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loss_rate(epoch, lr_current, lr_factor=1.0):  #, lr_step=1.2\n",
    "    saved_state = model.get_state()    # Save before any non-standard learning rate applied\n",
    "    \n",
    "    loss_best = None\n",
    "    loss_performance_best = None\n",
    "    \n",
    "    for lr_s in [0.0, -.1, +.1, -.15, +.2, -.2, ]:\n",
    "        lr = (1.+(lr_s*lr_factor))*lr_current\n",
    "        \n",
    "        print()\n",
    "        print(\"  Trying learning rate : %.8f\" % (lr,))\n",
    "\n",
    "        # Repeatability will increase when fix from \n",
    "        #   https://github.com/SeanNaren/deepspeech.pytorch/issues/210\n",
    "        #   is installed\n",
    "        model.set_state(saved_state)\n",
    "        \n",
    "        loss_epoch = train_epoch(epoch, lr, take_step=True, shuffle=False)\n",
    "        loss_base = loss_epoch[0]  \n",
    "\n",
    "        loss_epoch = train_epoch(epoch, lr, take_step=False, shuffle=False)\n",
    "        loss_this = loss_epoch[0]  \n",
    "\n",
    "        \n",
    "        if (loss_best is None) or loss_best>loss_this:\n",
    "            loss_best=loss_this\n",
    "            lr_loss_best = lr\n",
    "\n",
    "        # loss_performance is the actual performance experienced under lr\n",
    "        #   we want the best performing & highest lr available\n",
    "        loss_performance = (loss_base - loss_this)\n",
    "        print(\"  loss_performance : %.8f\" % (loss_performance,))\n",
    "        \n",
    "        if loss_performance_best is None:\n",
    "            loss_performance_best=loss_performance\n",
    "            lr_performance_best = lr\n",
    "            continue\n",
    "            \n",
    "        if loss_performance_best<loss_performance:\n",
    "            loss_performance_best=loss_performance\n",
    "            lr_best = lr\n",
    "            \n",
    "        #loss_epoch = train_epoch(epoch, lr) # This is after one lr step\n",
    "        #loss_this = loss_epoch[0]\n",
    "        # \n",
    "        #if loss_this<loss_base:\n",
    "        #    model.save() \n",
    "        #else: \n",
    "        #    model.load()  # Load the model with the previous parameters\n",
    "        #    lr /= lr_factor  # Back off one step\n",
    "        #    break\n",
    "            \n",
    "        #lr *= lr_factor\n",
    "        #loss_prev = loss_this\n",
    "    \n",
    "    lr = lr_loss_best\n",
    "    print(\"learning rate set to : %.8f\" % (lr,))\n",
    "    model.set_state(saved_state)    # Revert to after model 1 lr_initial step\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "for epoch in range(epochs*100):\n",
    "    print()\n",
    "    loss_epoch = train_epoch(epoch, lr)\n",
    "    if epoch % 50 == 20:  # Allow for warm-up\n",
    "        lr = find_loss_rate(0, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'{234.3453453453434534:6.2}'  Wierd choice for format specifiers : overall_width.digits_of_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(np.array([[10,11,12,13,14,15,16,17,18,19], \n",
    "                               [20,21,22,23,24,25,26,27,28,29], \n",
    "                               [30,31,32,33,34,35,36,37,38,39]\n",
    "                              ]))\n",
    "t.size(0)\n",
    "np.array([3,4,5]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#model.save('model/16symbols-no-enc-dec_epoch_{:04d}.pth', epoch)\n",
    "#model.save('model/16symbols_epoch_{:04d}_553.pth'.format(epoch))\n",
    "#model.save('model/64symbols_epoch_{:04d}_344.pth'.format(epoch))\n",
    "#model.save('model/16symbols_k2_epoch_{:04d}_353.pth'.format(epoch))\n",
    "model.save('model/16symbols_k2_epoch_{:04d}_553.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load('model/16symbols_epoch_0999_553.pth')\n",
    "#model.load('model/64symbols_epoch_0999_344.pth')\n",
    "model.load('model/16symbols_k2_epoch_0999_553.pth')\n",
    "#model.load('model/64symbols_k2_epoch_0999_353.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get in a full batch to view\n",
    "mel_filename_test = mel_filenames[1]\n",
    "model.quant.symbol_hist_init()\n",
    "test_input = TensorFromMelspectraFile(mel_filename_test, block_len=None) # Whole file\n",
    "test_input\n",
    "test_output, test_symbols = model.predict_( Variable( test_input.type(ftype) ) )\n",
    "#test_symbols\n",
    "model.quant.symbol_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symb_to_char = \"abcdef hikjlmnop\" # k=3\n",
    "symb_to_char = \" bcdefghikjlmnop\" # k=2\n",
    "#symb_to_char = \"abcdefghikjlmnopqrstuvwxyzABCDEFGHIJKLMNOP RSTUVWXYZ0123456789-+*@\" # k=3\n",
    "#symb_to_char = \"abcdefghikjlmnopqrstuvwxyzABCDEFGHI KLMNOPQRSTUVWXYZ0123456789-+*@\" # k=2\n",
    "\n",
    "chars = ''.join( [symb_to_char[v] for v in test_symbols.data.cpu().numpy()[0][:-model.pad_end]] )\n",
    "len(chars),chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mel_filename_test\n",
    "with open(mel_filename_test.replace('.hkl', '.16_k2.sym'), 'wt') as f:\n",
    "    f.write(chars)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert symbols to mels - and have a listen (tricky... since that's a whole project in itself)\n",
    "#test_output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
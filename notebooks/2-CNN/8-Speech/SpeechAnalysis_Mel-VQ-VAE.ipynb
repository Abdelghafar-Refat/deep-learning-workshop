{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.functional as F\n",
    "#import torch.utils.data # required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = [ './librivox/guidetomen_%02d_rowland_64kb.mp3' % (i,) for i in [1,2,3]]\n",
    "audio_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "librosa.__version__  # '0.5.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate= 24000 # input will be standardised to this rate\n",
    "\n",
    "fft_step   = 12.5/1000. # 12.5ms\n",
    "fft_window = 50.0/1000.  # 50ms\n",
    "\n",
    "n_fft = 512*4\n",
    "\n",
    "hop_length = int(fft_step*sample_rate)\n",
    "win_length = int(fft_window*sample_rate)\n",
    "\n",
    "n_mels = 80\n",
    "fmin = 125 # Hz\n",
    "#fmax = ~8000\n",
    "\n",
    "#np.exp(-7.0), np.log(spectra_abs_min)  # \"Audio tests\" suggest a min log of -4.605 (-6 confirmed fine)\n",
    "spectra_abs_min = 0.01 # From Google paper, seems justified\n",
    "\n",
    "win_length, hop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for the training windowing :\n",
    "mel_samples  = 1024\n",
    "batch_size   = 8\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install https://github.com/telegraphic/hickle/archive/dev.zip\n",
    "import hickle as hkl\n",
    "\n",
    "def audio_to_melspectrafile(audio_filepath, regenerate=False):\n",
    "    print(\"convert_wavs_to_spectra_learnable_records(%s)\" % (audio_filepath,))\n",
    "    melspectra_filepath = audio_filepath.replace('.mp3', '.melspectra.hkl')\n",
    "    if os.path.isfile(melspectra_filepath) and not regenerate:\n",
    "        print(\"  Already present\")\n",
    "        return melspectra_filepath\n",
    "\n",
    "    samples, _sample_rate = librosa.core.load(audio_filepath, sr=sample_rate)\n",
    "    samples = samples/np.max(samples)  # Force amplitude of waveform into range ~-1 ... +1.0\n",
    "\n",
    "    spectra_complex = librosa.stft(samples, n_fft=n_fft, \n",
    "                       hop_length=hop_length, \n",
    "                       win_length=win_length, window='hann', )\n",
    "\n",
    "    power_spectra = np.abs(spectra_complex)**2\n",
    "    melspectra = librosa.feature.melspectrogram(S=power_spectra, n_mels=n_mels, fmin=fmin)\n",
    "\n",
    "    # Shape of batches will be (Batch, MelsChannel, TimeStep) for PyTorch - no need for Transpose\n",
    "    data = dict( \n",
    "        mels = melspectra,\n",
    "        spectra_complex = spectra_complex,\n",
    "        #spectra_real = spectra_complex.real, \n",
    "        #spectra_imag = spectra_complex.imag, \n",
    "    )\n",
    "    \n",
    "    hkl.dump(data, melspectra_filepath, mode='w', compression='gzip')\n",
    "    return melspectra_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_filenames = [ audio_to_melspectrafile(f) for f in audio_filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't see a clean way of shuffling without having loaded all the input first...\n",
    "\n",
    "#class DatasetFromMelspectraFile(torch.utils.data.Dataset):\n",
    "#    def __init__(self, melspectra_filepath):\n",
    "#        super(DatasetFromMelspectraFile, self).__init__()\n",
    "#        \n",
    "#        data = hkl.load(melspectra_filepath)\n",
    "#        self.mels = data['mels']\n",
    "#\n",
    "#    def __getitem__(self, index):\n",
    "#        offset = index*mel_samples \n",
    "#        a = self.mels[:, offset:offset+mel_samples]\n",
    "#        return a,a  # This is a VAE situation\n",
    "#\n",
    "#    def __len__(self):  \n",
    "#        return self.mels.shape[1]//mel_samples\n",
    "#    \n",
    "#class DatasetFromFiles(torch.utils.data.Dataset):\n",
    "#    def __init__(self, filepath_arr, length_arr):\n",
    "#        super(DatasetFromFiles, self).__init__()\n",
    "#        self.filepaths = filepath_arr\n",
    "#        self.file_index, self.item_index = -1,-1\n",
    "#        self.d = None\n",
    "#        \n",
    "#    def __getitem__(self, index):\n",
    "#        self.item_index+=1\n",
    "#        if self.d is None or self.item_index >= len(self.d):\n",
    "#            self.file_index+=1\n",
    "#            self.d = DatasetFromMelspectraFile(self.filepaths[self.file_index])\n",
    "#            self.item_index=0\n",
    "#        return d[self.item_index]\n",
    "#\n",
    "#    def __len__(self):  \n",
    "#        #return len(self.filepaths)\n",
    "#        return -1 # DUNNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_batches_from(melspectra_filepath, bs=batch_size, shuffle=False):\n",
    "    data = hkl.load(melspectra_filepath)\n",
    "    mels = data['mels']\n",
    "    offsets = np.arange(0, mels.shape[1], mel_samples)\n",
    "    print(\"Batches from file : \", melspectra_filepath, mels.shape, offsets.shape)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(offsets)  # in-place\n",
    "    for i in range(0, offsets.shape[0], bs):\n",
    "        yield mels[:, offsets[i : i+bs] ]\n",
    "    # Stop\n",
    "\n",
    "def yield_batches_from_files(filepaths, bs=batch_size, shuffle=False, shuffle_within=False):\n",
    "    if shuffle:\n",
    "        #random.shuffle(filepaths)  # in-place = meh\n",
    "        filepaths = random.sample( filepaths, len(filepaths) )  # original unchanged(~)\n",
    "    for filepath in filepaths:\n",
    "        file_batcher = yield_batches_from(filepath, bs=bs, shuffle=shuffle_within)\n",
    "        for batch in file_batcher:\n",
    "            yield batch\n",
    "    # Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.shuffle(filenames)\n",
    "#filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNettyCell(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, cond_channels=0, \n",
    "                 kernel_size=3, stride=1, dilation=1):\n",
    "        super(WaveNetCell, self).__init__()\n",
    "        \n",
    "        self.gate   = torch.nn.Conv1d(in_channels, hidden_channels, \n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, dilation=dilation, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "        self.signal = torch.nn.Conv1d(in_channels, hidden_channels, \n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, dilation=dilation, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "        \n",
    "        self.cond = cond_channels>0\n",
    "        if self.cond:\n",
    "            self.gate_cond   = torch.nn.Conv1d(cond_channels, hidden_channels, kernel_size=1, bias=False)\n",
    "            self.signal_cond = torch.nn.Conv1d(cond_channels, hidden_channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.recombine = torch.nn.Conv1d(hidden_channels, in_channels, \n",
    "                                    kernel_size=1, stride=1, dilation=1, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "            \n",
    "    def forward(self, input, condition=None):\n",
    "        gate = self.gate(input)\n",
    "        signal = self.signal(input)\n",
    "        if self.cond:\n",
    "            gate   = gate   + self.gate_cond(condition)\n",
    "            signal = signal + self.signal_cond(condition)\n",
    "\n",
    "        gate = F.sigmoid(gate)\n",
    "            \n",
    "        mult = gate * F.tanh(signal)\n",
    "        \n",
    "        # Yes : There's no side/skip here : It's just a fancy feed-forward\n",
    "        return input + self.recombine(mult)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=128):\n",
    "        super(VQ_encoder, self).__init__()\n",
    "        \n",
    "        # See https://fomoro.com/tools/receptive-fields/\n",
    "        \n",
    "        #   #3,2,1,VALID;3,2,1,VALID;3,2,1,VALID;3,2,1,VALID;3,2,1,VALID\n",
    "        #self.conv = [ WaveNettyCell(in_channels, hidden_channels, \n",
    "        #                            stride=2) for c in range(4) ]\n",
    "            \n",
    "        #   #3,1,1,VALID;3,1,2,VALID;3,1,4,VALID;3,1,8,VALID;3,1,16,VALID\n",
    "        #   receptive field = 63 timesteps\n",
    "        self.conv = [ WaveNettyCell(in_channels, hidden_channels, \n",
    "                                    dilation=d) for d in [1,2,4,8,16] ]\n",
    "            \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for c in self.conv:\n",
    "            x = c(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_quantiser(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, cond_channels=0, \n",
    "                 kernel_size=3, stride=1, dilation=1):\n",
    "        super(VQ_quantiser, self).__init__()\n",
    "            \n",
    "    def forward(self, input):\n",
    "        return input  # Doesn't do quantisation yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_decoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, latent_channels, hidden_channels=128):\n",
    "        super(VQ_decoder, self).__init__()\n",
    "        \n",
    "        self.conv = [ WaveNettyCell(in_channels, hidden_channels, \n",
    "                                    #cond_channels=latent_channels,\n",
    "                                    dilation=d) for d in [1,2,4,8,16] ]\n",
    "            \n",
    "    def forward(self, input, latent):\n",
    "        x = input\n",
    "        for c in self.conv:\n",
    "            #x = c(x, latent)\n",
    "            x = c(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batcher = yield_batches_from_files(mel_filenames, bs=batch_size, shuffle=True, shuffle_within=True)\n",
    "\n",
    "#for epoch in range(epochs):\n",
    "#    for batch in train_batcher:\n",
    "#        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
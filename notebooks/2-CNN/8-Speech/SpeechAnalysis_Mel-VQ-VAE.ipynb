{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = [ './librivox/guidetomen_%02d_rowland_64kb.mp3' % (i,) for i in [1,2,3]]\n",
    "audio_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "librosa.__version__  # '0.5.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate= 24000 # input will be standardised to this rate\n",
    "\n",
    "fft_step   = 12.5/1000. # 12.5ms\n",
    "fft_window = 50.0/1000.  # 50ms\n",
    "\n",
    "n_fft = 512*4\n",
    "\n",
    "hop_length = int(fft_step*sample_rate)\n",
    "win_length = int(fft_window*sample_rate)\n",
    "\n",
    "n_mels = 80\n",
    "fmin = 125 # Hz\n",
    "#fmax = ~8000\n",
    "\n",
    "#np.exp(-7.0), np.log(spectra_abs_min)  # \"Audio tests\" suggest a min log of -4.605 (-6 confirmed fine)\n",
    "spectra_abs_min = 0.01 # From Google paper, seems justified\n",
    "\n",
    "win_length, hop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for the training windowing :\n",
    "mel_samples  = 1024\n",
    "batch_size   = 8\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install https://github.com/telegraphic/hickle/archive/dev.zip\n",
    "import hickle as hkl\n",
    "\n",
    "def audio_to_melspectrafile(audio_filepath, regenerate=False):\n",
    "    print(\"convert_wavs_to_spectra_learnable_records(%s)\" % (audio_filepath,))\n",
    "    melspectra_filepath = audio_filepath.replace('.mp3', '.melspectra.hkl')\n",
    "    if os.path.isfile(melspectra_filepath) and not regenerate:\n",
    "        print(\"  Already present\")\n",
    "        return melspectra_filepath\n",
    "\n",
    "    samples, _sample_rate = librosa.core.load(audio_filepath, sr=sample_rate)\n",
    "    samples = samples/np.max(samples)  # Force amplitude of waveform into range ~-1 ... +1.0\n",
    "\n",
    "    spectra_complex = librosa.stft(samples, n_fft=n_fft, \n",
    "                       hop_length=hop_length, \n",
    "                       win_length=win_length, window='hann', )\n",
    "\n",
    "    power_spectra = np.abs(spectra_complex)**2\n",
    "    melspectra = librosa.feature.melspectrogram(S=power_spectra, n_mels=n_mels, fmin=fmin)\n",
    "    \n",
    "    mel_log = np.log( np.maximum(spectra_abs_min, np.abs(melspectra) ))\n",
    "\n",
    "    # Shape of batches will be (Batch, MelsChannel, TimeStep) for PyTorch - no need for Transpose\n",
    "    data = dict( \n",
    "        mels = melspectra,\n",
    "        mel_log = mel_log,\n",
    "        spectra_complex = spectra_complex,\n",
    "        #spectra_real = spectra_complex.real, \n",
    "        #spectra_imag = spectra_complex.imag, \n",
    "    )\n",
    "    \n",
    "    hkl.dump(data, melspectra_filepath, mode='w', compression='gzip')\n",
    "    return melspectra_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_filenames = [ audio_to_melspectrafile(f) for f in audio_filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't see a clean way of shuffling without having loaded all the input first...\n",
    "\n",
    "#class DatasetFromMelspectraFile(torch.utils.data.Dataset):\n",
    "#    def __init__(self, melspectra_filepath):\n",
    "#        super(DatasetFromMelspectraFile, self).__init__()\n",
    "#        \n",
    "#        data = hkl.load(melspectra_filepath)\n",
    "#        self.mels = data['mels']\n",
    "#\n",
    "#    def __getitem__(self, index):\n",
    "#        offset = index*mel_samples \n",
    "#        a = self.mels[:, offset:offset+mel_samples]\n",
    "#        return a,a  # This is a VAE situation\n",
    "#\n",
    "#    def __len__(self):  \n",
    "#        return self.mels.shape[1]//mel_samples\n",
    "#    \n",
    "#class DatasetFromFiles(torch.utils.data.Dataset):\n",
    "#    def __init__(self, filepath_arr, length_arr):\n",
    "#        super(DatasetFromFiles, self).__init__()\n",
    "#        self.filepaths = filepath_arr\n",
    "#        self.file_index, self.item_index = -1,-1\n",
    "#        self.d = None\n",
    "#        \n",
    "#    def __getitem__(self, index):\n",
    "#        self.item_index+=1\n",
    "#        if self.d is None or self.item_index >= len(self.d):\n",
    "#            self.file_index+=1\n",
    "#            self.d = DatasetFromMelspectraFile(self.filepaths[self.file_index])\n",
    "#            self.item_index=0\n",
    "#        return d[self.item_index]\n",
    "#\n",
    "#    def __len__(self):  \n",
    "#        #return len(self.filepaths)\n",
    "#        return -1 # DUNNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach allows us to load the files into memory only as needed - \n",
    "#   But may not be necessary for our purposes, since the data is actually pretty small\n",
    "\n",
    "def yield_batches_from(melspectra_filepath, bs=batch_size, shuffle=False):\n",
    "    data = hkl.load(melspectra_filepath)\n",
    "    mels = data['mels']\n",
    "    offsets = np.arange(0, mels.shape[1]-mel_samples, mel_samples)\n",
    "    print(\"Batches from file : \", melspectra_filepath, mels.shape, offsets.shape)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(offsets)  # in-place\n",
    "    batch_x = np.zeros( shape=(bs, n_mels, mel_samples) )  # Allocate once\n",
    "    for batch_idx in range(0, offsets.shape[0], bs):\n",
    "        for i in range(0, bs):\n",
    "             batch_x[i, :, :] = mels[:, offsets[i]:offsets[i]+mel_samples]\n",
    "        yield batch_x, batch_x # input -> target\n",
    "    # Stop\n",
    "\n",
    "def yield_batches_from_files(filepaths, bs=batch_size, shuffle=False, shuffle_within=False):\n",
    "    if shuffle:\n",
    "        #random.shuffle(filepaths)  # in-place = meh\n",
    "        filepaths = random.sample( filepaths, len(filepaths) )  # original unchanged(~)\n",
    "    for filepath in filepaths:\n",
    "        file_batcher = yield_batches_from(filepath, bs=bs, shuffle=shuffle_within)\n",
    "        for batch in file_batcher:\n",
    "            yield batch\n",
    "    # Stop\n",
    "\n",
    "# This is how this code looks when used :\n",
    "#for epoch in range(epochs):\n",
    "#    t0 = datetime.datetime.now()\n",
    "#    train_batcher = yield_batches_from_files(mel_filenames, bs=batch_size, shuffle=True, shuffle_within=True)\n",
    "#    for batch_idx, batch in enumerate(train_batcher):\n",
    "#        input, target = batch\n",
    "#        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data # required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # Test ops to get correct Tensor format\n",
    "    t = torch.from_numpy(np.array([[10,11,12,13,14,15,16,17,18,19], \n",
    "                                   [20,21,22,23,24,25,26,27,28,29], \n",
    "                                   [30,31,32,33,34,35,36,37,38,39]\n",
    "                                  ]))\n",
    "    t\n",
    "    #t.view(2,3,5)\n",
    "    t.transpose(0,1).contiguous().view(2,5,3).transpose(1,2)\n",
    "\n",
    "    # Want to convert long set of mels into batches of length mel_samples: \n",
    "    # 0 :\n",
    "    #   10    11    12    13    14    \n",
    "    #   20    21    22    23    24   \n",
    "    #   30    31    32    33    34   \n",
    "    # 1 :\n",
    "    #   15    16    17    18    19\n",
    "    #   25    26    27    28    29\n",
    "    #   35    36    37    38    39\n",
    "\n",
    "def TensorFromMelspectraFile(melspectra_filepath, block_len=mel_samples):\n",
    "    data = hkl.load(melspectra_filepath)\n",
    "    mel_log = data['mel_log']\n",
    "    \n",
    "    if block_len is None: # Allow for 'whole of file' tensor(1,mels,everything)\n",
    "        block_len=mel_log.shape[1]\n",
    "    n_blocks = mel_log.shape[1]//mel_samples\n",
    "    print(\"Read %5d log(mel[%2d]) = %4d blocks from %s\" % \n",
    "          (mel_log.shape[1], mel_log.shape[0], n_blocks, melspectra_filepath,))\n",
    "    \n",
    "    mel_log_trunc_t = mel_log[:, :n_blocks*mel_samples ].T\n",
    "    #print(torch.from_numpy(mel_log_trunc_t).contiguous().size())\n",
    "    return ( torch.from_numpy(mel_log_trunc_t).contiguous()\n",
    "             .view(n_blocks, block_len, n_mels).transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_datasets = []\n",
    "for f in mel_filenames:\n",
    "    t = TensorFromMelspectraFile(f)\n",
    "    mel_datasets.append( torch.utils.data.TensorDataset(t, t) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_dataset = torch.utils.data.ConcatDataset(mel_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNettyCell(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, cond_channels=0, \n",
    "                 kernel_size=3, stride=1, dilation=1):\n",
    "        super(WaveNettyCell, self).__init__()\n",
    "        \n",
    "        self.gate   = torch.nn.Conv1d(in_channels, hidden_channels, \n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, dilation=dilation, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "        self.signal = torch.nn.Conv1d(in_channels, hidden_channels, \n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride, dilation=dilation, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "        \n",
    "        self.cond = cond_channels>0\n",
    "        if self.cond:\n",
    "            self.gate_cond   = torch.nn.Conv1d(cond_channels, hidden_channels, kernel_size=1, bias=False)\n",
    "            self.signal_cond = torch.nn.Conv1d(cond_channels, hidden_channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.recombine = torch.nn.Conv1d(hidden_channels, in_channels, \n",
    "                                    kernel_size=1, stride=1, dilation=1, \n",
    "                                    padding=0, groups=1, bias=True)\n",
    "        \n",
    "        self.padding = (0, (kernel_size-1)*(dilation+stride*0)  )\n",
    "            \n",
    "    def forward(self, input, condition=None):\n",
    "        gate = self.gate(input)\n",
    "        signal = self.signal(input)\n",
    "        if self.cond:\n",
    "            gate   = gate   + self.gate_cond(condition)\n",
    "            signal = signal + self.signal_cond(condition)\n",
    "\n",
    "        gate = F.sigmoid(gate)\n",
    "            \n",
    "        mult = gate * F.tanh(signal)\n",
    "        \n",
    "        # Yes : There's no side/skip here : It's just a fancy feed-forward\n",
    "        return input + F.pad( self.recombine(mult), self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=128):\n",
    "        super(VQ_encoder, self).__init__()\n",
    "        \n",
    "        # See https://fomoro.com/tools/receptive-fields/\n",
    "        \n",
    "        #   #3,2,1,VALID;3,2,1,VALID;3,2,1,VALID;3,2,1,VALID;3,2,1,VALID\n",
    "        #self.conv = [ WaveNettyCell(in_channels, hidden_channels, \n",
    "        #                            stride=2) for c in range(4) ]\n",
    "            \n",
    "        #   #3,1,1,VALID;3,1,2,VALID;3,1,4,VALID;3,1,8,VALID;3,1,16,VALID\n",
    "        #   receptive field = 63 timesteps\n",
    "        self.conv = torch.nn.ModuleList([ WaveNettyCell(in_channels, hidden_channels, \n",
    "                                    dilation=d) for d in [1,2,4,8,16] ])\n",
    "            \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for c in self.conv:\n",
    "            x = c(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_quantiser(torch.nn.Module):\n",
    "    def __init__(self, n_symbols, latent_dimension):\n",
    "        super(VQ_quantiser, self).__init__()\n",
    "            \n",
    "    def forward(self, input):\n",
    "        return input, []  # Doesn't do quantisation yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_decoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, latent_channels=0, hidden_channels=128):\n",
    "        super(VQ_decoder, self).__init__()\n",
    "        \n",
    "        self.conv = torch.nn.ModuleList([ WaveNettyCell(in_channels, hidden_channels, \n",
    "                                    #cond_channels=latent_channels,\n",
    "                                    dilation=d) for d in [1,2,4,8,16] ])\n",
    "        \n",
    "        #self.c1 = WaveNettyCell(in_channels, hidden_channels, dilation=1)\n",
    "            \n",
    "    def forward(self, input, latent=None):\n",
    "        x = input\n",
    "        for c in self.conv:\n",
    "            #x = c(x, latent)\n",
    "            x = c(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_VAE_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VQ_VAE_Model, self).__init__()\n",
    "        #self.name=name\n",
    "        \n",
    "        self.channels, self.n_symbols = n_mels, 64\n",
    "        \n",
    "        self.encoder = VQ_encoder(self.channels)\n",
    "        self.quant   = VQ_quantiser(self.n_symbols, self.channels)\n",
    "        self.decoder = VQ_decoder(self.channels)\n",
    "        \n",
    "        print(f\"Number of parameter variables : {len(list(self.parameters()))}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.encoder(input)\n",
    "        x, symbols = self.quant(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, symbols\n",
    "\n",
    "    def train_(self, input, target):\n",
    "        self.train()  # Set mode\n",
    "        self.optimizer.zero_grad()\n",
    "        output, symbols = self(input)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def test_(self, input):\n",
    "        self.eval()\n",
    "        output, symbols = self(input)\n",
    "        return symbols\n",
    "\n",
    "    def save(self, save_template, epoch):\n",
    "        #torch.save(self.state_dict(), 'model/epoch_{}_{:02d}.pth'.format(self.name, epoch))\n",
    "        torch.save(self.state_dict(), save_template.format(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQ_VAE_Model()\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t0 = datetime.datetime.now()\n",
    "    train_batches = torch.utils.data.DataLoader(mel_dataset, batch_size=batch_size, \n",
    "                                                shuffle=True, num_workers=1)\n",
    "    for batch_idx, batch in enumerate(train_batches):\n",
    "        input, target = batch\n",
    "        \n",
    "        x = Variable( input.type(dtype) )\n",
    "        y = Variable( target.type(dtype) )\n",
    "        mse = model.train_(x, y)\n",
    "        \n",
    "        print(f\"Epoch {epoch:2}, Batch {batch_idx:2}, %.6f\" % (float(mse*1000*1000),))\n",
    "        \n",
    "        #print('Train Epoch: {:2d} [{:6d}/{:6d} ({:3.0f}%)] Non-relations accuracy: {:3.0f}% | Relations accuracy: {:3.0f}% | Tricky accuracy: {:3.0f}% | '.format(\n",
    "        #        epoch, batch_idx * bs * example_factor, \n",
    "        #        len(norel[0]) * example_factor, \n",
    "        #        100. * batch_idx * bs/ len(norel[0]), \n",
    "        #        accuracy_norel, accuracy_birel, accuracy_trirel, \n",
    "        #     ))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'{234.3453453453434534:6.2}'  Wierd choice for format specifiers : overall_width.digits_of_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_step   = 12.5/1000. # 12.5ms\n",
    "\n",
    "audio_filenames = [ './librivox/guidetomen_%02d_rowland_64kb.mp3' % (i,) for i in [1,2,3]]\n",
    "audio_filenames\n",
    "\n",
    "mel_filenames = [ f.replace('.mp3', '.melspectra.hkl') for f in audio_filenames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_filename_test = mel_filenames[1]\n",
    "\n",
    "with open(mel_filename_test.replace('.hkl', '.16_k2.sym'), 'rt') as f:\n",
    "    mel_sym_str = f.read()\n",
    "    \n",
    "mel_sym_chars = set(mel_sym_str)\n",
    "mel_sym_dict  = { c:i for i,c in enumerate(list(mel_sym_chars)) }\n",
    "mel_sym = np.array( [mel_sym_dict[c] for c in mel_sym_str] )\n",
    "\n",
    "mel_sym_silence = mel_sym_dict[' ']\n",
    "\n",
    "mel_sym_str[100:115], mel_sym[100:115], mel_sym.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the sym data, by counting the duplicates, \n",
    "# and storing 'initial char', 'char count' and 'initial char idx'\n",
    "mel_sym_ct, mel_sym_cc, mel_sym_cn = [], [], []\n",
    "prev_c, prev_n = '', 0\n",
    "for t, c in enumerate(mel_sym_str):\n",
    "    if c==prev_c: \n",
    "        prev_n+=1 # Add one to count\n",
    "    else:\n",
    "        mel_sym_cn.append(prev_n)  # Store count of previous char\n",
    "        mel_sym_ct.append(t)       # Start on new char's index\n",
    "        mel_sym_cc.append(c)       # Start on new char's value\n",
    "        prev_c, prev_n = c, 1\n",
    "mel_sym_cn.append(prev_n)  # Store last count value\n",
    "\n",
    "mel_sym_ct = np.array( mel_sym_ct )\n",
    "mel_sym_ci = np.array( [mel_sym_dict[c] for c in mel_sym_cc] )\n",
    "mel_sym_cn = np.array( mel_sym_cn[1:])   # Kill the first value, and convert to numpy\n",
    "\n",
    "(mel_sym_str[66:95], \n",
    " mel_sym_cc[0:10], mel_sym_ci[0:10], \n",
    " mel_sym_cn[0:10], mel_sym_ct[0:10], \n",
    " mel_sym_ci.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_one_sec_per_line(s, t_min=0., t_max=None):\n",
    "    each_line = int(1/fft_step)\n",
    "    if t_max is None: t_max = len(s)*fft_step\n",
    "    for t in np.arange(t_min, t_max, 1.):\n",
    "        i = int(t/fft_step)\n",
    "        print( s[i:i+each_line] )\n",
    "        \n",
    "print_one_sec_per_line(mel_sym_str, 10.0, 15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in text as words\n",
    "\n",
    "# Create initial array of word starts\n",
    "#   with initial guess of maximum error bars\n",
    "# Create map of word -> word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set #FILE: = #FILE:  guidetomen_02_rowland_64kb.mp3\n",
    "#Set #OFFSET_START: = #OFFSET_START: 7.0\n",
    "#Set #OFFSET_END: = #OFFSET_END: 613.0\n",
    "offset_start, offset_end = 7.0, 613.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mel_filename_test.replace('.melspectra.hkl', '.txt'), 'rt') as f:\n",
    "    mel_txt = f.read()\n",
    "\n",
    "txt_arr = mel_txt.replace('\\n', ' ').split(' ')\n",
    "txt_arr.insert(0, '#EOS') # Extra one at start\n",
    "txt_arr.insert(0, '') # Helps start process\n",
    "len(txt_arr), ','.join( txt_arr[0:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some matrices to fill in - these are timings in seconds \n",
    "txt_length_est = np.array( [ len(s) for s in txt_arr ] )\n",
    "#txt_err = np.zeros_like( txt_starts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total up the lengths of the words (in characters) \n",
    "#  - the initial timing guess is going to be proportional \n",
    "txt_length_est[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_starts = txt_length_est.cumsum()\n",
    "txt_starts = offset_start + txt_starts*(offset_end-offset_start)/txt_starts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_starts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_err_min = 5. # Seconds\n",
    "txt_err_max = txt_starts[-1] * 0.10 # i.e. plus or minus the given percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_err = np.array( [ (i-txt_starts.shape[0]) for i, txt in enumerate(txt_arr) ] )\n",
    "txt_err = np.square(txt_err)\n",
    "txt_err = txt_err.max() - txt_err # Now a parabola peaking in the middle\n",
    "\n",
    "txt_err_scale = (txt_err_max - txt_err_min) / txt_err[ txt_starts.shape[0]//2 ]\n",
    "\n",
    "txt_err = txt_err*txt_err_scale + txt_err_min\n",
    "\n",
    "txt_err[0:5], txt_err[ txt_starts.shape[0]//2:txt_starts.shape[0]//2+5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global alignment\n",
    "\n",
    "# Idea : Train up a word embedding based on range (within error bars)\n",
    "# Function to map word to set of ranges\n",
    "# Function to convert ranges into a % of whole\n",
    "# Table of words with % coverage (to check whether that's a doable idea)\n",
    "# Find word-range average vector (vs. not-in-word-range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx={}\n",
    "for i, w in enumerate(txt_arr):\n",
    "    if w not in word_to_idx: word_to_idx[w]=[]\n",
    "    word_to_idx[w].append( i )\n",
    "word_to_idx['heart'], len(txt_arr), len(word_to_idx)  #bachelor mere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_range_mask(w):\n",
    "    mask = np.zeros_like(mel_sym)\n",
    "    for i in word_to_idx.get(w, []):\n",
    "        i_min = (txt_starts[i]-txt_err[i])/fft_step\n",
    "        if i_min<0: i_min=0/fft_step\n",
    "        i_next = i+1 if i<txt_starts.shape[0]-1 else i\n",
    "        i_max = (txt_starts[i_next]+txt_err[i])/fft_step\n",
    "        if i_max>mask.shape[0]: i_max=mask.shape[0]/fft_step\n",
    "        #print(\"(i_min, i_max) = \", i_min, i_max)\n",
    "        mask[ int(i_min):int(i_max) ] = 1.\n",
    "    return mask\n",
    "\n",
    "word_mask=word_range_mask('the')  # bachelor mere woman man the\n",
    "np.sum(word_mask) / word_mask.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at word frequencies, and mask coverage\n",
    "words_freq_ordered = sorted(word_to_idx.keys(), key=lambda k: -len(word_to_idx[k]))\n",
    "len(words_freq_ordered), words_freq_ordered[0], words_freq_ordered[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words_freq_ordered:\n",
    "    n = len(word_to_idx[w])\n",
    "    if n<2: continue # Not enough for anything...\n",
    "    word_mask=word_range_mask(w)\n",
    "    coverage=np.sum(word_mask) / word_mask.shape[0]\n",
    "    print(\"%6.2f%%, %4d, %s\" % (coverage*100., n, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create histogram of symbol frequencies corresponding to mask\n",
    "def histogram_freqs(mask, remove_silence=True):\n",
    "    print(np.sum(mask) / mask.shape[0])\n",
    "    #inside = bincount(mel_sym, weights=mask)\n",
    "    n_sym = len(mel_sym_chars)\n",
    "    \n",
    "    inside_bins  = np.bincount(mel_sym, weights=mask)\n",
    "    outside_bins = np.bincount(mel_sym, weights=1-mask)\n",
    "    \n",
    "    if remove_silence:\n",
    "        inside_bins[mel_sym_dict[' ']]=0\n",
    "        outside_bins[mel_sym_dict[' ']]=0\n",
    "    \n",
    "    rects1 = plt.bar(np.arange(0, n_sym)-.2, width=0.4, color='r',\n",
    "                     height=inside_bins/np.sum(inside_bins))\n",
    "    rects2 = plt.bar(np.arange(0, n_sym)+.2, width=0.4, color='b',\n",
    "                     height=outside_bins/np.sum(outside_bins))\n",
    "\n",
    "    plt.xlabel('Symbol#')\n",
    "    plt.ylabel('Freq')\n",
    "    plt.xticks(np.arange(0, n_sym, 1.0))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "histogram_freqs(word_range_mask('kiss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the ranges of the actual sounds in the \n",
    "# speech audio - ignoring short silences\n",
    "\n",
    "silence_is_short_len = 8  # 100ms\n",
    "\n",
    "spans, span_i, span_n = [], [], []\n",
    "def add_span(span_start_index, s_i, s_n):\n",
    "    if len(s_i)>0:\n",
    "        spans.append(dict(\n",
    "            t=span_start_index,\n",
    "            span=s_i,\n",
    "            count=s_n,\n",
    "        ))\n",
    "for idx, c in enumerate(mel_sym_cc):\n",
    "    ci, cn = mel_sym_ci[idx], mel_sym_cn[idx]\n",
    "    if ci==mel_sym_silence and cn>silence_is_short_len:\n",
    "        add_span(mel_sym_ct[idx-1], span_i, span_n)\n",
    "        span=[]\n",
    "        continue # \n",
    "    span_i.append(ci)\n",
    "    span_n.append(cn)\n",
    "add_span(mel_sym_ct[idx-1], span_i, span_n)\n",
    "\n",
    "len(spans), #spans[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local alignment (looking within word-error-bar ranges only)\n",
    "\n",
    "# Idea : Mostly textual alignment\n",
    "# Have a look symbols in appropriate ranges for several word examples\n",
    "# See whether a simple optimisation can align multiple segments\n",
    "# Would reduce error bars massively\n",
    "# Possibly : \n",
    "#   http://mlpy.sourceforge.net/docs/3.4/lcs.html#standard-lcs  (GPL3, though)\n",
    "#   https://github.com/Samnsparky/py_common_subseq (MIT)\n",
    "#   https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Longest_common_substring#Python_3 \n",
    "#   https://docs.python.org/3/library/difflib.html (not quite...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install py_common_subseq\n",
    "#import py_common_subseq as subseq  # But doesn't output alignment\n",
    "\n",
    "# Homebrew version : does what we actually want (return array of index correspondences)\n",
    "def lcs(a_arr, b_arr):  \n",
    "    m = np.zeros( (len(a_arr)+1, len(b_arr)+1) )\n",
    "    # offset all the i and j by 1, since we need blank first row+col\n",
    "    for i, a in enumerate(a_arr):\n",
    "        for j, b in enumerate(b_arr):\n",
    "            if a == b:\n",
    "                m[i+1, j+1] = m[i, j] + 1\n",
    "            else:\n",
    "                m[i+1, j+1] = max(m[i+1, j], m[i, j+1])\n",
    "                \n",
    "    #?  a_i=np.zeros( (lengths[-1,-1], ))\n",
    "    a_i, b_i = [], []\n",
    "    i, j = len(a_arr), len(b_arr)\n",
    "    while i>0 and j>0:\n",
    "        if m[i, j] == m[i-1, j]:\n",
    "            i -= 1\n",
    "        elif m[i, j] == m[i, j-1]:\n",
    "            j -= 1\n",
    "        else: # a_arr[i-1] == b_arr[j-1]\n",
    "            a_i.append(i-1)\n",
    "            b_i.append(j-1)\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    \n",
    "    return a_i[::-1], b_i[::-1]\n",
    "\n",
    "lcs([17, 9,99,7,4,8,3,7,5,2,4,1,2,1,2,4,5,6],\n",
    "    [1, 17,11,7,4,8,3,7,0,2,4,0,2,  2,  5,6],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://colinraffel.com/publications/thesis.pdf\n",
    "\n",
    "# Idea : Align symbols using linear DTW (v fast)\n",
    "# Assign random increments to symbols, and see whether linear DTW can match the alignments\n",
    "# https://blog.acolyer.org/2016/05/11/searching-and-mining-trillions-of-time-series-subsequences-under-dynamic-time-warping/\n",
    "\n",
    "# Idea : Align (using DTW) the mels or embeddings within the word-error-bar segments\n",
    "# This would be multiple small alignments too\n",
    "# Needs vector DTW (like librosa has...)\n",
    "\n",
    "#  https://github.com/pierre-rouanet/dtw (GPL3 : Unusable)\n",
    "#  https://github.com/slaypni/fastdtw/tree/master/fastdtw  (MIT)\n",
    "#  https://github.com/ricardodeazambuja/DTW (cardiod example : CC0 licensed)\n",
    "\n",
    "\n",
    "## Combo\n",
    "\n",
    "# Use global word embeddings to weight samples in local ranges\n",
    "\n",
    "\n",
    "## Global alignment\n",
    "\n",
    "# Idea : Train up a word embedding based on range (within error bars)\n",
    "# Find word-range average vector (vs. not-in-word-range)\n",
    "# Use this to do a DTW across all words vs all timesteps\n",
    "\n",
    "# Alternative : Use same word embedding to do some kind of annealing :\n",
    "#   gradually reducing error bars (and improving embedding, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Mode Resources \n",
    "\n",
    "*  https://www.tensorflow.org/get_started/eager\n",
    "*  https://github.com/tensorflow/models/blob/master/official/mnist/mnist_eager.py\n",
    "*  [Eager Execution (TensorFlow Dev Summit 2018)](https://www.youtube.com/watch?v=T8AW0fKP0Hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [10,6]\n",
    "\n",
    "#from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "plot = True\n",
    "\n",
    "## Inner Optimisations\n",
    "inner_stepsize = 0.02 # stepsize in inner SGD\n",
    "inner_epochs   = 1 # number of epochs of each inner SGD\n",
    "inner_batchsize= 10 # Size of training minibatches\n",
    "\n",
    "## Outer Optimisations\n",
    "# initial stepsize of outer optimization, i.e., meta-optimization\n",
    "outer_stepsize0 = 0.1 \n",
    "# number of outer updates; each iteration we sample one task and update on it\n",
    "outer_steps = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(seed)\n",
    "tf.set_random_seed(seed+0)\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task distribution\n",
    "x_all = np.linspace(-5, 5, 50)[:,None] # All of the x points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification problems that we're going to learn about\n",
    "def gen_task(): \n",
    "    phase = rng.uniform(low=0, high=2*np.pi)\n",
    "    ampl = rng.uniform(2., 5.)\n",
    "    f_randomsine = lambda x : np.sin(x+phase)*ampl\n",
    "    return f_randomsine # i.e. return a random *function* to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - this model is going to be easily *trainable* to \n",
    "#  solve the each of the problems we generate\n",
    "\n",
    "#model = nn.Sequential(\n",
    "#    nn.Linear(1, 64),\n",
    "#    nn.Tanh(), \n",
    "#    nn.Linear(64, 64),\n",
    "#    nn.Tanh(),\n",
    "#    nn.Linear(64, 1),\n",
    "#)\n",
    "\n",
    "# Reptile paper uses ReLU, but Tanh gives slightly better results\n",
    "inits=dict(kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform')\n",
    "model = tf.keras.Sequential([\n",
    "  # input shape required\n",
    "  tf.keras.layers.Dense(64, activation=\"tanh\", input_shape=(1,), **inits), \n",
    "  tf.keras.layers.Dense(64, activation=\"tanh\", **inits),\n",
    "  tf.keras.layers.Dense(1, **inits)  # NB: Keras 'knows' the previous layer sizes\n",
    "])\n",
    "\n",
    "# Alternatively (more PyTorch-y) : \n",
    "class SineModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(SineModel, self).__init__()\n",
    "    # Input shape not required, since it is set the first time model is called\n",
    "    self.dense1 = tf.keras.layers.Dense(units=64, activation=\"tanh\")\n",
    "    self.dense2 = tf.keras.layers.Dense(units=64, activation=\"tanh\")\n",
    "    self.dense3 = tf.keras.layers.Dense(units=1)  \n",
    "\n",
    "  def call(self, input):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    x = self.dense1(input)    \n",
    "    x = self.dense2(x)  \n",
    "    x = self.dense3(x) \n",
    "    return x\n",
    "#model = SineModel()\n",
    "\n",
    "\n",
    "#def totorch(x):\n",
    "#    return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "#def train_current_model_on_batch(x, y):\n",
    "#    x = totorch(x)\n",
    "#    y = totorch(y)\n",
    "#    model.zero_grad()\n",
    "#    ypred = model(x)\n",
    "#    loss = (ypred - y).pow(2).mean()\n",
    "#    loss.backward()\n",
    "#    for param in model.parameters():\n",
    "#        param.data -= inner_stepsize * param.grad.data\n",
    "\n",
    "def train_current_model_on_batch(x, y):\n",
    "    x = tfe.Variable(x, dtype='float32')\n",
    "    y = tfe.Variable(y, dtype='float32')\n",
    "    with tfe.GradientTape() as tape:\n",
    "        ypred = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(ypred - y))\n",
    "    grads = tape.gradient(loss, model.variables)    \n",
    "    \n",
    "    for param, grad in zip(model.variables, grads):\n",
    "        param.assign_sub(inner_stepsize * grad)\n",
    "        \n",
    "#def predict_using_current_model(x):\n",
    "#    x = totorch(x)\n",
    "#    return model(x).data.numpy()\n",
    "\n",
    "def predict_using_current_model(x):\n",
    "    x = tfe.Variable(x, dtype='float32')\n",
    "    return model(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a fixed task and minibatch for visualization\n",
    "f_plot = gen_task()  # This is one specfic task\n",
    "xtrain_plot = x_all[rng.choice(len(x_all), size=inner_batchsize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reptile training loop\n",
    "for outer_step in range(outer_steps):\n",
    "    #weights_before = deepcopy(model.state_dict())\n",
    "    weights_before = [ param.read_value() for param in model.variables ]\n",
    "    \n",
    "    # Generate a task of the sort we want to learn about learning\n",
    "    f = gen_task()  # This is a different task every time\n",
    "    y_all = f(x_all) # Get the correct outputs for the x_all values (i.e a dataset)\n",
    "    \n",
    "    # Do SGD on this task + dataset\n",
    "    inds = rng.permutation(len(x_all))  # Shuffle data indices\n",
    "    for _ in range(inner_epochs):\n",
    "        for start in range(0, len(x_all), inner_batchsize):\n",
    "            batch_indices = inds[start:start+inner_batchsize]\n",
    "            train_current_model_on_batch(\n",
    "                x_all[batch_indices], y_all[batch_indices]\n",
    "            )\n",
    "            \n",
    "    # Interpolate between current weights and trained weights from this task\n",
    "    #   i.e. (weights_before - weights_after) is the meta-gradient\n",
    "    \n",
    "    #weights_after = model.state_dict()\n",
    "    weights_after = [ param.read_value() for param in model.variables ]\n",
    "    \n",
    "    outer_stepsize = outer_stepsize0*(1-outer_step/outer_steps) # linear schedule\n",
    "    \n",
    "    # This updates the weights in the model -\n",
    "    #   Not a 'training' gradient update, but one reflecting\n",
    "    #   the training that occurred during the inner loop training\n",
    "    \n",
    "    #model.load_state_dict({name : \n",
    "    #    weights_before[name] + (weights_after[name]-weights_before[name])*outer_stepsize \n",
    "    #    for name in weights_before})\n",
    "    \n",
    "    for param, w_before, w_after in zip(model.variables, weights_before, weights_after):\n",
    "        param.assign( w_before + (w_after-w_before)*outer_stepsize )\n",
    "\n",
    "    # Periodically plot the results on a particular task and minibatch\n",
    "    if plot and outer_step==0 or (outer_step+1) % 1000 == 0:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = plt.subplot(111)\n",
    "        \n",
    "        f = f_plot\n",
    "        \n",
    "        #weights_before = deepcopy(model.state_dict()) # save snapshot before evaluation\n",
    "        weights_saved = [ param.read_value() for param in model.variables ]\n",
    "        \n",
    "        # Plot the initial model (having seen no data to train on for this task)\n",
    "        plt.plot(x_all, predict_using_current_model(x_all), \n",
    "                 label=\"pred after 0\", color=(0.5,0.5,1))\n",
    "\n",
    "        for inner_step in range(1, 32+1):\n",
    "            train_current_model_on_batch(xtrain_plot, f(xtrain_plot))\n",
    "            #if (inner_step) % 8 == 0:\n",
    "            if inner_step in [1, 2, 4, 8, 16, 32]:\n",
    "                frac = np.log(inner_step)/np.log(32.)\n",
    "                plt.plot(x_all, predict_using_current_model(x_all), \n",
    "                         label=\"pred after %i\" % (inner_step), \n",
    "                         color=(frac, 0, 1-frac))\n",
    "        lossval = np.square(predict_using_current_model(x_all) - f(x_all)).mean()\n",
    "\n",
    "        plt.plot(x_all, f(x_all), label=\"true\", color=(0,1,0))\n",
    "        \n",
    "        plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
    "        plt.ylim(-4.5,4.5)\n",
    "        \n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))        \n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"outer_step             {outer_step+1:6d}\")\n",
    "        # would be better to average loss over a set of examples, \n",
    "        #   but this is optimized for brevity\n",
    "        print(f\"loss on plotted curve   {lossval:.3f}\") \n",
    "        print(f\"-----------------------------\")\n",
    "\n",
    "        #model.load_state_dict(weights_before) # restore from snapshot \n",
    "        for param, w_saved in zip(model.variables, weights_saved):\n",
    "            param.assign( w_saved )\n",
    "        \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tfe.Variable([[1, 2], [3, 4]])\n",
    "\n",
    "b = a.read_value()\n",
    "d = a.numpy()\n",
    "\n",
    "a.assign_add([[2, 2], [1, 1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
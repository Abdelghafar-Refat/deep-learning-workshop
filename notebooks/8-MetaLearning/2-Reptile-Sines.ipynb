{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [10,6]\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "plot = True\n",
    "\n",
    "## Inner Optimisations\n",
    "inner_stepsize = 0.02 # stepsize in inner SGD\n",
    "inner_epochs   = 1 # number of epochs of each inner SGD\n",
    "inner_batchsize= 10 # Size of training minibatches\n",
    "\n",
    "## Outer Optimisations\n",
    "# initial stepsize of outer optimization, i.e., meta-optimization\n",
    "outer_stepsize0 = 0.1 \n",
    "# number of outer updates; each iteration we sample one task and update on it\n",
    "outer_steps = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task distribution\n",
    "x_all = np.linspace(-5, 5, 50)[:,None] # All of the x points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification problems that we're going to learn about\n",
    "def gen_task(): \n",
    "    phase = rng.uniform(low=0, high=2*np.pi)\n",
    "    ampl = rng.uniform(2., 5.)\n",
    "    f_randomsine = lambda x : np.sin(x+phase)*ampl\n",
    "    return f_randomsine # i.e. return a random *function* to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - this model is going to be easily *trainable* to \n",
    "#  solve the each of the problems we generate\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 64),\n",
    "    nn.Tanh(), # Reptile paper uses ReLU, but Tanh gives slightly better results\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "def totorch(x):\n",
    "    return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "def train_current_model_on_batch(x, y):\n",
    "    x = totorch(x)\n",
    "    y = totorch(y)\n",
    "    model.zero_grad()\n",
    "    ypred = model(x)\n",
    "    loss = (ypred - y).pow(2).mean()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data -= inner_stepsize * param.grad.data\n",
    "\n",
    "def predict_using_current_model(x):\n",
    "    x = totorch(x)\n",
    "    return model(x).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a fixed task and minibatch for visualization\n",
    "f_plot = gen_task()  # This is one specfic task\n",
    "xtrain_plot = x_all[rng.choice(len(x_all), size=inner_batchsize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reptile training loop\n",
    "for outer_step in range(outer_steps):\n",
    "    weights_before = deepcopy(model.state_dict())\n",
    "    \n",
    "    # Generate a task of the sort we want to learn about learning\n",
    "    f = gen_task()  # This is a different task every time\n",
    "    y_all = f(x_all) # Get the correct outputs for the x_all values (i.e a dataset)\n",
    "    \n",
    "    # Do SGD on this task + dataset\n",
    "    inds = rng.permutation(len(x_all))  # Shuffle data indices\n",
    "    for _ in range(inner_epochs):\n",
    "        for start in range(0, len(x_all), inner_batchsize):\n",
    "            batch_indices = inds[start:start+inner_batchsize]\n",
    "            train_current_model_on_batch(\n",
    "                x_all[batch_indices], y_all[batch_indices]\n",
    "            )\n",
    "            \n",
    "    # Interpolate between current weights and trained weights from this task\n",
    "    #   i.e. (weights_before - weights_after) is the meta-gradient\n",
    "    weights_after = model.state_dict()\n",
    "    \n",
    "    outer_stepsize = outer_stepsize0*(1-outer_step/outer_steps) # linear schedule\n",
    "    \n",
    "    # This updates the weights in the model -\n",
    "    #   Not a 'training' gradient update, but one reflecting\n",
    "    #   the training that occurred during the inner loop training\n",
    "    model.load_state_dict({name : \n",
    "        weights_before[name] + (weights_after[name]-weights_before[name])*outer_stepsize \n",
    "        for name in weights_before})\n",
    "\n",
    "    # Periodically plot the results on a particular task and minibatch\n",
    "    if plot and outer_step==0 or (outer_step+1) % 1000 == 0:\n",
    "        plt.cla()\n",
    "        f = f_plot\n",
    "        \n",
    "        weights_before = deepcopy(model.state_dict()) # save snapshot before evaluation\n",
    "        \n",
    "        # Plot the initial model (having seen no data to train on for this task)\n",
    "        plt.plot(x_all, predict_using_current_model(x_all), \n",
    "                 label=\"pred after 0\", color=(0.5,0.5,1))\n",
    "\n",
    "        for inner_step in range(1, 32+1):\n",
    "            train_current_model_on_batch(xtrain_plot, f(xtrain_plot))\n",
    "            #if (inner_step) % 8 == 0:\n",
    "            if inner_step in [1, 2, 4, 8, 16, 32]:\n",
    "                frac = np.log(inner_step)/np.log(32.)\n",
    "                plt.plot(x_all, predict_using_current_model(x_all), \n",
    "                         label=\"pred after %i\" % (inner_step), \n",
    "                         color=(frac, 0, 1-frac))\n",
    "        lossval = np.square(predict_using_current_model(x_all) - f(x_all)).mean()\n",
    "\n",
    "        plt.plot(x_all, f(x_all), label=\"true\", color=(0,1,0))\n",
    "        \n",
    "        plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
    "        plt.ylim(-4.5,4.5)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        #plt.pause(0.01)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"outer_step             {outer_step+1:6d}\")\n",
    "        # would be better to average loss over a set of examples, \n",
    "        #   but this is optimized for brevity\n",
    "        print(f\"loss on plotted curve   {lossval:.3f}\") \n",
    "        print(f\"-----------------------------\")\n",
    "\n",
    "        model.load_state_dict(weights_before) # restore from snapshot \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
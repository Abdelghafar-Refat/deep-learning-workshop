{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagger\n",
    "\n",
    "This example trains a RNN to tag words from a corpus - \n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32\n",
    "EMBEDDING_DIM=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentence_splitter.tokenize(\"This is Mr. Smith's tokenized test. The U.S.A gives us sent two. Is this sent three?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"This is Mr. Smith's tokenized test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An interesting corpus (TODO : provide a simple downloader) :\n",
    "corpus_text_file = './data/RNN/en.wikipedia.2010.100K.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                # print(line)\n",
    "                # .decode(\"utf-8\")\n",
    "                n,l = line.split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    tree_banked = tokenizer.tokenize(s)\n",
    "                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n",
    "                        yield tree_banked\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pos_tagger = PerceptronTagger(load=True)\n",
    "' | '.join(list(pos_tagger.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Let 's see what part of speech analysis on this sample text looks like .\".split(' ')\n",
    "#s = next(corpus_sentence_tokens_gen)\n",
    "pos_tagger.tag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twist : Not interested in all classes...\n",
    "\n",
    "To simplify (dramatically), our RNN will be trained to just tell the difference between 'is ordinary word' and 'is entity name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list = 'O E'.split(' ')\n",
    "pos_tagger_entity_tags = set('NNP'.split(' '))\n",
    "pos_tagger_to_idx   = dict([ (t,(1 if t in pos_tagger_entity_tags else 0)) for i,t in enumerate(pos_tagger.classes)])\n",
    "TAG_SET_SIZE= len(tag_list)\n",
    "\n",
    "pos_tagger_to_idx['NNP'], pos_tagger_to_idx['VBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings\n",
    "Using the python package :  https://github.com/maciejkula/glove-python , and code samples from : http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
    "\n",
    "### Create the Co-occurrence Matrix\n",
    "For speed, this looks at the first 100,000 tokens in the corpus - and should create the co-occurences in 30 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glove\n",
    "glove_corpus = glove.Corpus()\n",
    "\n",
    "corpus_sentences = [ \n",
    "        [ w.lower() for w in next(corpus_sentence_tokens_gen)] # All lower-case\n",
    "        for _ in range(0,100*1000) \n",
    "    ]\n",
    "\n",
    "# Fit the co-occurrence matrix using a sliding window of 10 words.\n",
    "t0 = time.time()\n",
    "glove_corpus.fit(corpus_sentences, window=10)\n",
    "\n",
    "print(\"Dictionary length=%d\" % (len(glove_corpus.dictionary),))\n",
    "print(\"Co-occurrence calculated in %5.1fsec\" % (time.time()-t0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return the index of the word in the dictionary\n",
    "glove_corpus.dictionary['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the Word Embedding\n",
    "\n",
    "This will make use of up to 4 threads - and each epoch takes 20-30 seconds on a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding = glove.Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)\n",
    "\n",
    "t0 = time.time()\n",
    "glove_epochs, glove_threads = 20, 4 \n",
    "\n",
    "word_embedding.fit(glove_corpus.matrix, epochs=glove_epochs, no_threads=glove_threads, verbose=True)\n",
    "\n",
    "print(\"%d-d word-embedding created in %5.1fsec = %5.1fsec per epoch\" % (\n",
    "        EMBEDDING_DIM, (time.time()-t0), (time.time()-t0)/glove_epochs*glove_threads, ))\n",
    "\n",
    "# Add the word -> id dictionary to the model to allow similarity queries.\n",
    "word_embedding.add_dictionary(glove_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_embedding.save(\"./data/RNN/glove.embedding.50.pkl\")\n",
    "#word_embedding.load(\"./data/RNN/glove.embedding.50.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word-similarity test\n",
    "word_embedding.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word-analogy test\n",
    "def get_embedding_vec(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
    "    if idx<0:\n",
    "        #print(\"Missing word : '%s'\" % (word,))\n",
    "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
    "    return word_embedding.word_vectors[idx]\n",
    "\n",
    "def get_closest_word(vec, number=5):\n",
    "    dst = (np.dot(word_embedding.word_vectors, vec)\n",
    "                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n",
    "                   / np.linalg.norm(vec))\n",
    "    word_ids = np.argsort(-dst)\n",
    "    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n",
    "            if x in word_embedding.inverse_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#analogy_vec = get_embedding_vec('woman') - get_embedding_vec('man') + get_embedding_vec('king')\n",
    "analogy_vec = get_embedding_vec('paris') - get_embedding_vec('france') + get_embedding_vec('italy')\n",
    "#analogy_vec = get_embedding_vec('kitten') - get_embedding_vec('cat') + get_embedding_vec('dog')\n",
    "#analogy_vec = get_embedding_vec('understand') - get_embedding_vec('understood') + get_embedding_vec('ran')\n",
    "get_closest_word(analogy_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem : Embedding is *Poor*\n",
    "\n",
    "Solution : Load a pre-trained word embedding, from a much larger corpus.  Source of this word embedding (created from a 6 billion tokens corpus, with results as 50d vectors): http://nlp.stanford.edu/projects/glove/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Create a downloaded / stripper etc\n",
    "\n",
    "# Due to size constraints, only have the first 100k vectors (i.e. 100k most frequently used words)\n",
    "word_embedding = glove.Glove.load_stanford(\"./data/RNN/glove.first-100k.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded that, play around with the similarity and analogy tests again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN Part-of-Speech Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = EMBEDDING_DIM # ?+1 for capitalisation flag\n",
    "GRAD_CLIP_BOUND = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding_rnn = np.vstack([ \n",
    "        np.zeros( (1, EMBEDDING_DIM,), dtype='float32'),   # This is the 'zero' value (used as a mask in Keras)\n",
    "        np.zeros( (1, EMBEDDING_DIM,), dtype='float32'),   # This is for 'UNK'  (word == 1)\n",
    "        word_embedding.word_vectors,\n",
    "    ])\n",
    "word_embedding_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training / Testing dataset\n",
    "And a 'batch generator' function that delivers data in the right format for RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def batch_sentences(size=BATCH_SIZE):\n",
    "#    return [ next(corpus_sentence_tokens_gen) for i in range(size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test it out\n",
    "#batch_test = lambda : batch_sentences(size=4)\n",
    "#print([ ' '.join(s) for s in batch_test()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesising a 'correct answer' for the Tagger\n",
    "\n",
    "Normally, this would be the (manual) annotations from the corpus itself.  However, we don't have an annotated corpus.  Instead, we're going to use the annotations produced by the NTLK tagger - simplified to only identify 'NNP = entities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After sampling a data batch, we transform it into a one hot feature representation with a mask\n",
    "#def prep_batch_for_network(batch_of_sentences, include_targets=False):\n",
    "#    sentence_max_length = np.array([ len(w) for w in batch_of_sentences ]).max()\n",
    "#    \n",
    "#    # translate into one-hot matrix, mask values and targets\n",
    "#    input_values = np.zeros((len(batch_of_sentences), sentence_max_length, EMBEDDING_DIM), dtype='float32')\n",
    "#    mask_values  = np.zeros((len(batch_of_sentences), sentence_max_length), dtype='float32')\n",
    "#    \n",
    "#    for i, sent in enumerate(batch_of_sentences):\n",
    "#      for j, word in enumerate(sent):\n",
    "#        input_values[i,j] = get_embedding_vec(word) # this is word.lower() in dictionary\n",
    "#      mask_values[i, 0:len(sent) ] = 1.\n",
    "#\n",
    "#    if not include_targets:\n",
    "#        return input_values, mask_values        \n",
    "#    \n",
    "#    target_values  = np.zeros((len(batch_of_sentences), sentence_max_length), dtype='int32')\n",
    "#    for i, sent in enumerate(batch_of_sentences):\n",
    "#        sentence_tags = pos_tagger.tag(sent)\n",
    "#        for j, word_tag in enumerate(sentence_tags):\n",
    "#            target_values[i,j] = pos_tagger_to_idx[word_tag[1]]  # tags are returned as tuples (word, tag)\n",
    "#    \n",
    "#    return input_values, mask_values, target_values\n",
    "\n",
    "def word_to_idx_rnn(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)  # since UNK=1 = (-1+2)\n",
    "    return idx+2  # skip ahead 2 places\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.utils.np_utils import to_categorical\n",
    "\n",
    "def batch_for_network_generator():\n",
    "    while True:\n",
    "        batch_of_sentences = [ next(corpus_sentence_tokens_gen) for i in range(BATCH_SIZE) ]    \n",
    "        #print(\"batch_for_network_generator.length = %d\" % (len(batch_of_sentences),))\n",
    "\n",
    "        input_values = np.zeros((BATCH_SIZE, SENTENCE_LENGTH_MAX), dtype='int32')\n",
    "\n",
    "        for i, sent in enumerate(batch_of_sentences):\n",
    "            for j, word in enumerate(sent):\n",
    "                input_values[i,j] = word_to_idx_rnn(word)\n",
    "\n",
    "        # Add extra dimension here to suit Keras' TimeDistributed(Dense(softmax))\n",
    "        #   as discussed : https://github.com/fchollet/keras/issues/6363\n",
    "        target_values  = np.zeros((BATCH_SIZE, SENTENCE_LENGTH_MAX, TAG_SET_SIZE), dtype='int32')\n",
    "        for i, sent in enumerate(batch_of_sentences):\n",
    "            sentence_tags = pos_tagger.tag(sent)\n",
    "            for j, word_tag in enumerate(sentence_tags):\n",
    "                tag = word_tag[1] # tags are returned as tuples (word, tag)\n",
    "                pos_class = pos_tagger_to_idx[tag]  # These are the class #s\n",
    "                target_values[i,j] = to_categorical(pos_class, num_classes=TAG_SET_SIZE)\n",
    "\n",
    "        yield (input_values, target_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the batchifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prep_batch_for_network([\"Mr. Smith works at Red Cat Labs .\".split(' ')], include_targets=True)\n",
    "next(batch_for_network_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Symbolically\n",
    "\n",
    "#### Lasagne RNN tutorial (including conventions &amp; rationale)\n",
    "\n",
    "*  http://colinraffel.com/talks/hammer2015recurrent.pdf\n",
    "\n",
    "#### Lasagne Examples\n",
    "\n",
    "*  https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/recurrent.py\n",
    "*  https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n",
    "\n",
    "#### Keras Examples\n",
    "\n",
    "* \n",
    "*  https://github.com/fchollet/keras/issues/5022 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dir(word_embedding)\n",
    "#word_embedding.max_count, word_embedding.no_components, \n",
    "#word_embedding.word_vectors.shape\n",
    "word_embedding_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras.api.keras.preprocessing import sequence\n",
    "from tensorflow.contrib.keras.api.keras.layers import Input, Embedding, GRU, Dense, Activation\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n",
    "\n",
    "# Hmm : The following is not in the API...\n",
    "from tensorflow.contrib.keras.python.keras.layers import Bidirectional, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(word_embedding_rnn.shape[0],\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[ word_embedding_rnn ],\n",
    "                            input_length=SENTENCE_LENGTH_MAX,\n",
    "                            trainable=False, \n",
    "                            mask_zero=True,\n",
    "                            name=\"SentencesEmbedded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_train = sequence.pad_sequences(x_train, maxlen=SENTENCE_LENGTH_MAX)\n",
    "#x_test  = sequence.pad_sequences(x_test,  maxlen=SENTENCE_LENGTH_MAX)\n",
    "TAG_SET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_input = Input(shape=(SENTENCE_LENGTH_MAX,), dtype='int32', name=\"SentencesTokens\")\n",
    "embedded_sequences = embedding_layer(tokens_input)\n",
    "\n",
    "#extra_input = ...\n",
    "\n",
    "aggregate_vectors = embedded_sequences # concat...\n",
    "\n",
    "rnn_outputs = Bidirectional( GRU(RNN_HIDDEN_SIZE, return_sequences=True),  merge_mode='concat' )(aggregate_vectors)\n",
    "\n",
    "is_ner_outputs  = TimeDistributed( Dense(TAG_SET_SIZE, activation='softmax'), \n",
    "                                   input_shape=(BATCH_SIZE, SENTENCE_LENGTH_MAX, RNN_HIDDEN_SIZE*2),\n",
    "                                   name='POS-class')(rnn_outputs)\n",
    "\n",
    "#is_ner_outputs =  TimeDistributed( Activation('softmax'), \n",
    "#                                   input_shape=(BATCH_SIZE, SENTENCE_LENGTH_MAX, TAG_SET_SIZE),                                  \n",
    "#                                   name='POS-class' )(is_ner_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[tokens_input], outputs=[is_ner_outputs])\n",
    "print( model.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")  # , metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.fit(x, y_one_hot)\n",
    "model.fit_generator(batch_for_network_generator(), 1000, epochs=1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Uncomment the ```pickle.dump()``` to actually save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_file = './data/RNN/tagger_rnn_trained_keras.h5'\n",
    "\n",
    "# Actually, this includes the embedding, which is a little redundant\n",
    "#model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tagger Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_results_for(test_sentences):\n",
    "    input_values, mask_values, target_values_int = prep_batch_for_network(test_sentences, include_targets=True)\n",
    "\n",
    "    rnn_output_, = rnn_predict(input_values, mask_values)\n",
    "\n",
    "    # rnn_output_ here is a softmax-vector at every word location\n",
    "    for i,sent in enumerate(test_sentences[0:5]):\n",
    "        annotated = [ \n",
    "                \"%s-%d-%d\" % (word, target_values_int[i,j], np.argmax(rnn_output_[i,j]), )    \n",
    "                for j,word in enumerate(sent) \n",
    "            ]\n",
    "        print(' '.join(annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences=[\n",
    "    \"Dr. Andrews works at Red Cat Labs .\",\n",
    "    \"Let 's see what part of speech analysis on this sample text looks like .\",\n",
    "    \"When are you off to New York , Chaitanya ?\",\n",
    "]\n",
    "\n",
    "#test_sentences = batch_sentences()\n",
    "test_sentences_mixed = [ s.split(' ') for s in sentences ]\n",
    "test_sentences_single = [ s.lower().split(' ') for s in sentences ]\n",
    "#test_sentences_single = [ s.upper().split(' ') for s in sentences ]\n",
    "\n",
    "print(\"Format : WORD-NLTK-RNN\")\n",
    "\n",
    "tag_results_for(test_sentences_mixed)\n",
    "print()\n",
    "tag_results_for(test_sentences_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  And let's look at the Statistics\n",
    "\n",
    "... actually, looking at the above samples, the NLTK PoS tagger is HOPELESS when the text is converted to a single case.  QED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Make the tagger identify different PoS (say : 'verbs')\n",
    "\n",
    "2.  Make the tagger return several different tags instead\n",
    "\n",
    "3.  See whether more advanced 'LSTM' nodes would improve the scores\n",
    "\n",
    "4.  Add a special 'is_uppercase' element to the embedding vector (or, more simply, just replace one of the elements with an indicator).  Does this help the NNP accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagger\n",
    "\n",
    "This example trains a RNN to tag words from a corpus - \n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32\n",
    "EMBEDDING_DIM=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentence_splitter.tokenize(\"This is Mr. Smith's tokenized test. The U.S.A gives us sent two. Is this sent three?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"This is Mr. Smith's tokenized test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Wikipedia Corpus\n",
    "\n",
    "From the corpus download page : http://wortschatz.uni-leipzig.de/en/download/\n",
    "\n",
    "Here's the paper that explains how the corpus was constructed : \n",
    "\n",
    "*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, shutil, requests\n",
    "\n",
    "corpus_dir = './data/RNN/'\n",
    "corpus_text_file = os.path.join(corpus_dir, 'en.wikipedia.2010.100K.txt')\n",
    "corpus_text_tar = 'eng_wikipedia_2010_100K.tar.gz'\n",
    "\n",
    "download_url = 'http://pcai056.informatik.uni-leipzig.de/downloads/corpora/'+corpus_text_tar\n",
    "\n",
    "# Fall-back url if too slow\n",
    "#download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+corpus_text_tar\n",
    "\n",
    "if not os.path.isfile( corpus_text_file ):\n",
    "    if not os.path.exists(corpus_dir):\n",
    "        os.makedirs(corpus_dir)\n",
    "        \n",
    "    # Get the download path from the web-service\n",
    "    #urllib.request.urlretrieve('http://wortschatz.uni-leipzig.de/download/service', corpus_text_tar)\n",
    "    # download_url = ...\n",
    "    \n",
    "    tarfilepath = os.path.join(corpus_dir, corpus_text_tar)\n",
    "    if not os.path.isfile( tarfilepath ):\n",
    "        response = requests.get(download_url, stream=True)\n",
    "        with open(tarfilepath, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    if os.path.isfile(tarfilepath):\n",
    "        import tarfile\n",
    "        #tarfile.open(tarfilepath, 'r:gz').extractall(corpus_dir)\n",
    "        tarfile.open(tarfilepath, 'r:gz').extract('eng_wikipedia_2010_100K-sentences.txt', corpus_dir)\n",
    "    shutil.move(os.path.join(corpus_dir, 'eng_wikipedia_2010_100K-sentences.txt'), corpus_text_file)\n",
    "    \n",
    "    # Get rid of tarfile source (the required text file itself will remain)\n",
    "    os.unlink(tarfilepath)\n",
    "\n",
    "print(\"Corpus available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This is a work-in-progress, since we should really discover 'download_url' from the 'service'\n",
    "#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data='file=%s&func=\"link\"' % (corpus_text_tar,))\n",
    "#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data=dict(file=corpus_text_tar, func=\"link\") )\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                n,l = line.split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    tree_banked = tokenizer.tokenize(s)\n",
    "                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n",
    "                        yield tree_banked\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pos_tagger = PerceptronTagger(load=True)\n",
    "' | '.join(list(pos_tagger.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Let 's see what part of speech analysis on this sample text looks like .\".split(' ')\n",
    "#s = next(corpus_sentence_tokens_gen)\n",
    "pos_tagger.tag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twist : Not interested in all classes...\n",
    "\n",
    "To simplify (dramatically), our RNN will be trained to just tell the difference between 'is ordinary word' and 'is entity name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list = 'O E'.split(' ')\n",
    "pos_tagger_entity_tags = set('NNP'.split(' '))\n",
    "pos_tagger_to_idx   = dict([ (t,(1 if t in pos_tagger_entity_tags else 0)) for i,t in enumerate(pos_tagger.classes)])\n",
    "TAG_SET_SIZE= len(tag_list)\n",
    "\n",
    "pos_tagger_to_idx['NNP'], pos_tagger_to_idx['VBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings\n",
    "Using the python package :  https://github.com/maciejkula/glove-python , and code samples from : http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
    "\n",
    "### Create the Co-occurrence Matrix\n",
    "For speed, this looks at the first 100,000 tokens in the corpus - and should create the co-occurences in 30 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glove\n",
    "glove_corpus = glove.Corpus()\n",
    "\n",
    "corpus_sentences = [ \n",
    "        [ w.lower() for w in next(corpus_sentence_tokens_gen)] # All lower-case\n",
    "        for _ in range(0,100*1000) \n",
    "    ]\n",
    "\n",
    "# Fit the co-occurrence matrix using a sliding window of 10 words.\n",
    "t0 = time.time()\n",
    "glove_corpus.fit(corpus_sentences, window=10)\n",
    "\n",
    "print(\"Dictionary length=%d\" % (len(glove_corpus.dictionary),))\n",
    "print(\"Co-occurrence calculated in %5.1fsec\" % (time.time()-t0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return the index of the word in the dictionary\n",
    "glove_corpus.dictionary['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the Word Embedding\n",
    "\n",
    "This will make use of up to 4 threads - and each epoch takes 20-30 seconds on a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding = glove.Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)\n",
    "\n",
    "t0 = time.time()\n",
    "glove_epochs, glove_threads = 20, 4 \n",
    "\n",
    "word_embedding.fit(glove_corpus.matrix, epochs=glove_epochs, no_threads=glove_threads, verbose=True)\n",
    "\n",
    "print(\"%d-d word-embedding created in %5.1fsec = %5.1fsec per epoch\" % (\n",
    "        EMBEDDING_DIM, (time.time()-t0), (time.time()-t0)/glove_epochs*glove_threads, ))\n",
    "\n",
    "# Add the word -> id dictionary to the model to allow similarity queries.\n",
    "word_embedding.add_dictionary(glove_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_embedding.save(\"./data/RNN/glove.embedding.50.pkl\")\n",
    "#word_embedding.load(\"./data/RNN/glove.embedding.50.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word-similarity test\n",
    "word_embedding.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word-analogy test\n",
    "def get_embedding_vec(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
    "    if idx<0:\n",
    "        #print(\"Missing word : '%s'\" % (word,))\n",
    "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
    "    return word_embedding.word_vectors[idx]\n",
    "\n",
    "def get_closest_word(vec, number=5):\n",
    "    dst = (np.dot(word_embedding.word_vectors, vec)\n",
    "                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n",
    "                   / np.linalg.norm(vec))\n",
    "    word_ids = np.argsort(-dst)\n",
    "    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n",
    "            if x in word_embedding.inverse_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#analogy_vec = get_embedding_vec('woman') - get_embedding_vec('man') + get_embedding_vec('king')\n",
    "analogy_vec = get_embedding_vec('paris') - get_embedding_vec('france') + get_embedding_vec('italy')\n",
    "#analogy_vec = get_embedding_vec('kitten') - get_embedding_vec('cat') + get_embedding_vec('dog')\n",
    "#analogy_vec = get_embedding_vec('understand') - get_embedding_vec('understood') + get_embedding_vec('ran')\n",
    "get_closest_word(analogy_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem : Embedding is *Poor*\n",
    "\n",
    "Solution : Load a pre-trained word embedding, from a much larger corpus.  Source of this word embedding (created from a 6 billion tokens corpus, with results as 50d vectors): http://nlp.stanford.edu/projects/glove/ \n",
    "\n",
    "NB: If you don't have the required data, the first step below downloads a 823Mb file via a fairly slow connection to a server at Stanford (this can take HOURS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, requests, shutil\n",
    "\n",
    "glove_dir = './data/RNN/'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "# These are temporary files if we need to download it from the original source (slow)\n",
    "data_cache = './data/cache'\n",
    "glove_full_tar = 'glove.6B.zip'\n",
    "glove_full_50d = 'glove.6B.50d.txt'\n",
    "\n",
    "#force_download_from_original=False\n",
    "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
    "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    if not os.path.exists(glove_dir):\n",
    "        os.makedirs(glove_dir)\n",
    "    \n",
    "    # First, try to download a pre-prepared file directly...\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
    "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    else:\n",
    "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "        if not os.path.exists(data_cache):\n",
    "            os.makedirs(data_cache)\n",
    "        \n",
    "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
    "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
    "            if not os.path.isfile( zipfilepath ):\n",
    "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
    "                response = requests.get(download_url, stream=True)\n",
    "                with open(zipfilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "            if os.path.isfile(zipfilepath):\n",
    "                print(\"Unpacking 50d GloVE file from zip\")\n",
    "                import zipfile\n",
    "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
    "\n",
    "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
    "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
    "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
    "                for i, l in enumerate(in_file.readlines()):\n",
    "                    if i>=100000: break\n",
    "                    out_file.write(l)\n",
    "    \n",
    "        # Get rid of tarfile source (the required text file itself will remain)\n",
    "        #os.unlink(zipfilepath)\n",
    "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
    "\n",
    "print(\"GloVE available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
    "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded that, play around with the similarity and analogy tests again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN Part-of-Speech Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = EMBEDDING_DIM # ?+1 for capitalisation flag\n",
    "GRAD_CLIP_BOUND = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the Embedding  Keras-Compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dir(word_embedding)\n",
    "#word_embedding.max_count, word_embedding.no_components, \n",
    "#word_embedding.word_vectors.shape\n",
    "word_embedding_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding_rnn = np.vstack([ \n",
    "        np.zeros( (1, EMBEDDING_DIM,), dtype='float32'),   # This is the 'zero' value (used as a mask in Keras)\n",
    "        np.zeros( (1, EMBEDDING_DIM,), dtype='float32'),   # This is for 'UNK'  (word == 1)\n",
    "        word_embedding.word_vectors,\n",
    "    ])\n",
    "word_embedding_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesising a 'correct answer' for the Tagger\n",
    "\n",
    "Normally, this would be the (manual) annotations from the corpus itself.  However, we don't have an annotated corpus.  Instead, we're going to use the annotations produced by the NTLK tagger - simplified to only identify 'NNP = entities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_to_idx_rnn(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)  # since UNK=1 = (-1+2)\n",
    "    return idx+2  # skip ahead 2 places\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.utils.np_utils import to_categorical\n",
    "\n",
    "def sentences_for_network(list_of_sentences, include_targets=False, one_hot_targets=False):\n",
    "    len_of_list = len(list_of_sentences)\n",
    "    #print(\"sentences_for_network.sentences.length = %d\" % (len_of_list,))\n",
    "    \n",
    "    input_values = np.zeros((len_of_list, SENTENCE_LENGTH_MAX), dtype='int32')\n",
    "    for i, sent in enumerate(list_of_sentences):\n",
    "        for j, word in enumerate(sent):\n",
    "            input_values[i,j] = word_to_idx_rnn(word)\n",
    "    \n",
    "    if not include_targets: \n",
    "        return (input_values, None)\n",
    "\n",
    "    if one_hot_targets:\n",
    "        # Add extra dimension here to suit Keras' TimeDistributed(Dense(softmax))\n",
    "        #   as discussed : https://github.com/fchollet/keras/issues/6363\n",
    "        target_values  = np.zeros((len_of_list, SENTENCE_LENGTH_MAX, TAG_SET_SIZE), dtype='int32')\n",
    "    else:\n",
    "        target_values  = np.zeros((len_of_list, SENTENCE_LENGTH_MAX), dtype='int32')\n",
    "        \n",
    "    for i, sent in enumerate(list_of_sentences):\n",
    "        sentence_tags = pos_tagger.tag(sent)\n",
    "        for j, word_tag in enumerate(sentence_tags):\n",
    "            tag = word_tag[1] # tags are returned as tuples (word, tag)\n",
    "            pos_class = pos_tagger_to_idx[tag]  # These are the class #s\n",
    "            if one_hot_targets:\n",
    "                target_values[i,j] = to_categorical(pos_class, num_classes=TAG_SET_SIZE)\n",
    "            else:\n",
    "                target_values[i,j] = pos_class\n",
    "\n",
    "    return (input_values, target_values)\n",
    "\n",
    "def batch_for_network_generator():\n",
    "    while True:\n",
    "        batch_of_sentences = [ next(corpus_sentence_tokens_gen) for i in range(BATCH_SIZE) ]    \n",
    "        yield sentences_for_network(batch_of_sentences, include_targets=True, one_hot_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the batchifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_batch_input, one_batch_targets = next(batch_for_network_generator())\n",
    "one_batch_input.shape, one_batch_targets.shape\n",
    "#one_batch_input[0]\n",
    "#one_batch_targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Symbolically\n",
    "\n",
    "#### Lasagne RNN tutorial (including conventions &amp; rationale)\n",
    "\n",
    "*  http://colinraffel.com/talks/hammer2015recurrent.pdf\n",
    "\n",
    "#### Lasagne Examples\n",
    "\n",
    "*  https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/recurrent.py\n",
    "*  https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n",
    "\n",
    "#### Keras Examples\n",
    "\n",
    "* \n",
    "*  https://github.com/fchollet/keras/issues/5022 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras.api.keras.preprocessing import sequence\n",
    "from tensorflow.contrib.keras.api.keras.layers import Input, Embedding, GRU, Dense, Activation\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n",
    "\n",
    "# Hmm : The following is not in the API...\n",
    "from tensorflow.contrib.keras.python.keras.layers import Bidirectional, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_input = Input(shape=(SENTENCE_LENGTH_MAX,), dtype='int32', name=\"SentencesTokens\")\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "#   note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedded_sequences = Embedding(word_embedding_rnn.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[ word_embedding_rnn ],\n",
    "                                input_length=SENTENCE_LENGTH_MAX,\n",
    "                                trainable=False, \n",
    "                                mask_zero=True,\n",
    "                                name=\"SentencesEmbedded\") (tokens_input)\n",
    "\n",
    "#extra_input = ...\n",
    "aggregate_vectors = embedded_sequences # concat...\n",
    "\n",
    "rnn_outputs = Bidirectional( GRU(RNN_HIDDEN_SIZE, return_sequences=True),  merge_mode='concat' )(aggregate_vectors)\n",
    "\n",
    "is_ner_outputs  = TimeDistributed( Dense(TAG_SET_SIZE, activation='softmax'), \n",
    "                                   input_shape=(BATCH_SIZE, SENTENCE_LENGTH_MAX, RNN_HIDDEN_SIZE*2),\n",
    "                                   name='POS-class')(rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[tokens_input], outputs=[is_ner_outputs])\n",
    "print( model.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")  # , metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase for the RNN\n",
    "\n",
    "This will actually **train** the RNN - which can take 3-5minutes (depending on your CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.fit(x, y_one_hot)\n",
    "model.fit_generator(batch_for_network_generator(), 1000, epochs=1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Uncomment the ```model.save_weights()``` to actually save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_file = './data/RNN/tagger_rnn_trained_keras.h5'\n",
    "\n",
    "# Actually, this includes the embedding, which is a little redundant\n",
    "#model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tagger Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_results_for(test_sentences):\n",
    "    #sentences_for_network(list_of_sentences, include_targets=False, one_hot_targets=False)\n",
    "    input_values, target_values_int = sentences_for_network(test_sentences, include_targets=True)\n",
    "\n",
    "    rnn_output = model.predict_on_batch(input_values)\n",
    "\n",
    "    # rnn_output here is a softmax-vector at every word location\n",
    "    for i,sent in enumerate(test_sentences): # [0:5]):\n",
    "        annotated = [ \n",
    "                \"%s-%d-%d\" % (word, target_values_int[i,j], np.argmax(rnn_output[i,j]), )    \n",
    "                for j,word in enumerate(sent) \n",
    "            ]\n",
    "        print(' '.join(annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences=[\n",
    "    \"Dr. Andrews works at Red Cat Labs .\",\n",
    "    \"Let 's see what part of speech analysis looks like .\",\n",
    "    \"When are you off to New York , Chaitanya ?\",\n",
    "]\n",
    "\n",
    "# Uncomment this for 8 sentences from the corpus\n",
    "#sentences = [ ' '.join(next(corpus_sentence_tokens_gen)) for i in range(8) ]\n",
    "\n",
    "test_sentences_mixed = [ s.split(' ') for s in sentences ]\n",
    "test_sentences_title = [ s.title().split(' ') for s in sentences ]\n",
    "test_sentences_single = [ s.lower().split(' ') for s in sentences ]\n",
    "#test_sentences_single = [ s.upper().split(' ') for s in sentences ]\n",
    "\n",
    "print(\"Format : WORD-NLTK-RNN\\n\")\n",
    "\n",
    "tag_results_for(test_sentences_mixed)\n",
    "print()\n",
    "tag_results_for(test_sentences_title)\n",
    "print()\n",
    "tag_results_for(test_sentences_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  And let's look at the Statistics\n",
    "\n",
    "... actually, looking at the above samples, the NLTK PoS tagger is HOPELESS when the text is converted to a single case, or title case. QED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Make the tagger identify different PoS (say : 'verbs')\n",
    "\n",
    "2.  Make the tagger return several different tags instead\n",
    "\n",
    "3.  See whether more advanced 'LSTM' nodes would improve the scores\n",
    "\n",
    "4.  Add a special 'is_uppercase' element to the embedding vector (or, more simply, just replace one of the elements with an indicator).  Does this help the NNP accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WJx1Iv7qRPN"
   },
   "outputs": [],
   "source": [
    "# Upload the IMDB_all_reviews.txt here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "eDRonKjxqVey",
    "outputId": "ee535134-964f-4f50-e2ed-03c71d346088"
   },
   "outputs": [],
   "source": [
    "!wc IMDB*.txt\n",
    "#    25000  6723817 33596339 IMDB_all_reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "sop8D99trBQA",
    "outputId": "745d84d4-6e63-4bb4-d263-6123a43d1da2"
   },
   "outputs": [],
   "source": [
    "! head IMDB_all_reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8wFph-nB13O"
   },
   "outputs": [],
   "source": [
    "# Now split into train / validation (and also lowercase it all)\n",
    "import random\n",
    "\n",
    "with open('IMDB_all_reviews.txt', 'rt') as fin,      \\\n",
    "     open('reviews-train.txt', 'wt') as ftrain, \\\n",
    "     open('reviews-valid.txt', 'wt') as fvalid:\n",
    "  for l in fin:\n",
    "    if random.random()<0.9:\n",
    "      ftrain.write(l.lower())  # No need for +'\\n' - l includes it\n",
    "    else:\n",
    "      fvalid.write(l.lower())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "uem6DSjYB1yO",
    "outputId": "a33713fd-359d-4399-ea44-444a62c3a9eb"
   },
   "outputs": [],
   "source": [
    "! wc reviews-*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7BSGdzjB1qZ"
   },
   "outputs": [],
   "source": [
    "num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeY-E7rtsxbM"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import requests, shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmmjJ8_dxl1T"
   },
   "outputs": [],
   "source": [
    "# ! rm glove.first-100k.6B.50d.txt  # Force download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "9DwFRoSSxlut",
    "outputId": "7118db1d-3b53-4af6-f734-3b4ae16c7a5d"
   },
   "outputs": [],
   "source": [
    "# Load the GloVe embedding, along with the words\n",
    "glove_dir = './'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "data_cache = './'\n",
    "glove_full_tar = 'glove.6B.zip'\n",
    "glove_full_50d = 'glove.6B.50d.txt'\n",
    "\n",
    "#force_download_from_original=False\n",
    "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
    "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    if not os.path.exists(glove_dir):\n",
    "        os.makedirs(glove_dir)\n",
    "    \n",
    "    # First, try to download a pre-prepared file directly...\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
    "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    else:\n",
    "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "        if not os.path.exists(data_cache):\n",
    "            os.makedirs(data_cache)\n",
    "        \n",
    "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
    "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
    "            if not os.path.isfile( zipfilepath ):\n",
    "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
    "                response = requests.get(download_url, stream=True)\n",
    "                with open(zipfilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "            if os.path.isfile(zipfilepath):\n",
    "                print(\"Unpacking 50d GloVE file from zip\")\n",
    "                import zipfile\n",
    "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
    "\n",
    "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
    "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
    "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
    "                for i, l in enumerate(in_file.readlines()):\n",
    "                    if i>=100000: break\n",
    "                    out_file.write(l)\n",
    "    \n",
    "        # Get rid of tarfile source (the required text file itself will remain)\n",
    "        #os.unlink(zipfilepath)\n",
    "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
    "\n",
    "if os.path.isfile( glove_100k_50d_path ):\n",
    "  print(\"GloVE available locally\")\n",
    "  ! head -3 {glove_100k_50d_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcB2wOs30HJE"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48677077/how-do-i-create-a-keras-embedding-layer-from-a-pre-trained-word-embedding-datase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "itquBmsx0HEu",
    "outputId": "cb9e31a5-110d-4fc5-b179-4d13a8bf38e5"
   },
   "outputs": [],
   "source": [
    "# Prepare Glove File\n",
    "def readGloveFile(gloveFile):\n",
    "    with open(gloveFile, 'r') as f:\n",
    "        wordToGlove = {}  # map from a token (word) to a Glove embedding vector\n",
    "        wordToIndex = {}  # map from a token to an index\n",
    "        indexToWord = {}  # map from an index to a token \n",
    "\n",
    "        for line in f:\n",
    "            record = line.strip().split()\n",
    "            token = record[0] # take the token (word) from the text line\n",
    "            # associate the Glove embedding vector to a that token (word)\n",
    "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
    "        \n",
    "        #tokens = sorted(wordToGlove.keys())\n",
    "        #tokens = wordToGlove.keys() # \n",
    "        \n",
    "        token_mask, token_unk = '<MASK>', '<UNK>'\n",
    "        token_list = [token_mask, token_unk,]+list(wordToGlove.keys())\n",
    "        \n",
    "        wordToGlove[token_mask] = np.zeros_like(wordToGlove[token]) \n",
    "        wordToGlove[token_unk]  = np.zeros_like(wordToGlove[token]) \n",
    "        \n",
    "        for idx, tok in enumerate(token_list):\n",
    "            #kerasIdx = idx + 1  # 0 is reserved for masking in Keras (see above)\n",
    "            #kerasIdx = idx\n",
    "            #wordToIndex[tok] = kerasIdx # associate an index to a token (word)\n",
    "            #indexToWord[kerasIdx] = tok # associate a word to a token (word). \n",
    "            wordToIndex[tok] = idx # associate an index to a token (word)\n",
    "            indexToWord[idx] = tok # associate a word to a token (word). \n",
    "            # Note: inverse of dictionary above\n",
    "\n",
    "    return wordToIndex, indexToWord, wordToGlove, token_list\n",
    "\n",
    "wordToIndex, indexToWord, wordToGlove, token_list = readGloveFile(glove_100k_50d_path)\n",
    "[ indexToWord[i] for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eo7NNbW7aiYb"
   },
   "outputs": [],
   "source": [
    "# Create Pretrained Keras Embedding Layer\n",
    "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
    "    #vocabLen = len(wordToIndex) + 1  # adding 1 to account for masking\n",
    "    vocabLen = len(wordToIndex)\n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]  # works with any glove dimensions (e.g. 50)\n",
    "\n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))    # initialize with zeros\n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] # create embedding: word index to Glove word embedding\n",
    "\n",
    "    embeddingLayer = keras.layers.Embedding(vocabLen, embDim, \n",
    "                                     weights=[embeddingMatrix], \n",
    "                                     mask_zero=True,  # zero embedding for zero_padding\n",
    "                                     trainable=isTrainable)\n",
    "    return embeddingLayer\n",
    "  \n",
    "pretrainedEmbeddingLayer = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiIkvhVFadKA"
   },
   "outputs": [],
   "source": [
    "# usage\n",
    "#model = Sequential()\n",
    "#model.add(pretrainedEmbeddingLayer)\n",
    "# or\n",
    "#model.add(Embedding(max_features, 128, mask_zero = True))  # zero embedding for zero_padding\n",
    "#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ms069Ow_0HAa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oeVu3YUrM33"
   },
   "outputs": [],
   "source": [
    "## Terrible documentation :\n",
    "# https://www.tensorflow.org/guide/datasets#consuming_text_data\n",
    "\n",
    "## Much better documentation :\n",
    "# https://cs230-stanford.github.io/tensorflow-input-data.html#introduction-to-tfdata-with-a-text-example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HAwd9p4xlrY"
   },
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/api_docs/python/tf/contrib/lookup/index_table_from_tensor\n",
    "\n",
    "mapping_strings = tf.constant(token_list)\n",
    "embedding_mapping = tf.contrib.lookup.index_table_from_tensor(\n",
    "    mapping=mapping_strings, num_oov_buckets=0, default_value=wordToIndex['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ve7yOmLHsxXN"
   },
   "outputs": [],
   "source": [
    "reviews_train = tf.data.TextLineDataset(\"reviews-train.txt\")\n",
    "reviews_valid = tf.data.TextLineDataset(\"reviews-valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kq8YurX8tp6C"
   },
   "outputs": [],
   "source": [
    "def preprocess_line(line):\n",
    "  line_data = tf.string_split([line], delimiter='|').values\n",
    "  \n",
    "  label = tf.string_to_number( line_data[0], out_type=tf.int32)\n",
    "  txt = tf.string_split([ line_data[1] ], \n",
    "                        delimiter=' ', \n",
    "                        skip_empty=True).values  # lower-case conversion done above\n",
    "  \n",
    "  # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "  #txt = tf.keras.preprocessing.text.text_to_word_sequence( line_data[1] )\n",
    "\n",
    "  txt_ids = embedding_mapping.lookup(txt)\n",
    "  label_onehot = tf.one_hot(label, depth=num_classes, axis=-1)\n",
    "  \n",
    "  # This gives is reviews[0] as a review tensor with variable length and reviews[1] as label\n",
    "  return txt_ids, label_onehot\n",
    "\n",
    "def batch_padded(ds, is_training=False, buffer_size=100, batch_size=4):\n",
    "  if is_training:\n",
    "    ds = ds.shuffle(buffer_size=buffer_size)\n",
    "  ds = ds.map(preprocess_line)\n",
    "  ds = ds.padded_batch(batch_size, \n",
    "                       padded_shapes=(tf.TensorShape([None]), tf.TensorShape([num_classes])), \n",
    "                       #padding_values=(0,0)  # Defaults to 0 padding, which is <MASK> which is fine\n",
    "                      )\n",
    "  \n",
    "  #ds = ds.prefetch(1)  # Makes it run async\n",
    "  return ds  \n",
    "\n",
    "reviews_train_ds = batch_padded(reviews_train, is_training=True)\n",
    "reviews_valid_ds = batch_padded(reviews_valid, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "neJacOIVsxT6",
    "outputId": "16788b82-a48c-401a-b09b-67e86ea0070b"
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer()\n",
    "tf.tables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1139
    },
    "colab_type": "code",
    "id": "LhzkDu70sxQs",
    "outputId": "6a04699d-ab1c-46b5-a5dc-0e2d55b2610c"
   },
   "outputs": [],
   "source": [
    "iterator = reviews_train_ds.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run( tf.tables_initializer() )\n",
    "  sess.run( iterator.initializer )\n",
    "  for i in range(1):\n",
    "    sentence, label = sess.run(next_element)\n",
    "    print(label[0], ' :: ', ' '.join([indexToWord[i] for i in sentence[0] ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkJwU31gsxK0"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46135499/how-to-properly-combine-tensorflows-dataset-api-and-keras\n",
    "# http://ruder.io/text-classification-tensorflow-estimators/\n",
    "# https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMBD-txt-to-Dataset",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
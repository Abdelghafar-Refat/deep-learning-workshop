{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, shutil\n",
    "\n",
    "download_dir = './data/RNN/'\n",
    "data_cache = './data/cache'\n",
    "\n",
    "def ensure_downloaded_and_prepared(expected_file, original_url, zipsize_check, \n",
    "                                   vocab_size=100000, embedding_dim=50):\n",
    "    print('\"%s\" preparation' % (expected_file, ))\n",
    "    final_path = os.path.join(download_dir, expected_file)\n",
    "\n",
    "    download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/'+final_path\n",
    "    \n",
    "    if not os.path.isfile( final_path ):\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "        # First, try to download a pre-prepared file directly...\n",
    "        response = requests.get(download_url, stream=True)\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            print(\"  Downloading pre-prepared file from RedCatLabs\")\n",
    "            with open(final_path, 'wb') as out_file:\n",
    "                shutil.copyfileobj(response.raw, out_file)\n",
    "        else:\n",
    "            # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "            if not os.path.exists(data_cache):\n",
    "                os.makedirs(data_cache, exist_ok=True)\n",
    "\n",
    "            archivefile = original_url[ original_url.rfind('/')+1:]\n",
    "            archivefilepath = os.path.join(data_cache, archivefile)\n",
    "            \n",
    "            if not os.path.isfile( archivefilepath ):\n",
    "                print(\"  Downloading file of size %d from %s\" % (zipsize_check, original_url,))\n",
    "                response = requests.get(original_url, stream=True)\n",
    "                with open(archivefilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "                # Should also check the size of the download...\n",
    "                print(\"  Finished Download\")\n",
    "                \n",
    "            vecfile = archivefile.replace('.zip', '').replace('.gz', '')\n",
    "            vecfilepath = os.path.join(data_cache, vecfile)\n",
    "            if not os.path.isfile( vecfilepath ):\n",
    "                if archivefile.endswith('.zip'):\n",
    "                    print('  Unpacking \"%s\" from .zip' % (vecfile,))\n",
    "                    import zipfile\n",
    "                    zipfile.ZipFile(archivefilepath, 'r').extract(vecfile, data_cache)\n",
    "                if archivefile.endswith('.gz'):\n",
    "                    print('  Unpacking \"%s\" from .gz' % (vecfile,))\n",
    "                    import gzip\n",
    "                    with gzip.open(archivefilepath, 'rb') as f_in:\n",
    "                        with open(vecfilepath, 'wb') as f_out:\n",
    "                            #f_out.write(f_in.read())\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                print(\"  Finished unpacking\")\n",
    "\n",
    "            with open(vecfilepath, 'rt') as in_file:\n",
    "                with open(final_path, 'wt') as out_file:\n",
    "                    print(\"  Reducing vec file to first 100k words, 50 columns\")\n",
    "                    print('  First line : \"%s\"' % (in_file.readline().strip(),))\n",
    "                    out_file.write(\"%d %d\\n\" % (vocab_size, embedding_dim))\n",
    "                    for i, l in enumerate(in_file): # .readlines() is not an iterator...\n",
    "                        if i>=vocab_size: break\n",
    "                        # Parse the line\n",
    "                        arr = l.strip().split(' ')\n",
    "                        word = arr[0]\n",
    "                        nums = arr[1:embedding_dim+1]\n",
    "                        out_file.write(\"%s %s\\n\" % (word, ' '.join(nums),))\n",
    "\n",
    "            # Get rid of tarfile source (the required text file itself will remain)\n",
    "            #os.unlink(archivefilepath)\n",
    "            #os.unlink(vecfilepath)\n",
    "\n",
    "    print('  File is available locally')\n",
    "\n",
    "    \n",
    "# See : https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "download_base = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors'\n",
    "\n",
    "ensure_downloaded_and_prepared(  # English\n",
    "    'wiki-news-300d-1M.vec.50d-100k.txt', \n",
    "    download_base+'/wiki-news-300d-1M.vec.zip',\n",
    "     682631666)  # 683MB download\n",
    "\n",
    "ensure_downloaded_and_prepared(  # Chinese (Mandarin)\n",
    "    'cc.zh.300.vec.50d-100k.txt', \n",
    "    download_base+'/word-vectors-v2/cc.zh.300.vec.gz',\n",
    "    1358778961)  # 1.36GB download\n",
    "\n",
    "ensure_downloaded_and_prepared(  # Malay\n",
    "    'cc.ms.300.vec.50d-100k.txt',\n",
    "    download_base+'/word-vectors-v2/cc.ms.300.vec.gz',\n",
    "     710958603) # 711MB download\n",
    "\n",
    "ensure_downloaded_and_prepared(  # French\n",
    "    'cc.fr.300.vec.50d-100k.txt',\n",
    "    download_base+'/word-vectors-v2/cc.fr.300.vec.gz',\n",
    "    1287757366) # 1.29GB download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls -l ./data/cache/\n",
    "#! rm ./data/cache/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim==3.4.0\n",
    "import gensim\n",
    "gensim.__version__  # '3.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecfile = './data/RNN/wiki-news-300d-1M.vec.50d-100k.txt'\n",
    "xx_vecfile = './data/RNN/cc.zh.300.vec.50d-100k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the english language model from the vectors stored on disk\n",
    "en_model = KeyedVectors.load_word2vec_format(en_vecfile)\n",
    "en_model.init_sims(replace=True)\n",
    "\n",
    "len(en_model.vocab), en_model.vector_size # Vocab size and dim (expect 100k x 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join([ f for f in dir(en_model) if not f.startswith('_') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "', '.join([ en_model.index2word[random.randint(0,len(en_model.vocab))] \n",
    "           for _ in range(30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a word \n",
    "find_similar_to = 'dog'\n",
    "\n",
    "# Finding out similar words\n",
    "for similar_word in en_model.similar_by_word(find_similar_to, topn=10):\n",
    "    print(\"Similarity: %.2f, Word: %s\" % ( similar_word[1], similar_word[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(s):  # A is to B as C is to 'D'?\n",
    "    (a,b,c,d) = s.split(' ')\n",
    "    print(\"Trying for '%s'\" % (d,))\n",
    "    for similar_word in en_model.most_similar(\n",
    "            positive=[b,c], negative=[a], topn=3):\n",
    "        print(\"  Similarity: %.2f, Word: %s\" % ( similar_word[1], similar_word[0],))\n",
    "\n",
    "test_analogy('man woman king queen')\n",
    "test_analogy('paris france london england')\n",
    "test_analogy('kitten cat puppy dog')\n",
    "test_analogy('look looked run ran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in another language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_model = KeyedVectors.load_word2vec_format(xx_vecfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick some pairs of translations...\n",
    "translations = ( 'house:\u5c4b door:\u95e8 wheel:\u8f6e money:\u94b1 book:\u4e66 ear:\u8033 mouth:\u5634 '\n",
    "                +'electronic:\u7535\u5b50 car:\u6c7d\u8f66 key:\u952e village:\u6751 student:\u5b66\u751f '\n",
    "                +'sky:\u5929\u7a7a mountain:\u5c71 tree:\u6811 river:\u6cb3 beach:\u6d77\u6ee9 rain:\u96e8 '\n",
    "                +'bird:\u9e1f fish:\u9c7c butterfly:\u8774\u8776 cow:\u725b rat:\u9f20 strawberry:\u8349\u8393 honey:\u871c '\n",
    "                +'jump:\u8df3 speak:\u8bf4\u8bdd count:\u8ba1\u6570 explain:\u8bf4\u660e climb:\u722c '\n",
    "                +'tall:\u9ad8 heavy:\u91cd red:\u7ea2 gold:\u91d1 ancient:\u53e4 rapid:\u5feb\u901f '\n",
    "                +'seven:\u4e03 thousand:\u5343 circle:\u5708 perpendicular:\u5782\u76f4 fraction:\u5206\u6570 '\n",
    "                +'hero:\u82f1\u96c4 sword:\u5251 awkward:\u5c34\u5c2c night:\u665a arrival:\u5230\u8fbe '\n",
    "                #+'aggressive:\u4fb5\u7565\u6027 discount:\u6298\u6263 apartment:\u516c\u5bd3 computer:\u7535\u8111 '\n",
    "               ).strip().split()\n",
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "en_arr, xx_arr = [], []\n",
    "for en_word,xx_word in [pair.split(':') for pair in translations]:\n",
    "    if not en_word in en_model.vocab:\n",
    "        print(\"Failed to find %s (~%s)\" % (en_word,xx_word))\n",
    "        continue\n",
    "    if not xx_word in xx_model.vocab:\n",
    "        print(\"Failed to find %s (~%s)\" % (xx_word,en_word))\n",
    "        continue\n",
    "    en_arr.append( en_model.get_vector(en_word))\n",
    "    xx_arr.append( xx_model.get_vector(xx_word))\n",
    "\n",
    "len(en_arr), en_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_known = np.array( en_arr )\n",
    "xx_known = np.array( xx_arr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate en->xx      EN.A = XX => A=inv(EN).XX\n",
    "en_trans = np.dot(np.linalg.pinv(en_known), xx_known)\n",
    "\n",
    "# Translate xx->en      XX.B = EN => B=inv(XX).EN\n",
    "en_trans = np.dot(np.linalg.pinv(xx_known), en_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's attempt a translation of some unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test = 'door cat chair bee twenty entrance haircut'\n",
    "for en_word in en_test.split(' '):\n",
    "    print(\"Translate : '%s'\" % (en_word,))\n",
    "    en_vec = en_model.get_vector(en_word)\n",
    "    xx_vec = np.dot(en_vec, en_trans)\n",
    "    #print(en_vec.shape, en_trans.shape)\n",
    "    for xx_word in xx_model.similar_by_vector(xx_vec, topn=3):\n",
    "        print(\"  Similarity: %.2f, Word: %s\" % ( xx_word[1], xx_word[0],))\n",
    "# ideal(?) : '\u732b \u6905\u5b50 \u871c\u8702 \u4e8c\u5341 \u5165\u53e3 \u7406\u53d1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
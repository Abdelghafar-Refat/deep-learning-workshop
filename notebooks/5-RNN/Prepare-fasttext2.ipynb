{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, shutil\n",
    "\n",
    "download_dir = './data/RNN/'\n",
    "data_cache = './data/cache'\n",
    "\n",
    "def ensure_downloaded_and_prepared(expected_file, original_url, zipsize_check_IGNORED, \n",
    "                                   vocab_size=100000, embedding_dim=50):\n",
    "    final_path = os.path.join(download_dir, expected_file)\n",
    "\n",
    "    # These are temporary files if we need to download it from the original source (slow)\n",
    "    #full_archive = 'glove.6B.zip'\n",
    "    #full_extract = 'glove.6B.50d.txt'\n",
    "\n",
    "    download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/'+final_path\n",
    "    \n",
    "    if not os.path.isfile( final_path ):\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "        # First, try to download a pre-prepared file directly...\n",
    "        response = requests.get(download_url, stream=True)\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            print(\"Downloading pre-prepared file from RedCatLabs\")\n",
    "            with open(final_path, 'wb') as out_file:\n",
    "                shutil.copyfileobj(response.raw, out_file)\n",
    "        else:\n",
    "            # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "            if not os.path.exists(data_cache):\n",
    "                os.makedirs(data_cache, exist_ok=True)\n",
    "\n",
    "            zipfile = original_url[ original_url.rfind('/')+1:]\n",
    "            zipfilepath = os.path.join(data_cache, zipfile)\n",
    "            \n",
    "            if not os.path.isfile( zipfilepath ):\n",
    "                print(\"Downloading large file from %s\" % (original_url,))\n",
    "                response = requests.get(original_url, stream=True)\n",
    "                with open(zipfilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "                print(\"Finished Download\")\n",
    "                \n",
    "            vecfile = zipfile.replace('.zip', '').replace('.gz', '')\n",
    "            vecfilepath = os.path.join(data_cache, vecfile)\n",
    "            if not os.path.isfile( vecfilepath ):\n",
    "                if zipfile.endswith('.zip'):\n",
    "                    print('Unpacking \"%s\" from .zip' % (vecfile,))\n",
    "                    import zipfile\n",
    "                    zipfile.ZipFile(zipfilepath, 'r').extract(vecfile, data_cache)\n",
    "                if zipfile.endswith('.gz'):\n",
    "                    print('Unpacking \"%s\" from .gz' % (vecfile,))\n",
    "                    import gzip\n",
    "                    with gzip.open(zipfilepath, 'rb') as f_in:\n",
    "                        with open(vecfilepath, 'wb') as f_out:\n",
    "                            f_out.write(f_in.read())\n",
    "                print(\"Finished unpacking\")\n",
    "\n",
    "            with open(vecfilepath, 'rt') as in_file:\n",
    "                with open(final_path, 'wt') as out_file:\n",
    "                    print(\"Reducing vec file to first 100k words, 50 columns\")\n",
    "                    print('  First line : \"%s\"' % (in_file.readline().strip(),))\n",
    "                    out_file.write(\"%d %d\\n\" % (vocab_size, embedding_dim))\n",
    "                    for i, l in enumerate(in_file.readlines()):\n",
    "                        if i>=vocab_size: break\n",
    "                        # Parse the line\n",
    "                        arr = l.strip().split(' ')\n",
    "                        word = arr[0]\n",
    "                        nums = arr[1:embedding_dim+1]\n",
    "                        out_file.write(\"%s %s\\n\" % (word, ' '.join(nums),))\n",
    "\n",
    "            # Get rid of tarfile source (the required text file itself will remain)\n",
    "            #os.unlink(zipfilepath)\n",
    "            #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
    "\n",
    "    print('\"%s\" available locally' % (expected_file, ))\n",
    "\n",
    "download_base = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors'\n",
    "\n",
    "ensure_downloaded_and_prepared(  # English\n",
    "    'wiki-news-300d-1M.vec.50d-100k.txt', \n",
    "    download_base+'/wiki-news-300d-1M.vec.zip',\n",
    "     682631666)  # 683MB download\n",
    "\n",
    "ensure_downloaded_and_prepared(  # Chinese (Mandarin)\n",
    "    'cc.zh.300.vec.50d-100k.txt', \n",
    "    download_base+'/word-vectors-v2/cc.zh.300.vec.gz',\n",
    "    1358778961)  # 1.36GB download\n",
    "\n",
    "ensure_downloaded_and_prepared(  # Malay\n",
    "    'cc.ms.300.vec.50d-100k.txt',\n",
    "    download_base+'/word-vectors-v2/cc.ms.300.vec.gz',\n",
    "     710958603) # 711MB download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls -l ./data/cache/\n",
    "#! rm ./data/cache/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim==3.4.0\n",
    "import gensim\n",
    "gensim.__version__  # '3.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecfile = './data/RNN/wiki-news-300d-1M.vec.50d-100k.txt'\n",
    "xx_vecfile = './data/RNN/cc.zh.300.vec.50d-100k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the english language model from the vectors stored on disk\n",
    "en_model = KeyedVectors.load_word2vec_format(en_vecfile)\n",
    "\n",
    "len(en_model.vocab), en_model.vector_size # Vocab size and dim (expect 100k x 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join([ f for f in dir(en_model) if not f.startswith('_') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a word \n",
    "find_similar_to = 'dog'\n",
    "\n",
    "# Finding out similar words\n",
    "for similar_word in en_model.similar_by_word(find_similar_to, topn=10):\n",
    "    print(\"Similarity: %.2f, Word: %s\" % ( similar_word[1], similar_word[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
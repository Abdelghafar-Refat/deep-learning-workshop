{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano + Lasagne :: MNIST MLP\n",
    "==============================\n",
    "\n",
    "This is a quick illustration of a single-layer network training on the MNIST data.\n",
    "\n",
    "( Credit for this workbook : Eben Olson :: https://github.com/ebenolson/pydata2015 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download the MNIST digits dataset (Already downloaded locally)\n",
    "# !wget -N --directory-prefix=./data/MNIST/ http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and test splits as numpy arrays\n",
    "train, val, test = pickle.load(gzip.open('./data/MNIST/mnist.pkl.gz'))\n",
    "\n",
    "X_train, y_train = train\n",
    "X_val, y_val = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The original 28x28 pixel images are flattened into 784 dimensional feature vectors\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the first few examples \n",
    "plt.figure(figsize=(12,3))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For training, we want to sample examples at random in small batches\n",
    "def batch_gen(X, y, N):\n",
    "    while True:\n",
    "        idx = np.random.choice(len(y), N)\n",
    "        yield X[idx].astype('float32'), y[idx].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# A very simple network, a single layer with one neuron per target class.\n",
    "# Using the softmax activation function gives us a probability distribution at the output.\n",
    "l_in = lasagne.layers.InputLayer((None, 784))\n",
    "l_out = lasagne.layers.DenseLayer(\n",
    "    l_in,\n",
    "    num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Symbolic variables for our input features and targets\n",
    "X_sym = T.matrix()\n",
    "y_sym = T.ivector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Theano expressions for the output distribution and predicted class\n",
    "output = lasagne.layers.get_output(l_out, X_sym)\n",
    "pred = output.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The loss function is cross-entropy averaged over a minibatch, we also compute accuracy as an evaluation metric\n",
    "loss = T.mean(lasagne.objectives.categorical_crossentropy(output, y_sym))\n",
    "acc = T.mean(T.eq(pred, y_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We retrieve all the trainable parameters in our network - a single weight matrix and bias vector\n",
    "params = lasagne.layers.get_all_params(l_out)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the gradient of the loss function with respect to the parameters.\n",
    "# The stochastic gradient descent algorithm produces updates for each param\n",
    "grad = T.grad(loss, params)\n",
    "updates = lasagne.updates.sgd(grad, params, learning_rate=0.05)\n",
    "print(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We define a training function that will compute the loss and accuracy, and take a single optimization step\n",
    "f_train = theano.function([X_sym, y_sym], [loss, acc], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The validation function is similar, but does not update the parameters\n",
    "f_val = theano.function([X_sym, y_sym], [loss, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The prediction function doesn't require targets, and outputs only the predicted class values\n",
    "f_predict = theano.function([X_sym], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll choose a batch size, and calculate the number of batches in an \"epoch\"\n",
    "# (approximately one pass through the data).\n",
    "BATCH_SIZE = 64\n",
    "N_BATCHES = len(X_train) // BATCH_SIZE\n",
    "N_VAL_BATCHES = len(X_val) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minibatch generators for the training and validation sets\n",
    "train_batches = batch_gen(X_train, y_train, BATCH_SIZE)\n",
    "val_batches = batch_gen(X_val, y_val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try sampling from the batch generator.\n",
    "# Plot an image and corresponding label to verify they match.\n",
    "X, y = next(train_batches)\n",
    "plt.imshow(X[0].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For each epoch, we call the training function N_BATCHES times,\n",
    "# accumulating an estimate of the training loss and accuracy.\n",
    "# Then we do the same thing for the validation set.\n",
    "# Plotting the ratio of val to train loss can help recognize overfitting.\n",
    "for epoch in range(10):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for _ in range(N_BATCHES):\n",
    "        X, y = next(train_batches)\n",
    "        loss, acc = f_train(X, y)\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "    train_loss /= N_BATCHES\n",
    "    train_acc /= N_BATCHES\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for _ in range(N_VAL_BATCHES):\n",
    "        X, y = next(val_batches)\n",
    "        loss, acc = f_val(X, y)\n",
    "        val_loss += loss\n",
    "        val_acc += acc\n",
    "    val_loss /= N_VAL_BATCHES\n",
    "    val_acc /= N_VAL_BATCHES\n",
    "    \n",
    "    print('Epoch {:2d}, Train loss {:.03f}     (validation : {:.03f}) ratio {:.03f}'.format(\n",
    "            epoch, train_loss, val_loss, val_loss/train_loss))\n",
    "    print('          Train accuracy {:.03f} (validation : {:.03f})'.format(train_acc, val_acc))\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We can retrieve the value of the trained weight matrix from the output layer.\n",
    "# It can be interpreted as a collection of images, one per class\n",
    "weights = l_out.W.get_value()\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the weight images, we can recognize similarities to the target images\n",
    "plt.figure(figsize=(12,3))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(weights[:,i].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exercises\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Logistic regression\n",
    "----------------------\n",
    "\n",
    "The simple network we created is similar to a logistic regression model. Verify that the accuracy is close to that of `sklearn.linear_model.LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment and execute this cell for an example solution\n",
    "# %load spoilers/logreg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hidden layer\n",
    "---------------\n",
    "\n",
    "Try adding one or more \"hidden\" `DenseLayers` between the input and output. Experiment with different numbers of hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment and execute this cell for an example solution\n",
    "# %load spoilers/hiddenlayer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Optimizer\n",
    "------------\n",
    "\n",
    "Try one of the other algorithms available in `lasagne.updates`. You may also want to adjust the learning rate.\n",
    "Visualize and compare the trained weights. Different optimization trajectories may lead to very different results, even if the performance is similar. This can be important when training more complicated networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment and execute this cell for an example solution\n",
    "# %load spoilers/optimizer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}